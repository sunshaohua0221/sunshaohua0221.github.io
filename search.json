[{"title":"什么是AI Agent","path":"/2025/11/06/什么是AI Agent/","content":"大模型大家都知道，比如我们常见的ChatGPT、DeepSeek等，但是，这些大模型都有一个关键的问题，那就是他们没办法用工具，比如我想要让大模型帮我查询一个接口，他是做不到的。那么，如果给大模型增加工具的调用能力，并且他知道该什么时候调用什么工具，这基本上就是一个Agent了。Agent翻译成中文是智能体，或者叫做助理更合适，比如说这就是个Agent：你对你的小爱同学说，我想吃肯德基，他就能分析出你可能想吃什么，然后让你确认后，直接就帮你把肯德基点好了。这个过程需要： 小爱同学知道你想吃什么，了解你的口味。 小爱同学知道点餐需要打开先软件，然后搜索，然后付款 小爱同学可以帮你自动完成这些操作 该怎么实现这样的功能呢？下面这张图就是非常出名的Agent的图： 可以看到，这里面包括了Tools、Action、Planning以及Memory，Tools就是我们前面说过的工具，而Action就可以理解为是对工具的调用。剩下的Memory这个好理解，就是需要有记忆的能力，包括了长期记忆和短期记忆，短期记忆可以理解为上下文记忆，就像你打开一个ChatGPT的对话窗口，这个就是个短期记忆，换个窗口记忆就清楚了。长期记忆一般是通过一些其他的方式，比如数据库做存储，在每次对话前先让模型读取这些信息，作为长期记忆。还有一个Plan的功能，这其实是在Agent有了记忆，会了工具之后，还需要他知道什么时候该调用哪些工具，这就是所谓的规划的能力。那么总结下，Agent=LLM+Memory+Tools（使用+规划）基于以上介绍，差不多就能总结出一个Agent具备的能力。主要包括了： 感知（Perception）： Agent能够接收来自环境的输入信息，包括用户输入的问题，以及Memory。 决策（Decision-making）： Agent根据感知到的信息和内部状态，选择合适（Planning）的行动（包包括Tools）。 行动（Action）： Agent执行所选的行为，以实现特定目标。"},{"title":"什么是MCP","path":"/2025/11/06/什么是MCP/","content":"MCP和Function Calling一样，都是让大模型会用工具的一种手段。全称是Model Context Protocol，顾名思义，它是一种协议，只要每一个MCP Server（工具）都遵守这个协议，那么大模型就可以直接使用这些工具了，而不需要像function calling一样，要写一大堆的适配代码（提示词）。就像下面这张图一样，他就像一个USB的规范一样，只要大家都遵守，就能一起玩。 Anthropic 2024年年底推出的，就是那个研发了Claude模型的公司，他们制定的这个规范，后来因为大名鼎鼎的Cursor开始支持了，慢慢的就火起来了，后来OpenAI也不得不支持了。现在还是比较火的。有了MCP之后，大模型就不再需要为每个数据源或工具单独开发接口，开发者只需遵循MCP规范，即可快速集成各种工具。 MCP中的核心三个组件 MCP Hosts：如Claude Desktop或IDE（比如Cursor），作为AI应用的入口，发起数据请求 MCP Servers：轻量级服务，负责对接具体数据源或工具（如GitHub API、本地文件系统），提供标准化接口。（一般是别人开发好的，你要用的工具） MCP Clients：协议客户端，维护与服务器的连接并转发请求。 有了MCP之后，当用户提出一个问题时，就是大致下面的流程： 客户端（Claude Desktop &#x2F; Cursor）将你的问题发送给大模型（如Claude）。 Claude 分析可用的工具，并决定使用哪一个（或多个）。 客户端通过 MCP Server 执行所选的工具。 工具的执行结果被送回给 Claude。 Claude 结合执行结果构造最终的 prompt 并生成自然语言的回应。 回应最终展示给用户"},{"title":"什么是Function Calling","path":"/2025/11/06/什么是Functing Calling/","content":"Function Calling都是一种让大模型会使用工具的方案。如果一个大模型不会用工具，那就只能是一个简单的对话机器人，并且只能根据以往训练的数据进行对话。如果你想让给大模型能够帮你联网查询、帮你操作本地文件、帮你调外部服务，都需要让他会用工具，而Function Call，MCP、A2A都是可以让大模型更好的使用工具的技术方案。Function Call是Open AI提出的，最开始时只针对自家的GPT用的，他需要先通过结构化的方式定义出来有哪些工具，如： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990def get_current_temperature(location: str, unit: str = &quot;celsius&quot;): &quot;&quot;&quot;Get current temperature at a location. Args: location: The location to get the temperature for, in the format &quot;City, State, Country&quot;. unit: The unit to return the temperature in. Defaults to &quot;celsius&quot;. (choices: [&quot;celsius&quot;, &quot;fahrenheit&quot;]) Returns: the temperature, the location, and the unit in a dict &quot;&quot;&quot; return &#123; &quot;temperature&quot;: 26.1, &quot;location&quot;: location, &quot;unit&quot;: unit, &#125;def get_temperature_date(location: str, date: str, unit: str = &quot;celsius&quot;): &quot;&quot;&quot;Get temperature at a location and date. Args: location: The location to get the temperature for, in the format &quot;City, State, Country&quot;. date: The date to get the temperature for, in the format &quot;Year-Month-Day&quot;. unit: The unit to return the temperature in. Defaults to &quot;celsius&quot;. (choices: [&quot;celsius&quot;, &quot;fahrenheit&quot;]) Returns: the temperature, the location, the date and the unit in a dict &quot;&quot;&quot; return &#123; &quot;temperature&quot;: 25.9, &quot;location&quot;: location, &quot;date&quot;: date, &quot;unit&quot;: unit, &#125;#不想用上面方式也可以直接定义如下TOOLS = [ &#123; &quot;type&quot;: &quot;function&quot;, &quot;function&quot;: &#123; &quot;name&quot;: &quot;get_current_temperature&quot;, &quot;description&quot;: &quot;Get current temperature at a location.&quot;, &quot;parameters&quot;: &#123; &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: &#123; &quot;location&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;description&quot;: &#x27;The location to get the temperature for, in the format &quot;City, State, Country&quot;.&#x27;, &#125;, &quot;unit&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;enum&quot;: [&quot;celsius&quot;, &quot;fahrenheit&quot;], &quot;description&quot;: &#x27;The unit to return the temperature in. Defaults to &quot;celsius&quot;.&#x27;, &#125;, &#125;, &quot;required&quot;: [&quot;location&quot;], &#125;, &#125;, &#125;, &#123; &quot;type&quot;: &quot;function&quot;, &quot;function&quot;: &#123; &quot;name&quot;: &quot;get_temperature_date&quot;, &quot;description&quot;: &quot;Get temperature at a location and date.&quot;, &quot;parameters&quot;: &#123; &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: &#123; &quot;location&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;description&quot;: &#x27;The location to get the temperature for, in the format &quot;City, State, Country&quot;.&#x27;, &#125;, &quot;date&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;description&quot;: &#x27;The date to get the temperature for, in the format &quot;Year-Month-Day&quot;.&#x27;, &#125;, &quot;unit&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;enum&quot;: [&quot;celsius&quot;, &quot;fahrenheit&quot;], &quot;description&quot;: &#x27;The unit to return the temperature in. Defaults to &quot;celsius&quot;.&#x27;, &#125;, &#125;, &quot;required&quot;: [&quot;location&quot;, &quot;date&quot;], &#125;, &#125;, &#125;,]tools = [get_current_temperature, get_temperature_date]messages = [ &#123;&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant.&quot;&#125;, &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;What&#x27;s the temperature in San Francisco now? How about tomorrow?&quot;&#125;] 其中的TOOLS部分就是关于工具的定义，对于每个工具，它是一个具有两个字段的JSON object： type：string，用于指定工具类型，目前仅“function”有效 function：object，详细说明了如何使用该函数 对于每个function，它是一个具有三个字段的JSON object： name：string表示函数名称 description：string描述函数用途 parameters：JSON Schema，用于指定函数接受的参数。请参阅链接文档以了解如何构建JSON Schema。值得注意的字段包括type、required和enum 大多数框架使用“工具”格式，有些可能使用“函数”格式。根据命名，应该很明显使用哪一个。定义好了工具之后，再向他提问的时候，将我们的prompts和上面定义的可用的工具都传给LLM，那么他就能根据用户的问题，选择工具去使用，更好的做回答了。"},{"title":"RAG","path":"/2025/11/06/RAG/","content":"RAG全称是：Retrieval-Augmented Generation，翻译成中文就是检索增强生成。说白了就是让大语言模型（ChatGPT）在“生成答案”之前，先去找资料（检索）来增强它的知识，再用这些资料来生成更准确的答案。 为什么需要RAG因为对于很多大语言模型来说，他的知识是基于历史数据训练出来的，比如GPT-4是截止到2023年的数据，而在这之后发生的所有的新的事件，新的数据，他都是不知道的，那么他的回答就会有这部分的局限性。还有就是，很多大模型是基于公开的资料训练出来的，而很多私域的信息他是没有学习过的，而很多知识是私有的知识，这就需要通过资料的方式增强他原来不熟悉的知识。所以，有了RAG之后，就可以基于自己的知识构建自己的知识库，这样就能做到知识的更新和迭代，也能弥补大模型不知道一些特性领域的专业知识的不足。这样就能让大模型的回答更加的准确， 减少幻觉的发生。 如何构建一个RAG可以看一下这张图（这张图是我从网上找到的），这里面就包括了构建RAG的主要流程。 前置准备首先我们需要做数据准备，把你要用的资料收集好，比如：公司内部文档（PDF、Word、Markdown）、FAQ列表、产品手册等，然后清洗这些数据，比如去掉无关信息、切分成合理的小段。然后把每一小段文本用Embedding模型转成向量，把这些向量存到向量数据库里，比如FAISS、Milvus等。 检索查询当用户提问时，先用相同的Embedding模型把问题也转成向量。然后在向量数据库里用向量相似度搜索，找出最相关的几段资料（比如Top 5）。这些找到的内容就是上下文增强材料 生成回答紧接着，就可以把用户的问题 + 检索到的资料一起，作为Prompt发给大语言模型（LLM）。 这样可以保证模型只在资料范围内生成答案，降低幻觉。"},{"title":"大模型产生幻觉的原因","path":"/2025/11/06/大模型产生幻觉的原因/","content":"大模型的“幻觉”指的是 AI 生成了看似合理但实际上错误或编造的信息。例如，它可能会编造不存在的事实、错误引用文献、甚至捏造公司或人物的信息。 幻觉产生原因语言模型的“填空”机制 Transformer语言模型本质上是一个“填空预测器”，它是根据概率预测来选择下一个输出的词，而不是在“思考”正确答案。 训练数据存在缺陷 训练数据本身可能包含错误信息、不完整数据、偏见信息，导致模型学到不真实的内容。 训练数据可能过时，例如，GPT-4的数据截止到2023年初，无法回答最新时事。 缺乏事实验证能力 语言模型在生成文本时，并不会主动去查证答案的真实性 长文本记忆力有限 由于上下文窗口有限（如 GPT-4-turbo约128k tokens），当文本过长时，AI可能遗忘前面提到的信息。 解决幻觉的几个方案 RAG，通过RAG的方式，让大模型在回答问题之前先检索真实数据，再让模型进行回答。 Fine-tuning，即微调，通过微调的方式，给模型学习专业领域的知识，让他更好的回答。 限制AI的回答，比如在提示词中告诉他如果你不知道，直接就回答不知道。 通过标注和反馈不断优化模型。并把反馈可以给到模型让模型调整。 让同一个模型多次生成同一个内容的答案，然后选择一个最终版本。 在问题回答之后，让 AI 自己检查自己的答案，并标记不确定的部分。 联网， 在 AI 生成答案之前，先通过网络查询最新数据。 让 AI先写下推理过程，再得出最终结论，而不是直接给出答案。"},{"title":"Innodb事务更新过程","path":"/2025/03/16/Innodb事务更新过程/","content":"InnoDB事务更新过程 在Buffer Pool中读取数据：当InnoDB需要更新一条记录时，首先会在Buffer Pool中查找该记录是否在内存中。如果没有在内存中，则从磁盘读取该页到Buffer Pool中。 记录UndoLog：在修改操作前，InnoDB会在Undo Log中记录修改前的数据。Undo Log是用来保证事务原子性和一致性的一种机制，用于在发生事务回滚等情况时，将修改操作回滚到修改前的状态，以达到事务的原子性和一致性。UndoLog的写入最开始写到内存中的，然后由1个后台线程定时刷新到磁盘中的。 在Buffer Pool中更新：当执行update语句时，InnoDB会先更新已经读取到Buffer Pool中的数据，而不是直接写入磁盘。同时，InnoDB会将修改后的数据页状态设置为“脏页”（Dirty Page）状态，表示该页已经被修改但尚未写入磁盘。 记录RedoLog Buffer：InnoDB在Buffer Pool中记录修改操作的同时，InnoDB 会先将修改操作写入到 redo log buffer 中。 提交事务：在执行完所有修改操作后，事务被提交。在提交事务时，InnoDB会将Redo Log写入磁盘，以保证事务持久性。 写入磁盘：在提交过程后，InnoDB会将Buffer Pool中的脏页写入磁盘，以保证数据的持久性。但是这个写入过程并不是立即执行的，是有一个后台线程异步执行的，所以可能会延迟写入，总之就是MYSQL会选择合适的时机把数据写入磁盘做持久化。 记录Binlog：在提交过程中，InnoDB会将事务提交的信息记录到Binlog中。Binlog是MySQL用来实现主从复制的一种机制，用于将主库上的事务同步到从库上。在Binlog中记录的信息包括：事务开始的时间、数据库名、表名、事务ID、SQL语句等。 二阶段提交所谓的MySQL事务的2阶段提交，其实是在更新过程中，保证binlog和redolog一致性的一种手段。 上图中右侧部分即为2阶段提交。他的过程是： Prepare 阶段这个阶段SQL已经成功执行并生成 redolog并写入磁盘，处于prepare阶段 BinLog持久化binlog提交，将binlog内存日志写入磁盘 Commit在执行引擎内部执行事务操作，写入redolog，处于Commit阶段 为什么这个过程需要用2阶段提交的方式呢？假设我们执行一条SQL语句，修改他的name为ssh： update user set name &#x3D; ‘ssh’ where id &#x3D; 1 假设先写入redolog成功，但是没来得及写入binlog，系统崩了。在MySQL重启后，可以根据redolog把记录更新成’ssh’，但是，binlog由于没写成功，所以他是没有记录下来这次变更的，那么也就意味着，主备同步的时候，是缺了一条SQL的，导致主备库之间数据不一致。 那么，如果换个顺序，先写入binlog成功，但是没来及的写入redolog，系统崩了。在MySQL重启之后，崩溃恢复的时候由于redolog还没写，所以什么都不用做，数据库记录还是旧值。但是因为binlog已经写入成功了，所以在做主备同步的时候，就会把新值同步到备库，就导致了主备库之间数据不一致。 如上面的例子，如果不引入二阶段提交的话，在binlog和redolog没办法保证一致性的情况下，就会导致主备库之间的数据不一致。 而为了解决这个问题，那就引入了2阶段提交，来整体的控制redolog和binlog的一致性写入。 2阶段如何保证一致性的？ 情况一：一阶段提交之后崩溃了，即写入redolog，处于prepare状态的时候崩溃了 此时已经写了redolog，处于prepare状态，binlog还没写。这时候如果崩溃恢复，直接回滚事务即可，这样主备是一致的，就都没有执行这个事务。 情况二：一阶段提交成功，写完binlog之后崩溃了 此时，redolog处于prepare状态，binlog已写入，这时候检查 binlog 中的事务是否存在并且完整，如果存在且完整，则直接提交事务，如果不存在或者不完整，则回滚事务。 情况三：假设redolog处于commit状态的时候崩溃了，那么重启后的处理方案同情况二。 由此可见，两阶段提交能够确保数据的一致性。 如何判断binlog和redolog达成一致了？当MySQL写完redolog并将它标记为prepare状态时，并且会在redolog中记录一个XID，它全局唯一的标识着这个事务。而当你设置sync_binlog&#x3D;1时，做完了上面第一阶段写redolog后，mysql就会对应binlog并且会直接将其刷新到磁盘中。 下图就是磁盘上的row格式的binlog记录。binlog结束的位置上也有一个XID。 只要这个XID和redolog中记录的XID是一致的，MySQL就会认为binlog和redolog逻辑上是一致的。"},{"title":"Innodb的RR到底有没有解决幻读","path":"/2025/03/12/Innodb的RR到底有没有解决幻读/","content":"InnoDB中的REPEATABLE READ这种隔离级别通过间隙锁+MVCC解决了大部分的幻读问题，但是并不是所有的幻读都能解读，想要彻底解决幻读，需要使用Serializable的隔离级别。 RR中，通过间隙锁解决了部分当前读的幻读问题，通过增加间隙锁将记录之间的间隙锁住，避免新的数据插入。 RR中，通过MVCC机制的，解决了快照读的幻读问题，RR中的快照读只有第一次会进行数据查询，后面都是直接读取快照，所以不会发生幻读。 但是，如果两个事务，事务1先进行快照读，然后事务2插入了一条记录并提交，再在事务1中进行update新插入的这条记录是可以更新成功的，这就是发生了幻读。 还有一种场景，如果两个事务，事务1先进行快照读，然后事务2插入了一条记录并提交，在事务1中进行了当前读之后，再进行快照读也会发生幻读。 MVCC解决幻读MVCC，是Multiversion Concurrency Control的缩写，翻译过来是多版本并发控制，和数据库锁一样，他也是一种并发控制的解决方案。它主要用来解决读-写并发的情况。 我们知道，在MVCC中有两种读，一种是快照读、一种是当前读。 所谓快照读，就是读取的是快照数据，即快照生成的那一刻的数据，像我们常用的普通的SELECT语句在不加锁情况下就是快照读。 SELECT * FROM xx_table WHERE … 在RC中，每次读取都会重新生成一个快照，总是读取行的最新版本。在RR中，快照会在事务中第一次SELECT语句执行时生成，只有在本事务中对数据进行更改才会更新快照。 那么也就是说，如果在RR下，一个事务中的多次查询，是不会查询到其他的事务中的变更内容的，所以，也就是可以解决幻读的。 如果我们把事务隔离级别设置为RR，那么因为有了MVCC的机制，就能解决幻读的问题： 有这样一张表： 1234567891011CREATE TABLE users ( id INT UNSIGNED AUTO_INCREMENT, gmt_create DATETIME NOT NULL, age INT NOT NULL, name VARCHAR(16) NOT NULL, PRIMARY KEY (id)) ENGINE=InnoDB;INSERT INTO users(gmt_create,age,name) values(now(),18,&#x27;Hollis&#x27;);INSERT INTO users(gmt_create,age,name) values(now(),18,&#x27;Hollis&#x27;);INSERT INTO users(gmt_create,age,name) values(now(),38,&#x27;Hollis666&#x27;); 执行如下事务时序： 可以看到，同一个事务中的两次查询结果是一样的，就是在RR级别下，因为有快照读，所以第二次查询其实读取的是一个快照数据。 间隙锁与幻读上面我们讲过了MVCC能解决RR级别下面的快照读的幻读问题，那么当前读下面的幻读问题怎么解决呢？ 当前读就是读取最新数据，所以，加锁的SELECT，或者对数据进行增删改都会进行当前读，比如： 12345SELECT * FROM xx_table LOCK IN SHARE MODE;SELECT * FROM xx_table FOR UPDATE;INSERT INTO xx_table ...DELETE FROM xx_table ...UPDATE xx_table ... 举一个下面的例子： 像上面这种情况，在RR的级别下，当我们使用SELECT … FOR UPDATE的时候，会进行加锁，不仅仅会对行记录进行加锁，还会对记录之间的间隙进行加锁，这就叫做间隙锁。因为记录之间的间隙被锁住了，所以事务2的插入操作就被阻塞了，一直到事务1把锁释放掉他才能执行成功。因为事务2无法插入数据成功，所以也就不会存在幻读的现象了。所以，在RR级别中，通过加入间隙锁的方式，就避免了幻读现象的发生。 解决不了的幻读前面我们介绍了快照读（无锁查询）和当前读（有锁查询）下是如何解决幻读的问题的，但是，上面的例子就是幻读的所有情况了吗？显然并不是。 我们说MVCC只能解决快照读的幻读，那如果在一个事务中发生了当前读，并且在另一个事务插入数据前没来得及加间隙锁的话，会发生什么呢？ 那么，我们稍加修改一下上面的SQL代码，通过当前读的方式进行查询数据： 在上面的例子中，在事务1中，我们并没有在事务开启后立即加锁，而是进行了一次普通的查询，然后事务2插入数据成功之后，再通过事务1进行了2次查询。 我们发现，事务1后面的两次查询结果完全不一样，没加锁的情况下，就是快照读，读到的数据就和第一次查询是一样的，就不会发生幻读。但是第二次查询加了锁，就是当前读，那么读取到的数据就有其他事务提交的数据了，就发生了幻读。 那么，如果你理解了上面的这个例子，并且你也理解了当前读的概念，那么你很容易就能想到，下面的这个CASE其实也是会发生幻读的： 这里发生幻读的原理，和上面的例子其实是一样的，那就是MVCC只能解决快照读中的幻读问题，而对于当前读（SELECT FOR UPDATE、UPDATE、DELETE等操作）还是会产生幻读的现象的。即，在同一个事务里面，如果既有快照读，又有当前读，那是会产生幻读的、 UPDATE语句也是一种当前读，所以它是可以读到其他事务的提交结果的。 为什么事务1的最后一次查询和倒数第二次查询的结果也不一样呢？ 是因为根据快照读的定义，在RR中，如果本事务中发生了数据的修改，那么就会更新快照，那么最后一次查询的结果也就发生了变化。 如何避免幻读那么了解了幻读的解决场景，以及不能解决的几个CASE之后，我们来总结一下该如何解决幻读的问题呢？ 首先，如果想要彻底解决幻读的问题，在InnoDB中只能使用Serializable这种隔离级别。 那么，如果想在一定程度上解决或者避免发生幻读的话，使用RR也可以，但是RC、RU肯定是不行的。 在RR级别中，能使用快照读（无锁查询）的就使用快照读，这样不仅可以减少锁冲突，提升并发度，而且还能避免幻读的发生。 那么，如果在并发场景中，一定要加锁的话怎么办呢？那就一定要在事务一开始就立即加锁，这样就会有间隙锁，也能有效的避免幻读的发生。 但是需要注意的是，间隙锁是导致死锁的一个重要根源~所以，用起来也需要慎重。"},{"title":"JDK21虚拟线程","path":"/2025/02/04/JDK21虚拟线程/","content":"传统线程在以前的JDK中，Java的线程模型其实比较简单，在大多数操作系统中，主要采用的是基于轻量级进程实现的一对一(1:1)的线程模型，简单来说就是每一个Java线程对应一个操作系统中的轻量级进程，这种线程模型中的线程创建、析构及同步等动作，都需要进行系统调用。而系统调用则需要在用户态（User Mode）和内核态（Kernel Mode）中来回切换，所以性能开销很大，这严重限制了应用程序的扩展性。为了解决这个问题，一些编程语言采用了更轻量级的并发原语-协程。比如 Go 语言的 goroutine、Python 的 asyncio，它们都能以极低的资源消耗支持大规模并发。而在 Java 世界中，随着 JDK 21 的发布，虚拟线程（Virtual Thread）终于正式成为 Java 平台的一部分。虚拟线程是一种轻量级的线程实现，它由 JVM 而不是操作系统来调度。通过将大量虚拟线程复用在少量操作系统线程上，通过有效的调度来避免那些上下文切换，使得 JVM 也可以支持数百万并发任务，同时保持了传统线程编程模型的简单性。这不仅大大提升了 Java 在高并发场景下的性能，也为构建现代云原生应用提供了更好的支持。 传统线程核心痛点 资源消耗巨大，每个平台线程都需要独立的栈空间（默认 1MB 以上）和内核资源，这导致一个进程能创建的线程数量非常有限。在常规服务器上，最多只能创建几千个平台线程，超过这个数量就会出现 OOM（内存溢出）或系统崩溃。 调度成本高昂，操作系统调度线程时需要在用户态和内核态之间切换（上下文切换），每次切换耗时约 1-10 微秒。在高并发场景下，大量线程的频繁切换会严重消耗 CPU 资源，导致 “线程调度 overhead 超过实际任务执行时间” 的尴尬局面。 并发编程门槛高，为了规避资源限制，开发者必须使用线程池手动控制线程数量，但线程池参数（核心线程数、最大线程数、队列大小）的调优极其复杂，稍有不慎就会出现 “线程耗尽” 或 “资源浪费” 的问题。 虚拟线程虚拟线程是 Java 21（JEP 444）引入的轻量级线程，基于 Project Loom 开发，旨在解决传统线程的局限。其核心理念是将线程从操作系统层面解耦，它不直接映射到操作系统内核线程，而是通过 多对多（M:N） 的方式映射到少量平台线程上：多个虚拟线程可以共享一个平台线程，由 JVM 负责在用户态完成调度，提供以下特性： 轻量级：虚拟线程仅占用 2KB 内存，创建成本极低。 高并发：支持百万级并发线程，适合高吞吐场景。 简化编程：延续熟悉的线程模型，无需复杂异步代码。 高效调度：由 JVM 而不是操作系统负责调度，避免了昂贵的内核态上下文切换。 虚拟线程核心优势 极致轻量化 初始栈大小仅几百字节（平台线程是 MB 级） 支持创建百万级甚至千万级线程（平台线程仅支持几千级） 线程创建成本降低 99% 以上 智能调度机制，当虚拟线程执行阻塞操作（如网络 IO、文件读写）时，JVM 会自动将其 “挂起”，并将底层平台线程让给其他虚拟线程使用。这种 “阻塞即让出” 的特性，彻底解决了传统线程 “阻塞时浪费资源” 的问题。 兼容现有代码，虚拟线程完全兼容Thread、Runnable、ExecutorService等现有并发 API，无需学习新框架就能上手，这意味着你可以用同步代码的写法，实现异步代码的性能。 对比 特性 传统线程 虚拟线程 底层映射关系 1:1（Java 线程→操作系统线程） M:N（多个虚拟线程→少量操作系统线程） 资源占用 高（栈内存固定 1MB+） 低（栈内存动态分配，初始仅几百字节） 最大并发支持 几千个线程 百万级线程 调度方 操作系统内核（内核态调度） JVM（用户态调度） 上下文切换成本 高（约 1-10 微秒，需内核态切换） 低（约 0.1 微秒，用户态内完成） 阻塞时资源利用率 低（线程阻塞时仍占用操作系统线程） 高（阻塞时自动释放底层线程资源） 适用场景 计算密集型任务 IO 密集型任务（网络、数据库、文件 IO） 创建成本 高（需操作系统调用） 极低（JVM 直接创建） 虚拟线程工作原理 虚拟线程的工作原理主要包含三个关键部分： 调度机制 JVM维护一个载体线程池（通常与CPU核心数相当） 虚拟线程通过调度器动态分配到载体线程 调度器负责虚拟线程的挂载和卸载 状态管理 新建：创建虚拟线程 就绪：等待被调度 运行：在载体线程上执行 阻塞：等待IO或同步操作 终止：执行完成 执行流程 挂载：获取载体线程 执行：运行业务代码 卸载：遇到阻塞时保存状态 恢复：阻塞结束后重新调度 代码12345678910111213141516171819202122232425import java.util.concurrent.Executors;public class Test &#123; public static void main(String[] args) &#123; // 创建虚拟线程 Thread.ofVirtual().name(&quot;virtual-thread-1&quot;).start(() -&gt; &#123; // 虚拟线程执行的代码 &#125;); // 创建并启动虚拟线程 Thread vThread = Thread.startVirtualThread(() -&gt; &#123; // 虚拟线程执行的代码 &#125;); // 使用 Executors.newVirtualThreadPerTaskExecutor() 创建虚拟线程池，注意只有明确需要限制并发数量时才建议使用虚拟线程池。 try (var executor = Executors.newVirtualThreadPerTaskExecutor()) &#123; executor.submit(() -&gt; &#123; // 任务逻辑 &#125;); &#125; &#125;&#125; 使用注意事项 虚拟线程不要和线程池一起使用在JEP425中，关于虚拟线程的介绍中，多次提到，不要将虚拟线程池化。也就是说，不要把虚拟线程和线程池合着用。 • 线程池的设计初衷是复用线程，因为传统线程的创建和销毁开销很大，但是虚拟线程的创建和销毁开销很小，所以自然没必要进行池化。• 使用线程池反而会限制并发数，限制了虚拟线程轻量级、高并发的优势。• 可能导致线程饥饿或死锁 避免使用ThreadLocal • 虚拟线程会频繁切换载体线程，ThreadLocal 的值可能会丢失或混乱，建议使用 ScopedValue。• 会导致内存泄漏，因为虚拟线程数量可能非常大。 避免使用同步块或重量级锁（jdk25版本有优化） 123456789101112131415// x 不推荐的做法public class SynchronizedExample &#123; private final Object lock = new Object(); private int counter = 0; public void increment() &#123; try (var executor = Executors.newVirtualThreadPerTaskExecutor()) &#123; executor.submit(() -&gt; &#123; synchronized (lock) &#123; // 使用同步块会阻塞载体线程 counter++; &#125; &#125;); &#125; &#125;&#125; 12345678910111213// √ 推荐的做法public class LockFreeExample &#123; private final AtomicInteger counter = new AtomicInteger(0); public void increment() &#123; try (var executor = Executors.newVirtualThreadPerTaskExecutor()) &#123; executor.submit(() -&gt; &#123; counter.incrementAndGet(); // 使用无锁数据结构 &#125;); &#125; &#125;&#125; • 同步块会导致载体线程被阻塞，降低性能。• 可能导致其他虚拟线程无法执行• 应该使用无锁数据结构或更轻量级的并发控制机制 避免固定虚拟线程到载体线程1234567891011// x 不推荐的做法public class PinnedExample &#123; public void doWork() &#123; Thread vThread= Thread.ofVirtual().start(() -&gt; &#123; // 这些操作会导致虚拟线程被固定到载体线程 synchronized (this) &#123; heavyComputation(); &#125; &#125;); &#125;&#125; 123456789// √ 推荐的做法public class NonPinnedExample &#123; public void doWork() &#123; Thread vThread = Thread.ofVirtual().start(() -&gt; &#123; // 使用非阻塞操作或轻量级同步机制 CompletableFuture.runAsync(this::heavyComputation); &#125;); &#125;&#125; • 固定（pinning）会导致虚拟线程失去其轻量级特性• 会降低整体性能和可伸缩性• 应该尽量使用非阻塞操作或异步方式 总结这些注意事项的核心是：保持虚拟线程的轻量级特性，避免池化，避免使用会导致阻塞（例如synchronized）或固定（pinning）的操作，合理管理资源使用。遵循这些原则，才能充分发挥虚拟线程的优势。其实，虚拟线程的出现一部分原因也是为了让并发编程更加简单，让我们可以像编写普通代码一样编写多线程代码。"},{"title":"Jackson","path":"/2024/11/23/Jackson/","content":"Jackson 简介Jackson 是一个用于在Java对象和 json 数据之间进行转换的框架。它支持众多 json 数据格式，诸如 JSON-LD、SMILE 等。Jackson是一个流式处理 JSON 的工具，它很容易与其它 JSON 处理工具进行集成，同时在序列化和反序列化性能方面也表现得相当优异。 核心包 jackson-databind：提供了通用的数据绑定功能（将Java对象与JSON数据相互转换） jackson-core：提供了核心的低级JSON处理API（例如JsonParser和JsonGenerator） jackson-annotations：提供了用于配置数据绑定的注解 为什么选择Jackson尽管Java生态系统中有其他处理JSON数据的库（如Gson和JSON-java），但Jackson仍然是许多开发者的首选，原因包括： 性能：Jackson性能优越，对内存和CPU的使用都相对较低。许多性能基准测试表明，Jackson在序列化和反序列化方面都比其他库更快。 功能丰富：Jackson提供了许多功能，包括注解、自定义序列化和反序列化、动态解析等，使其非常灵活和强大。 易于使用：Jackson的API简单易用，使得开发者可以轻松地在他们的应用程序中集成和使用。 社区支持：Jackson拥有庞大的开发者社区，这意味着有更多的文档、教程和问题解答可供参考。 模块化：Jackson支持通过模块扩展其功能，例如Java 8时间库、Joda-Time和Kotlin等。 兼容性：Jackson可以很好地与其他流行的Java框架（如Spring）集成。 将Java对象转换为JSON字符串（序列化）序列化是将Java对象转换为JSON字符串的过程。这在许多场景中非常有用，例如在将数据发送到Web客户端时，或者在将数据存储到文件或数据库时。Jackson通过ObjectMapper类来实现序列化。以下是一个简单的示例： 123456789101112131415161718192021public class Person &#123; public String name; public int age; public Person(String name, int age) &#123; this.name = name; this.age = age; &#125; public static void main(String[] args) &#123; ObjectMapper objectMapper = new ObjectMapper(); Person person = new Person(&quot;Alice&quot;, 30); try &#123; String jsonString = objectMapper.writeValueAsString(person); System.out.println(jsonString); // 输出：&#123;&quot;name&quot;:&quot;Alice&quot;,&quot;age&quot;:30&#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 将JSON字符串转换为Java对象（反序列化）反序列化是将JSON字符串转换回Java对象的过程。这在从Web客户端接收数据或从文件或数据库读取数据时非常有用。同样，Jackson使用ObjectMapper类来实现反序列化。以下是一个简单的示例： 12345678910111213141516171819public class Person &#123; public String name; public int age; public Person() &#123; &#125; public static void main(String[] args) &#123; ObjectMapper objectMapper = new ObjectMapper(); String jsonString = &quot;&#123;\\&quot;name\\&quot;:\\&quot;Alice\\&quot;,\\&quot;age\\&quot;:30&#125;&quot;; try &#123; Person person = objectMapper.readValue(jsonString, Person.class); System.out.println(&quot;Name: &quot; + person.name + &quot;, Age: &quot; + person.age); // 输出：Name: Alice, Age: 30 &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 这些示例展示了Jackson库的基本功能，由于Jackson库的API非常多，以下是Jackson库的一些主要API和组件： ObjectMapper：这是Jackson库的核心类，用于序列化和反序列化操作。主要方法有：writeValueAsString(Object)：将Java对象序列化为JSON字符串。readValue(String, Class)：将JSON字符串反序列化为Java对象。 JsonParser：用于从JSON数据源（如文件、输入流或字符串）解析JSON数据。主要方法有：nextToken()：获取下一个JSON令牌（如START_OBJECT、FIELD_NAME等）。getValueAsString()：将当前令牌作为字符串返回。getValueAsInt()：将当前令牌作为整数返回。 JsonGenerator：用于将JSON数据写入数据源（如文件、输出流或字符串缓冲区）。主要方法有：writeStartObject()：写入开始对象标记（{）。writeFieldName(String)：写入字段名称。writeString(String)：写入字符串值。writeEndObject()：写入结束对象标记（}）。 JsonNode：用于表示JSON树模型中的节点，可以是对象、数组、字符串、数字等。主要方法有：get(String)：获取指定字段的子节点。path(String)：获取指定字段的子节点，如果不存在则返回一个“missing”节点。isObject()：检查当前节点是否是一个对象。isArray()：检查当前节点是否是一个数组。 注解：Jackson提供了一系列注解来配置序列化和反序列化过程。一些常用注解包括：@JsonProperty：指定字段在JSON数据中的名称。@JsonIgnore：指定字段在序列化和反序列化过程中被忽略。@JsonCreator：指定用于反序列化的构造函数或工厂方法。@JsonSerialize：指定用于序列化特定字段或类的自定义序列化器。@JsonDeserialize：指定用于反序列化特定字段或类的自定义反序列化器。 注解（如@JsonProperty, @JsonIgnore）Jackson库提供了一系列注解，可以帮助你在序列化和反序列化过程中对字段和类进行配置。以下是一些常用注解的示例： @JsonProperty 注解：该注解用于指定 Java 属性与 JSON 属性之间的映射关系，常用的参数有：value：用于指定 JSON 属性的名称，当 Java 属性和 JSON 属性名称不一致时使用。access：用于指定该属性的访问方式，常用的取值有 JsonAccess.READ_ONLY（只读），JsonAccess.WRITE_ONLY（只写）和 JsonAccess.READ_WRITE（可读可写）。 @JsonIgnore 注解：该注解用于禁用 Java 属性的序列化和反序列化，常用的参数有：无参数。 @JsonFormat 注解：该注解用于指定 Java 属性的日期和时间格式，常用的参数有：shape：用于指定日期和时间的格式，可选的取值有 JsonFormat.Shape.STRING（以字符串形式表示）和 JsonFormat.Shape.NUMBER（以时间戳形式表示）。pattern：用于指定日期和时间的格式模板，例如 “yyyy-MM-dd HH:mm:ss”。 @JsonInclude 注解：该注解用于指定序列化 Java 对象时包含哪些属性，常用的参数有：value：用于指定包含哪些属性，可选的取值有 JsonInclude.Include.ALWAYS（始终包含）、JsonInclude.Include.NON_NULL（值不为 null 时包含）、JsonInclude.Include.NON_DEFAULT（值不为默认值时包含）、JsonInclude.Include.NON_EMPTY（值不为空时包含）和 JsonInclude.Include.CUSTOM（自定义条件）。content：用于指定自定义条件的实现类。 @JsonCreator 注解：该注解用于指定反序列化时使用的构造方法或工厂方法，常用的参数有：无参数。 @JsonSetter 注解：该注解用于指定反序列化时使用的方法，常用的参数有：value：用于指定 JSON 属性的名称，当方法名和 JSON 属性名称不一致时使用。 @JsonGetter 注解：该注解用于指定序列化时使用的方法，常用的参数有：value：用于指定 JSON 属性的名称，当方法名和 JSON 属性名称不一致时使用。 @JsonAnySetter 注解：该注解用于指定反序列化时使用的方法，用于处理 JSON 中未知的属性，常用的参数有：无参数。 @JsonAnyGetter 注解：该注解用于指定序列化时使用的方法，用于处理 Java 对象中未知的属性，常用的参数有：无参数。 @JsonTypeInfo 注解：该注解用于指定 Java 对象在序列化和反序列化时的类型信息，常用的参数有：use：用于指定类型信息的使用方式，可选的取值有 JsonTypeInfo.Id.CLASS（使用 Java 类的全限定名）、JsonTypeInfo.Id.NAME（使用名称）和 JsonTypeInfo.Id.NONE（不使用类型信息）。include：用于指定类型信息的包含方式，可选的取值有 JsonTypeInfo.As.PROPERTY（作为 JSON 属性）和 JsonTypeInfo.As.EXTERNAL_PROPERTY（作为外部属性）。property：用于指定包含类型信息的属性名，当 include 的值为 JsonTypeInfo.As.PROPERTY 时使用。visible：用于指定类型信息是否可见。 其他的JSON库除了Jackson之外，还有其他一些流行的Java JSON处理库。以下是一些常见的库： Gson：Gson是Google开发的一个Java库，用于将Java对象转换为JSON表示以及将JSON字符串转换为等效的Java对象。Gson的API简洁易用，性能也相当不错。官方网站：github.com&#x2F;google&#x2F;gson Fastjson：Fastjson是Alibaba开发的一个高性能的JSON库。Fastjson提供了灵活的API和丰富的功能，同时注重性能优化。然而，它在安全性方面存在一些问题，因此在使用时需要谨慎。官方网站：github.com&#x2F;alibaba&#x2F;fas… JSON-java（org.json）：JSON-java库，也称为org.json库，是一个非常轻量级的JSON处理库。它提供了基本的JSON编码和解码功能，但不支持对象映射等高级功能。官方网站：github.com&#x2F;stleary&#x2F;JSO… Moshi：Moshi是Square公司开发的一个现代化的JSON库，具有简单易用的API和良好的性能。Moshi支持Kotlin协程，并与Kotlin编程语言非常兼容。官方网站：github.com&#x2F;square&#x2F;mosh… Boon：Boon是另一个高性能的JSON处理库。它具有易用的API，支持流式处理和速度优化。然而，Boon的社区和文档相对较少。官方网站：github.com&#x2F;boonproject…"},{"title":"junit5+Mockito单元测试案例","path":"/2024/09/11/junit5/","content":"MockMvc+Controller单元测试12345678910111213141516171819@Autowiredprivate WebApplicationContext wac;private MockMvc mockMvc;private HttpHeaders headers;@BeforeAllvoid setUp() throws Exception &#123; mockMvc = MockMvcBuilders.webAppContextSetup(wac).build(); MultiValueMap&lt;String, String&gt; headerMap = new LinkedMultiValueMap&lt;&gt;(); MvcResult mvcResult = mockMvc.perform(MockMvcRequestBuilders.post(&quot;/user/authentication&quot;) .content(&quot;&#123;\\&quot;username\\&quot;:\\&quot;zhangsan\\&quot;,\\&quot;password\\&quot;:\\&quot;123456\\&quot;&#125;&quot;) .contentType(MediaType.APPLICATION_JSON)).andExpect(MockMvcResultMatchers.status().isOk()).andReturn(); String loginContent = mvcResult.getResponse().getContentAsString(StandardCharsets.UTF_8); headerMap.add(&quot;Authorization&quot;, JSON.parseObject(loginContent).getJSONObject(&quot;data&quot;).getString(&quot;token&quot;)); headers = new HttpHeaders(); headers.addAll(headerMap);&#125; Service层测试+间接注入1"},{"title":"RestTemplate拦截器","path":"/2024/05/25/RestTemplate/","content":"拦截器的使用场景除了修改HTTP头之外，RestTemplate拦截器还可以用于下面的场景： 打印请求和响应日志 用可配置的回滚策略进行重试 基于某些请求参数来拒绝请求 改变请求的URL 创建拦截器Spring RestTemplate允许我们添加实现了ClientHttpRequestInterceptor接口的拦截器。这个接口的intercept(HttpRequest, byte[], ClientHttpRequestExecution)方法将通过让我们访问request、body和execution对象来拦截指定的请求并返回响应。 我们将使用ClientHttpRequestExecution参数来执行实际的操作，并将请求传递给后续的调用链。 改变请求的URL123456789101112131415161718192021222324252627282930public class UrlClientHttpRequestInterceptor implements ClientHttpRequestInterceptor &#123; @Autowired private Environment env; @Override public ClientHttpResponse intercept(HttpRequest request, byte[] body, ClientHttpRequestExecution execution) throws IOException &#123; URI newUri = UriComponentsBuilder.fromUri(request.getURI()) .queryParam(&quot;accessid&quot;, env.getProperty(&quot;accessid&quot;)) .queryParam(&quot;appName&quot;, env.getProperty(&quot;appName&quot;)) .build().toUri(); return execution.execute(new UriModifyHttpRequestWrapper(request, newUri), body); &#125; private static class UriModifyHttpRequestWrapper extends HttpRequestWrapper &#123; private final URI uri; public UriModifyHttpRequestWrapper(HttpRequest request, URI uri) &#123; super(request); this.uri = uri; &#125; @Override public URI getURI() &#123; return uri; &#125; &#125;&#125; 1234567891011121314151617181920212223242526@Configurationpublic class RestTemplateConfig &#123; @Bean public ClientHttpRequestInterceptor serviceAuthClientHttpRequestInterceptor() &#123; return new ServiceAuthClientHttpRequestInterceptor(); &#125; @Bean @LoadBalanced public RestTemplate restTemplate(@Autowired MappingJackson2HttpMessageConverter converter, @Autowired ObjectMapper mapper, @Autowired JacksonProperties properties) &#123; RestTemplate restTemplate = new RestTemplate(); List&lt;ClientHttpRequestInterceptor&gt; interceptors = restTemplate.getInterceptors(); interceptors.add(serviceAuthClientHttpRequestInterceptor()); if (null != properties.getTimeZone()) &#123; mapper.setTimeZone(properties.getTimeZone()); &#125; if (!StringUtils.isNullOrEmpty(properties.getDateFormat())) &#123; mapper.setDateFormat(new SimpleDateFormat(properties.getDateFormat())); &#125; return restTemplate; &#125;&#125; 修改HTTP头1234567891011121314public class RestTemplateHeaderModifierInterceptor implements ClientHttpRequestInterceptor &#123; @Override public ClientHttpResponse intercept( HttpRequest request, byte[] body, ClientHttpRequestExecution execution) throws IOException &#123; ClientHttpResponse response = execution.execute(request, body); response.getHeaders().add(&quot;Foo&quot;, &quot;bar&quot;); return response; &#125;&#125;"},{"title":"dependency-check检查依赖漏洞","path":"/2024/04/05/dependency-check/","content":"dependency-check介绍 Dependency-Check 是 OWASP（Open Web Application Security Project）的一个实用开源程序，用于识别项目依赖项并检查是否存在任何已知的，公开披露的漏洞。目前，已支持Java、.NET、Ruby、Node.js、Python等语言编写的程序，并为C&#x2F;C++构建系统（autoconf和cmake）提供了有限的支持。 Dependency-Check 支持面广（支持多种语言）、可集成性强，作为一款开源工具，在多年来的发展中已经支持和许多主流的软件进行集成，比如：命令行、Ant、Maven、Gradle、Jenkins、Sonar等；具备使用方便，落地简单等优势。 dependency-check官网下载地址: https://owasp.org/www-project-dependency-check NVDDependency-Check依赖NVD漏洞数据库（美国国家通用漏洞数据库）进行依赖漏洞检查（全球信息安全领域著名的漏洞数据库包括中国国家信息安全漏洞库，美国国家信息安全漏洞库NVD，赛门铁克漏洞库等等）官网：https://nvd.nist.gov/ NVD的更新频率是出现问题实时更新，具体链接： https://nvd.nist.gov/general/nvd-dashboard CVSSNVD评级依赖CVSS（CommonVulnerability Scoring System），即“通用漏洞评分系统”，是一个“行业公开标准，其被设计用来评测漏洞的严重程度，并帮助确定所需反应的紧急度和重要度，具体评分标准如下： 目前主要参考cvss v3.0，具体级别的漏洞数目如下图所示： CNNVD官网地址：国家信息安全漏洞库 (cnnvd.org.cn) CNNVD是我们国家的信息安全漏洞库，咱们这个漏洞库不仅仅收录了NVD的漏洞数据（CVE），还收录咱们国内的漏洞信息（CNNVD），也就是说同样一条漏洞信息，CNNVD的漏洞数据上面既有CVE漏洞编码，也有CNNVD编码，而且描述信息还是中文的。获取方式： dependency-check插件扫描在maven的pom.xml的插件配置中添加如下配置内容: 12345678910111213141516171819202122&lt;plugin&gt; &lt;groupId&gt;org.owasp&lt;/groupId&gt; &lt;artifactId&gt;dependency-check-maven&lt;/artifactId&gt; &lt;configuration&gt; &lt;autoUpdate&gt;true&lt;/autoUpdate&gt; &lt;dataDirectory&gt;/Users/sunyucheng/Documents/CVE&lt;/dataDirectory&gt; &lt;versionCheckEnabled&gt;false&lt;/versionCheckEnabled&gt; &lt;retireJsForceUpdate&gt;false&lt;/retireJsForceUpdate&gt; &lt;ossindexAnalyzerUseCache&gt;true&lt;/ossindexAnalyzerUseCache&gt; &lt;skipRuntimeScope&gt;true&lt;/skipRuntimeScope&gt; &lt;skipProvidedScope&gt;true&lt;/skipProvidedScope&gt; &lt;skipSystemScope&gt;true&lt;/skipSystemScope&gt; &lt;skipTestScope&gt;true&lt;/skipTestScope&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;check&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt;&lt;/plugin&gt; 以上配置了插件常用的一些参数，如果需要查看更多参数，可以使用如下命令查看:mvn help:describe -Dcmd&#x3D;dependency-check:check -Ddetail 重点参数解析autoUpdate 如果为true，每次执行漏洞检查时都会下载CVE漏洞数据 如果为false，不会在线更新漏洞数据，相当于离线扫描 dataDirectory这是存放CVE漏洞数据的目录，如果autoUpdate为true，也会在这个目录下更新 也可以参考这个来配置 插件运行 第一次执行的话，速度挺慢的，因为他需要从NIST托管的国家漏洞数据库下载漏洞数据到本地备份库。在IDEA主界面的右边侧边栏上找到Maven模块，在Plugins目录Depedancy-check下双击depedancy-check:check即可成功运作。 相关的结果报告将会输出在该模块的target目录下 dependency-check命令行扫描在官网右边侧栏点击Command Line下载应用到本地即可 执行命令文件解压后，进入bin目录，执行dependency-check.bat或者dependency-check.sh 1dependency-check.sh --disableRetireJS --disableNodeJS --project &quot;第三方依赖CVE漏洞扫描报告&quot; --scan D:\\checkjar\\ -n -f JSON -o D:\\report\\ –project 给这次扫描的项目起个名称–scan 要扫描的jar文件路径，或者要扫描的maven私服库-n 此次扫描不更新漏洞库：不加这个会先去更新漏洞库，然后再进行扫描-f 要输出的报告格式，有HTML ，JSON等，不加默认是html-o 扫描报告的存放路径–disableRetireJS 不检查js–disableNodeJS 不检查nodejs 本地漏洞库搭建NVD+CNNVD待实现"},{"title":"kafka常见问题总结","path":"/2024/03/20/kafka常见问题总结/","content":"kafka消息模型队列模型：早期的消息模型 使用队列（Queue）作为消息通信载体，满足生产者与消费者模式，一条消息只能被一个消费者使用，未被消费的消息在队列中保留直到被消费或超时。 比如：我们生产者发送 100 条消息的话，两个消费者来消费一般情况下两个消费者会按照消息发送的顺序各自消费一半（也就是你一个我一个的消费） 队列模型存在的问题：假如我们存在这样一种情况：我们需要将生产者产生的消息分发给多个消费者，并且每个消费者都能接收到完整的消息内容。 这种情况，队列模型就不好解决了。很多比较杠精的人就说：我们可以为每个消费者创建一个单独的队列，让生产者发送多份。这是一种非常愚蠢的做法，浪费资源不说，还违背了使用消息队列的目的。 发布-订阅模型：kafka消息模型发布-订阅模型主要是为了解决队列模型存在的问题。 发布订阅模型（Pub-Sub） 使用主题（Topic） 作为消息通信载体，类似于广播模式；发布者发布一条消息，该消息通过主题传递给所有的订阅者，在一条消息广播之后才订阅的用户则是收不到该条消息的。 在发布 - 订阅模型中，如果只有一个订阅者，那它和队列模型就基本是一样的了。所以说，发布 - 订阅模型在功能层面上是可以兼容队列模型的。 Kafka 采用的就是发布 - 订阅模型。 RocketMQ 的消息模型和 Kafka 基本是完全一样的。唯一的区别是 Kafka 中没有队列这个概念，与之对应的是 Partition（分区） kafka核心概念什么是 Producer、Consumer、Broker、Topic、Partition？ 上面这张图也为我们引出了，Kafka 比较重要的几个概念： Producer（生产者） : 产生消息的一方。 Consumer（消费者） : 消费消息的一方。 Broker（代理） : 可以看作是一个独立的 Kafka 实例。多个 Kafka Broker 组成一个 Kafka Cluster。 同时，你一定也注意到每个 Broker 中又包含了 Topic 以及 Partition 这两个重要的概念： Topic（主题） : Producer 将消息发送到特定的主题，Consumer 通过订阅特定的 Topic(主题) 来消费消息。 Partition（分区） : Partition 属于 Topic 的一部分。一个 Topic 可以有多个 Partition ，并且同一 Topic 下的 Partition 可以分布在不同的 Broker 上，这也就表明一个 Topic 可以横跨多个 Broker 。这正如我上面所画的图一样。 Kafka 的多副本机制了解吗？带来了什么好处？这点比较重要的是 Kafka 为分区（Partition）引入了多副本（Replica）机制。分区（Partition）中的多个副本之间会有一个叫做 leader 的家伙，其他副本称为 follower。我们发送的消息会被发送到 leader 副本，然后 follower 副本才能从 leader 副本中拉取消息进行同步。 生产者和消费者只与 leader 副本交互。你可以理解为其他副本只是 leader 副本的拷贝，它们的存在只是为了保证消息存储的安全性。当 leader 副本发生故障时会从 follower 中选举出一个 leader,但是 follower 中如果有和 leader 同步程度达不到要求的参加不了 leader 的竞选。 Kafka 的多分区（Partition）以及多副本（Replica）机制有什么好处呢？ Kafka 通过给特定 Topic 指定多个 Partition, 而各个 Partition 可以分布在不同的 Broker 上, 这样便能提供比较好的并发能力（负载均衡）。 Partition 可以指定对应的 Replica 数, 这也极大地提高了消息存储的安全性, 提高了容灾能力，不过也相应的增加了所需要的存储空间。 Kafka 如何保证消息的消费顺序？我们在使用消息队列的过程中经常有业务场景需要严格保证消息的消费顺序，比如我们同时发了 2 个消息，这 2 个消息对应的操作分别对应的数据库操作是： 更改用户会员等级。 根据会员等级计算订单价格。 假如这两条消息的消费顺序不一样造成的最终结果就会截然不同。 我们知道 Kafka 中 Partition(分区)是真正保存消息的地方，我们发送的消息都被放在了这里。而我们的 Partition(分区) 又存在于 Topic(主题) 这个概念中，并且我们可以给特定 Topic 指定多个 Partition。 每次添加消息到 Partition(分区) 的时候都会采用尾加法，如上图所示。 Kafka 只能为我们保证 Partition(分区) 中的消息有序。 消息在被追加到 Partition(分区)的时候都会分配一个特定的偏移量（offset）。Kafka 通过偏移量（offset）来保证消息在分区内的顺序性。 所以，我们就有一种很简单的保证消息消费顺序的方法：1 个 Topic 只对应一个 Partition。这样当然可以解决问题，但是破坏了 Kafka 的设计初衷。 Kafka 中发送 1 条消息的时候，可以指定 topic, partition, key,data（数据） 4 个参数。如果你发送消息的时候指定了 Partition 的话，所有消息都会被发送到指定的 Partition。并且，同一个 key 的消息可以保证只发送到同一个 partition，这个我们可以采用表&#x2F;对象的 id 来作为 key 。 总结一下，对于如何保证 Kafka 中消息消费的顺序，有了下面两种方法： 1 个 Topic 只对应一个 Partition。 （推荐）发送消息的时候指定 key&#x2F;Partition。 当然不仅仅只有上面两种方法，上面两种方法是我觉得比较好理解的 Kafka 如何保证消息不丢失？producer端生产者(Producer) 调用send方法发送消息之后，消息可能因为网络问题并没有发送过去。 所以，我们不能默认在调用send方法发送消息之后消息发送成功了。为了确定消息是发送成功，我们要判断消息发送的结果。但是要注意的是 Kafka 生产者(Producer) 使用 send 方法发送消息实际上是异步的操作，我们可以通过 get()方法获取调用结果，但是这样也让它变为了同步操作，示例代码如下： 详细代码见我的这篇文章：Kafka 系列第三篇！10 分钟学会如何在 Spring Boot 程序中使用 Kafka 作为消息队列? 12345SendResult&lt;String, Object&gt; sendResult = kafkaTemplate.send(topic, o).get();if (sendResult.getRecordMetadata() != null) &#123; logger.info(&quot;生产者成功发送消息到&quot; + sendResult.getProducerRecord().topic() + &quot;-&gt; &quot; + sendRe sult.getProducerRecord().value().toString());&#125; 但是一般不推荐这么做！可以采用为其添加回调函数的形式，示例代码如下： 123ListenableFuture&lt;SendResult&lt;String, Object&gt;&gt; future = kafkaTemplate.send(topic, o); future.addCallback(result -&gt; logger.info(&quot;生产者成功发送消息到topic:&#123;&#125; partition:&#123;&#125;的消息&quot;, result.getRecordMetadata().topic(), result.getRecordMetadata().partition()), ex -&gt; logger.error(&quot;生产者发送消失败，原因：&#123;&#125;&quot;, ex.getMessage())); 如果消息发送失败的话，我们检查失败的原因之后重新发送即可！ 另外，这里推荐为 Producer 的retries（重试次数）设置一个比较合理的值，一般是 3 ，但是为了保证消息不丢失的话一般会设置比较大一点。设置完成之后，当出现网络问题之后能够自动重试消息发送，避免消息丢失。另外，建议还要设置重试间隔，因为间隔太小的话重试的效果就不明显了，网络波动一次你 3 次一下子就重试完了。 consumer端我们知道消息在被追加到 Partition(分区)的时候都会分配一个特定的偏移量（offset）。偏移量（offset)表示 Consumer 当前消费到的 Partition(分区)的所在的位置。Kafka 通过偏移量（offset）可以保证消息在分区内的顺序性。 当消费者拉取到了分区的某个消息之后，消费者会自动提交了 offset。自动提交的话会有一个问题，试想一下，当消费者刚拿到这个消息准备进行真正消费的时候，突然挂掉了，消息实际上并没有被消费，但是 offset 却被自动提交了。 解决办法，我们手动关闭自动提交 offset，每次在真正消费完消息之后再自己手动提交 offset 。 但是，这样会带来消息被重新消费的问题。比如你刚刚消费完消息之后，还没提交 offset，结果自己挂掉了，那么这个消息理论上就会被消费两次，这里只能业务自己保证幂等性。 broker端我们知道 Kafka 为分区（Partition）引入了多副本（Replica）机制。分区（Partition）中的多个副本之间会有一个叫做 leader 的家伙，其他副本称为 follower。我们发送的消息会被发送到 leader 副本，然后 follower 副本才能从 leader 副本中拉取消息进行同步。生产者和消费者只与 leader 副本交互。你可以理解为其他副本只是 leader 副本的拷贝，它们的存在只是为了保证消息存储的安全性。 试想一种情况：假如 leader 副本所在的 broker 突然挂掉，那么就要从 follower 副本重新选出一个 leader ，但是 leader 的数据还有一些没有被 follower 副本的同步的话，就会造成消息丢失。 设置 acks &#x3D; all 解决办法就是我们设置 acks &#x3D; all。acks 是 Kafka 生产者(Producer) 很重要的一个参数。 acks 的默认值即为 1，代表我们的消息被 leader 副本接收之后就算被成功发送。当我们配置 acks &#x3D; all 表示只有所有 ISR 列表的副本全部收到消息时，生产者才会接收到来自服务器的响应. 这种模式是最高级别的，也是最安全的，可以确保不止一个 Broker 接收到了消息. 该模式的延迟会很高. 设置 replication.factor &gt;&#x3D; 3 为了保证 leader 副本能有 follower 副本能同步消息，我们一般会为 topic 设置 replication.factor &gt;&#x3D; 3。这样就可以保证每个 分区(partition) 至少有 3 个副本。虽然造成了数据冗余，但是带来了数据的安全性。 设置 min.insync.replicas &gt; 1 一般情况下我们还需要设置 min.insync.replicas&gt; 1 ，这样配置代表消息至少要被写入到 2 个副本才算是被成功发送。min.insync.replicas 的默认值为 1 ，在实际生产中应尽量避免默认值 1。 但是，为了保证整个 Kafka 服务的高可用性，你需要确保 replication.factor &gt; min.insync.replicas 。为什么呢？设想一下假如两者相等的话，只要是有一个副本挂掉，整个分区就无法正常工作了。这明显违反高可用性！一般推荐设置成 replication.factor &#x3D; min.insync.replicas + 1。 设置 unclean.leader.election.enable &#x3D; false Kafka 0.11.0.0 版本开始 unclean.leader.election.enable 参数的默认值由原来的 true 改为 false 我们最开始也说了我们发送的消息会被发送到 leader 副本，然后 follower 副本才能从 leader 副本中拉取消息进行同步。多个 follower 副本之间的消息同步情况不一样，当我们配置了 unclean.leader.election.enable = false 的话，当 leader 副本发生故障时就不会从 follower 副本中和 leader 同步程度达不到要求的副本中选择出 leader ，这样降低了消息丢失的可能性。 Kafka 如何保证消息不重复消费？kafka 出现消息重复消费的原因： 服务端侧已经消费的数据没有成功提交 offset（根本原因）。 Kafka 侧 由于服务端处理业务时间长或者网络链接等等原因让 Kafka 认为服务假死，触发了分区 rebalance。 解决方案： 消费消息服务做幂等校验，比如 Redis 的 set、MySQL 的主键等天然的幂等功能。这种方法最有效。 将 enable.auto.commit 参数设置为 false，关闭自动提交，开发者在代码中手动提交 offset。那么这里会有个问题：什么时候提交 offset 合适？ 处理完消息再提交：依旧有消息重复消费的风险，和自动提交一样 拉取到消息即提交：会有消息丢失的风险。允许消息延时的场景，一般会采用这种方式。然后，通过定时任务在业务不繁忙（比如凌晨）的时候做数据兜底。"},{"title":"nacos日志不打印","path":"/2024/03/05/nacos日志不打印/","content":"背景项目使用log4j2作为日志框架, 配置一切正常, 程序启动日志的打印也没有问题, 但是引入了nacos作为注册中心后, 日志在打印了一句WARN No Root logger was configured, creating default ERROR-level Root logger with Console appender 后日志都不打印了。 原因在项目的启动过程中, nacos-discovery会尝试去读系统内的日志配置文件, 然后刷新配置, 但是如果找不到的话, 会使用自身内部的默认配置, 但是默认的日志配置没有设置日志的Root的信息。 原因分析 在 com.alibaba.nacos.client.utils.LogUtils 中有一段逻辑如下:123456789101112AbstractNacosLogging nacosLogging;try &#123; Class.forName(&quot;ch.qos.logback.classic.Logger&quot;); nacosLogging = new LogbackNacosLogging(); isLogback = true;&#125; catch (ClassNotFoundException e) &#123; nacosLogging = new Log4J2NacosLogging();&#125;nacosLogging.loadConfiguration();// 从代码可以知道LogUtils, 会尝试加载 `ch.qos.logback.classic.Logger` 这个类, 加载成功的话, 说明项目使用的是 logback 日志框架, 加载不到的话, 就是使用 log4j2(我的项目中使用的是 log4j2) 进入到Log4J2NacosLogging的构造函数12345678910111213141516171819202122232425262728293031323334public class Log4J2NacosLogging extends AbstractNacosLogging &#123;\tpublic Log4J2NacosLogging() &#123; // NACOS_LOG4J2_LOCATION ===&gt; &quot;classpath:nacos-log4j2.xml&quot; String location = getLocation(NACOS_LOG4J2_LOCATION); // location 有值, 将值放到 locationList, 用于后续的日志加载 if (!StringUtils.isBlank(location)) &#123; locationList.add(location); &#125; &#125; // 父类的方法, 为了方便, 写在这里 protected String getLocation(String defaultLocation) &#123; // 获取系统属性 NACOS_LOGGING_CONFIG_PROPERTY 的值, 值不为空 返回 NACOS_LOGGING_CONFIG_PROPERTY 的值 String location = System.getProperty(NACOS_LOGGING_CONFIG_PROPERTY); if (StringUtils.isBlank(location)) &#123; // NACOS_LOGGING_CONFIG_PROPERTY 的值为空, 根据是否需要返回默认值, 来返回 null 或者 defaultLocation if (isDefaultConfigEnabled()) &#123; // 返回 defaultLocation 也就是 classpath:nacos-log4j2.xml, 这个配置文件在 nacos-client.jar 这个 jar 包里面可以找到 return defaultLocation; &#125; return null; &#125; return location; &#125; private boolean isDefaultConfigEnabled() &#123; // 判断是否设置了系统属性 NACOS_LOGGING_DEFAULT_CONFIG_ENABLED_PROPERTY, 如果设置了, 同时值为 false, 则返回false, 否则为 true String property = System.getProperty(NACOS_LOGGING_DEFAULT_CONFIG_ENABLED_PROPERTY); // The default value is true. return property == null || BooleanUtils.toBoolean(property); &#125;&#125; 进入到Log4J2NacosLogging的loadConfiguration方法12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970public class Log4J2NacosLogging extends AbstractNacosLogging &#123;\tpublic void loadConfiguration() &#123; // 构造函数中处理的 locationList, 如果为空, 直接返回了 if (locationList.isEmpty()) &#123; return; &#125; // 获取到当前系统能支持的配置文件的路径 List&lt;String&gt; configList = findConfig(getCurrentlySupportedConfigLocations()); if (configList != null) &#123; locationList.addAll(configList); &#125; final List&lt;AbstractConfiguration&gt; configurations = new ArrayList&lt;AbstractConfiguration&gt;(); LoggerContext loggerContext = (LoggerContext)LogManager.getContext(false); for (String location : locationList) &#123; try &#123; // 把配置文件转为配置 Configuration configuration = loadConfiguration(loggerContext, location); if (configuration instanceof AbstractConfiguration) &#123; configurations.add((AbstractConfiguration)configuration); &#125; &#125; catch (Exception e) &#123; throw new IllegalStateException(&quot;Could not initialize Log4J2 Nacos logging from &quot; + location, e); &#125; &#125; CompositeConfiguration compositeConfiguration = new CompositeConfiguration(configurations); // 重新加载日志配置 loggerContext.start(compositeConfiguration); &#125; private String[] getCurrentlySupportedConfigLocations() &#123; List&lt;String&gt; supportedConfigLocations = new ArrayList&lt;String&gt;(); // YAML_PARSER_CLASS_NAME ===&gt; com.fasterxml.jackson.dataformat.yaml.YAMLParser if (ClassUtils.isPresent(YAML_PARSER_CLASS_NAME)) &#123; Collections.addAll(supportedConfigLocations, &quot;log4j2.yaml&quot;, &quot;log4j2.yml&quot;, &quot;log4j2-test.yaml&quot;, &quot;log4j2-test.yml&quot;); &#125; // JSON_PARSER_CLASS_NAME ===&gt; com.fasterxml.jackson.databind.ObjectMapper if (ClassUtils.isPresent(JSON_PARSER_CLASS_NAME)) &#123; Collections.addAll(supportedConfigLocations, &quot;log4j2.json&quot;, &quot;log4j2.jsn&quot;, &quot;log4j2-test.json&quot;, &quot;log4j2-test.jsn&quot;); &#125; supportedConfigLocations.add(&quot;log4j2.xml&quot;); supportedConfigLocations.add(&quot;log4j2-test.xml&quot;); // 把当前系统支持的配置文件的格式, 以数组的格式返回回去 return supportedConfigLocations.toArray(new String[supportedConfigLocations.size()]); &#125; private List&lt;String&gt; findConfig(String[] locations) &#123; // 把上面 getCurrentlySupportedConfigLocations 获取到的 配置文件格式拼接成程序能找到的路径 final String configLocationStr = this.strSubstitutor.replace(PropertiesUtil.getProperties().getStringProperty(CONFIGURATION_FILE_PROPERTY)); if (configLocationStr != null) &#123; return Arrays.asList(configLocationStr.split(&quot;, &quot;)); &#125; for (String location : locations) &#123; ClassLoader defaultClassLoader = ClassUtils.getDefaultClassLoader(); if (defaultClassLoader != null &amp;&amp; defaultClassLoader.getResource(location) != null) &#123; List&lt;String&gt; list = new ArrayList&lt;String&gt;(); list.add(&quot;classpath:&quot; + location); return list; &#125; &#125; return null; &#125;&#125; 从上面看下来, 报错的原因是, nacos会重新加载日志的配置, 但是需要的配置文件都没有, 后续的默认的配置文件classpath:nacos-log4j2.xml也没有设置日志的Root, 所以报错了。 解决isDefaultConfigEnabled方法提示了, 只需要在项目启动的时候设置一下System.setProperty(“nacos.logging.default.config.enabled”, “false”); 就行了 getCurrentlySupportedConfigLocations方法提示了,只需要把你的日志配置文件的修改为里面的其中的一种格式就行了 (因为基于 springboot 搭建的, 所以我的日志命名为了 log4j2-spring.xml 所以读取不到)"},{"title":"幂等","path":"/2024/02/15/幂等/","content":"什么是幂等幂等是一个数学与计算机科学概念 在数学中，幂等用函数表达式就是：f(x) &#x3D; f(f(x))。比如求绝对值的函数就是幂等的，abs(x) &#x3D; abs(abs(x))。 在计算机科学中，幂等表示一次或多次请求某一个资源应该具有同样的副作用，或者说，多次请求所产生的影响与一次请求所产生的影响效果相同。 为什么需要幂等当前互联网的系统几乎都是解耦隔离的，会存在各个不同系统相互远程调用。调用远程服务会有三个状态，成功、失败、超时，前两者都是明确的状态，超时则是未知状态。比如转账超时的时候，如果下游转账系统做好幂等控制，发起重试，那么即可以保证转账正常进行，又可以保证不会多转一笔。 其实，除了转账这个例子，日常开发中很多很多例子需要考虑幂等。比如： MQ（消息中间件）消费者读取消息时，有可能会读取到重复消息（重复消费） 提交form表单时，如果快速点击提交按钮，可能产生了两条一样的数据（前端重复提交） 接口超时，如何处理两种方案 下游系统提供一个对应的查询接口，如果接口超时了，先查下对应的记录，如果查询到成功，就走成功流程，如果是失败，就走失败流程。 拿我们的转账例子来说，转账系统提供一个查询转账记录的接口，如果渠道系统调用转账系统超时时，渠道系统先去查询一下这笔记录，看下这笔转账记录成功还是失败，如果成功就走成功流程，失败再重试发起转账。 下游接口支持幂等，上游系统如果调用超时，发起重试即可 两种方案都挺不错的，但是如果是MQ重复消费的场景，方案一处理并不是很妥，所以，还是得要求下游系统对外接口支持幂等。 如何设计幂等幂等意味着请求的唯一性，不管哪个方案设计幂等，都需要一个全局唯一的ID，去标记这个请求是独一无二的。 如果是利用唯一索引控制幂等，那唯一索引是唯一的 如果是利用数据库主键控制幂等，那主键是唯一的 如果是悲观锁的方式，底层标记还是全局唯一的 全局唯一ID可以通过数据库主键、UUID、雪花算法、百度的Uidgenerator或者美团的Leaf都可以。 幂等设计的基本流程幂等处理的过程，说到底其实就是过滤一下已经收到的请求，当然，请求一定要有一个全局唯一的ID标记。然后，怎么判断请求是否之前收到过呢？把请求储存起来，收到请求时，先查下存储记录，如果存在就返回上次的结果，不存在就处理请求。 实现幂等的9种方式select+insert+主键&#x2F;唯一索引冲突交易请求过来，会先根据请求的唯一流水号bizSeq字段，先select一下数据库的流水表 如果数据已经存在，就拦截是重复请求，直接返回成功 如果数据不存在，就执行insert插入，如果insert成功就返回成功，如果insert产生主键冲突异常，则捕获异常，接着直接返回成功 直接insert+主键&#x2F;唯一索引冲突跟上边唯一的区别是，如果请求重复的概率比较低的话，可以直接插入请求，利用主键&#x2F;唯一索引冲突，去判断重复请求 千万别搞混，防重和幂等设计其实是有区别的。防重主要为了避免产生重复数据，把重复请求拦截下来即可。而幂等设计除了拦截已经处理的请求，还要求每次相同的请求都返回一样的效果。不过呢，很多时候，它们的处理流程可以是类似的。 状态机幂等很多业务表，都是有状态的，比如转账流水表，就会有0-待处理，1-处理中、2-成功、3-失败状态。转账流水更新的时候，都会涉及流水状态更新，即涉及状态机 (即状态变更图)。我们可以利用状态机实现幂等，一起来看下它是怎么实现的。 比如转账成功后，把处理中的转账流水更新为成功状态，SQL这么写： 1update transfr_flow set status=2 where biz_seq=‘666’ and status=1; 第1次请求来时，bizSeq流水号是 666，该流水的状态是处理中，值是 1，要更新为2-成功的状态，所以该update语句可以正常更新数据，sql执行结果的影响行数是1，流水状态最后变成了2。 第2请求也过来了，如果它的流水号还是 666，因为该流水状态已经2-成功的状态了，所以更新结果是0，不会再处理业务逻辑，接口直接返回。 抽取防重表上面的两种方案，都是建立在业务流水表上bizSeq的唯一性上。很多时候，我们业务表唯一流水号希望后端系统生成，又或者我们希望防重功能与业务表分隔开来，这时候我们可以单独搞个防重表。当然防重表也是利用主键&#x2F;索引的唯一性，如果插入防重表冲突即直接返回成功，如果插入成功，即去处理请求。 token令牌token令牌方案一般包括两个请求方案 客户端请求申请获取token，服务端生成token返回 客户端带着token请求，服务端校验token 客户端发起请求，申请获取token。 服务端生成全局唯一的token，保存到redis中（一般会设置一个过期时间），然后返回给客户端。 客户端带着token，发起请求。 服务端去redis确认token是否存在，一般用 redis.del(token)的方式，如果存在会删除成功，即处理业务逻辑，如果删除失败不处理业务逻辑，直接返回结果。 悲观锁（select for update）123456789begin; # 1.开始事务select * from order where order_id=&#x27;666&#x27; for update # 查询订单，判断状态,锁住这条记录if（status !=处理中）&#123; //非处理中状态，直接返回； return ;&#125;## 处理业务逻辑update order set status=&#x27;完成&#x27; where order_id=&#x27;666&#x27; # 更新完成commit; # 5.提交事务 这里面order_id需要是索引或主键哈，要锁住这条记录就好，如果不是索引或者主键，会锁表的！ 悲观锁在同一事务操作过程中，锁住了一行数据。别的请求过来只能等待，如果当前事务耗时比较长，就很影响接口性能。所以一般不建议用悲观锁。 乐观锁表增加一列version版本号，每次更新记录version都升级一下（version&#x3D;version+1）。具体流程就是先查出当前的版本号version，然后去更新修改数据时，确认下是不是刚刚查出的版本号，如果是才执行更新比如，我们更新前，先查下数据，查出的版本号是version &#x3D;1 1select order_id，version from order where order_id=&#x27;666&#x27;； 然后使用version &#x3D;1和订单Id一起作为条件，再去更新 1update order set version = version + 1，status=&#x27;P&#x27; where order_id=&#x27;666&#x27; and version = 1 最后更新成功，才可以处理业务逻辑，如果更新失败，默认为重复请求，直接返回。 为什么版本号建议自增的呢？ 因为乐观锁存在ABA的问题，如果version版本一直是自增的就不会出现ABA的情况了 分布式锁分布式锁实现幂等性的逻辑就是，请求过来时，先去尝试获得分布式锁，如果获得成功，就执行业务逻辑，反之获取失败的话，就舍弃请求直接返回成功。 分布式锁可以使用Redis，也可以使用ZooKeeper，不过还是Redis相对好点，因为较轻量级。 Redis分布式锁，可以使用命令SET EX PX NX + 唯一流水号实现，分布式锁的key必须为业务的唯一标识哈 Redis执行设置key的动作时，要设置过期时间哈，这个过期时间不能太短，太短拦截不了重复请求，也不能设置太长，会占存储空间。 HTTP的幂等接口一般都是基于http的，所以我们再来聊聊Http的幂等吧。HTTP 请求方法主要有以下这几种，我们看下各个接口是否都是幂等的。 GET方法 HEAD方法 OPTIONS方法 DELETE方法 POST 方法 PUT方法 GET 方法HTTP 的GET方法用于获取资源，可以类比于数据库的select查询，不应该有副作用，所以是幂等的。它不会改变资源的状态，不论你调用一次还是调用多次，效果一样的，都没有副作用。 如果你的GET方法是获取最近最新的新闻，不同时间点调用，返回的资源内容虽然不一样，但是最终对资源本质是没有影响的哈，所以还是幂等的。 HEAD 方法HTTP HEAD和GET有点像，主要区别是HEAD不含有呈现数据，而仅仅是HTTP的头信息，所以它也是幂等的。如果想判断某个资源是否存在，很多人会使用GET，实际上用HEAD则更加恰当。即HEAD方法通常用来做探活使用。 OPTIONS方法HTTP OPTIONS 主要用于获取当前URL所支持的方法，也是有点像查询，因此也是幂等的。 DELETE方法HTTP DELETE 方法用于删除资源，它是的幂等的。比如我们要删除id&#x3D;666的帖子，一次执行和多次执行，影响的效果是一样的呢。 POST 方法HTTP POST 方法用于创建资源，可以类比于提交信息，显然一次和多次提交是有副作用，执行效果是不一样的，不满足幂等性。 比如：POST http://www.tianluo.com/articles的语义是在http://www.tianluo.com/articles下创建一篇帖子，HTTP 响应中应包含帖子的创建状态以及帖子的 URI。两次相同的POST请求会在服务器端创建两份资源，它们具有不同的 URI；所以，POST方法不具备幂等性。 PUT 方法HTTP PUT 方法用于创建或更新操作，所对应的URI是要创建或更新的资源本身，有副作用，它应该满足幂等性。 比如：PUT http://www.tianluo.com/articles/666的语义是创建或更新 ID 为666的帖子。对同一 URI 进行多次 PUT 的副作用和一次 PUT 是相同的；因此，PUT 方法具有幂等性。"},{"title":"限流","path":"/2024/01/01/限流/","content":"面对越来越多的高并发场景，限流显的由为重要，限流有多种实现的方式，下面介绍下通过Redis的限流方式 常见的4种限流算法计数器算法在指定周期内累加访问次数，当访问次数达到设定的阀值时，触发限流策略，当进行下一个时间周期时进行访问次数清零，如果所示，我们要求3s内的请求不要超过150次 但是，貌似看似很完美的流量统计其实存在一个非常严重的临界问题，如果第2-3秒内产生了150次请求，而3-4秒内产生了150次请求，那么其实在2-4秒2秒内，就已经发生了300次请求了，远远大于我们要求的3秒内的请求不要超过150次这个限制 滑动窗口法滑动窗口为固定窗口的改良版，解决了固定窗口在窗口切换时会受到两倍于阀值数量的请求，滑动窗口在固定窗口的基础上，将一个窗口分为若干个等份的小窗口，每个小窗口对应不同的时间点，拥有独立的计数器，当请求的时间点大于当前窗口的最大时间点时，则将窗口向前平移一个小窗口（将第一个小窗口的数据舍弃，第二个小窗口变成第一个小窗口，当前请求放在最后一个小窗口），整个窗口的所有请求数相加不能大于阈值。其中，Sentinel就是采用滑动窗口算法来实现限流的。 把3秒钟划分为3个小窗，每个小窗限制请求不能超过50秒。 比如我们设置，3秒内不能超过150个请求，那么这个窗口就可以容纳3个小窗，并且随着时间推移，往前滑动。每次请求过来后，都要统计滑动窗口内所有小窗的请求总量。 令牌桶限流算法控制令牌生成的速度，取的速度不控制令牌桶是网络流量整形（Traffic Shaping）和速率限制（Rate Limiting）中最常使用的一种算法。对于每一个请求，都需要从令牌桶中获得一个令牌；如果没有获得令牌，则需要触发限流策略。系统会以恒定速度（r tokens&#x2F;sec）往固定容量的令牌桶中放入令牌。令牌桶有固定的大小，如果令牌桶被填满，则会丢弃令牌。会存在三种情况： 【请求速度 大于 令牌生成速度】当令牌被取空后，会被限流 【请求速度 等于 令牌生成速度】流量处于平稳状态 【请求速度 小于 令牌生成速度】请求可被正常处理，桶满则丢弃令牌 漏桶限流算法控制水流流出速度，不控制水滴产生速度主要的作用： 控制数据注入网络的速度 平滑网络上的突发流量 漏桶限流算法的核心就是：不管上面水流的速度有多快，漏桶水滴的流出速度始终保持不变。消息中间件就采用漏桶限流的思想。 基于Redis的setnx操作我们在使用Redis的分布式锁的时候，大家都知道是依靠了setnx指令，在CAS（compare and swap）的操作的时候，同时给指定的key设置了过期时间（expire），我们限流的主要目的是为了在单位时间内，有且仅有N数量的请求能够访问我的代码程序，所以依靠setnx可以很轻松的做到这方面的功能。比如我们需要在10秒内限定20个请求，那么我们在setnx的时候可以设置过期时间10s，当请求的setnx数量达到20的时候即达到了限流效果。当然这种方法的弊端是很多的，比如当统计1-10秒的时候，无法统计2-11秒之内，如果统计N秒内M个请求，那么Redis中需要保持N个key等等问题。 1234567891011121314151617/*** @param key redis key* @param unitTime 单位时间内，单位ms* @param sendMaxCount 发送最大次数* @description 比如10秒内最大发送次数100次，暂停发送，超多10秒后恢复发送，传值：unitTime=10000，sendMaxCount=100*/public boolean isLimit(String key, long unitTime, int sendMaxCount) &#123; long count = jCloudCache.incrBy(key, 1); if (count == 1) &#123; jCloudCache.expire(key, unitTime, TimeUnit.MILLISECONDS); &#125; if (count &gt; sendMaxCount) &#123; logger.info(&quot;&#123;&#125; ms内超过了发送的次数&#123;&#125;，暂停发送&quot;, unitTime, sendMaxCount); return false; &#125; return true;&#125; 基于Redis的数据结构zset其实限流涉及的最主要的就是滑动窗口，上面也提到1-10怎么变成2-11，其实也就是起始值和末端值都各加1即可。我们用Redis的list数据结构可以实现整个功能。我们可以将请求打造一个zset数组，当每一次请求进来的时候，value保持唯一，可以用UUID生成，而score可以用当前时间戳表示，因为score可以用来计算当前时间戳之内有多少个请求数量。而zset数据结构也提供了range方法让我们可以跟轻易的获得个时间戳内有多少请求。 12345678910111213public Response limitFlow()&#123; Long currentTime = new Date().getTime(); System.out.println(currentTime); if(redisTemplate.hasKey(&quot;limit&quot;)) &#123; Integer count = redisTemplate.opsForZSet().rangeByScore(&quot;limit&quot;, currentTime - intervalTime, currentTime).size(); // intervalTime是限流的时间 System.out.println(count); if (count != null &amp;&amp; count &gt; 5) &#123; return Response.ok(&quot;每分钟最多只能访问5次&quot;); &#125; &#125; redisTemplate.opsForZSet().add(&quot;limit&quot;,UUID.randomUUID().toString(),currentTime); return Response.ok(&quot;访问成功&quot;);&#125; 通过上述代码可以看到滑动窗口的效果，并且能保证每N个请求内至多M个请求，缺点就是zset的数据结构越来越大。 基于Redis令牌桶算法提到限流不得不提到令牌桶算法了令牌桶算法涉及到输入速率和输出速率，当输出速率大于输入速率，那么就是超出流量限制了。也就是说我们每访问一次请求的时候，可以从Redis中获取一个令牌，如果拿到令牌了，说明没有超出限制，如果拿不到，说明超出限制了。依靠list的leftPop来获取令牌 12345678// 输出令牌public Response limitFlow2(Long id)&#123; Object result = redisTemplate.opsForList().leftPop(&quot;limit_list&quot;); if(result == null)&#123; return Response.ok(&quot;当前令牌桶中无令牌&quot;); &#125; return Response.ok(articleDescription2);&#125; 再依靠java的定时任务，定时往list中rightPush令牌，当然令牌也需要唯一性，所以这里还是用UUID生成 12345// 10S的速率往令牌桶中添加UUID，只为保证唯一性@Scheduled(fixedDelay = 10_000,initialDelay = 0)public void setIntervalTimeTask()&#123; redisTemplate.opsForList().rightPush(&quot;limit_list&quot;,UUID.randomUUID().toString());&#125;"},{"title":"Redis rehash","path":"/2023/12/31/redis-rehash/","content":"Redis键值对结构 HashTableRedis中有一个全局哈希表，该哈希表中保存所有的键值对，对于Hash表查找的操作的复杂度为O(1)。 Bucket哈希表中每一个元素称为哈希桶（Bucket），哈希桶中保存了键值对数据。 Entry用来保存键值对数据，保存的是Key、Value的指针值，通过对应的指针对Key、Value进行查找。 什么是rehashredis在项目中时常用来放缓存信息，由上面可知是由位桶组成的，随着缓存信息越来越大，这时候就需要对redis进行扩容。然而，之前的键值对是redis经过hash计算出的值存放在hash桶上，此时扩容需要对之前的键值对进行重新计算也就是rehash。由于存在表中的键值对可能有成百上千个，一次性rehash到ht[1]的话会导致服务器在一段时间内停止服务，于是出现了渐进式rehash，这个动作并不是一次性的，而是分多次，渐进式完成的 rehash触发时机 触发收缩：负载因子小于 0.1 触发扩展：以下任一条件符合即可 服务器目前没有在执行 BGSAVE 命令或者 BGREWRITEAOF 命令， 并且哈希表的负载因子大于等于 1 服务器目前正在执行 BGSAVE 命令或者 BGREWRITEAOF 命令， 并且哈希表的负载因子大于等于 512# 负载因子 = 哈希表已保存节点数量 / 哈希表大小load_factor = ht[0].used / ht[0].size rehash步骤Redis其实有两个全局全局哈希表，一开始默认使用的是Hash Table0（ht[0]）来存储数据，而Hash Table1（ht[1]）并没有分配内存空间，随着Hash Table0中的元素越来越多时，Redis会进行rehash操作。 为ht[1]分配空间 在字典中维持一个索引计数器变量rehashidx，并将它的值设置为0，表示rehash工作正式开始 在rehash进行期间，每次对字段执行添加、删除、查找或者更新操作时，程序除了执行指定的操作以外，还会顺带将ht[0]哈希表在rehashidx索引上的所有键值对rehash到ht[1]，当rehash工作完成之后，程序将rehashidx属性值增1 随着字典操作的不断执行，最终在某个时间点上，ht[0]上的所有键值对都会被rehash至ht[1]，这是程序将rehashidx的值设置为-1，表示rehash操作已完成 准备开始rehash rehash索引0上的键值对 rehash索引1上的键值对 rehash索引2上的键值对 rehash索引3上的键值对 rehash执行完毕 在进行渐进式rehash过程中，字典会同时使用ht[0]和ht[1]两个哈希表，所以在渐进式rehash进行期间，字段的删除、查找、更新等操作会在两个哈希表上进行，比如，在字典里查找一个键的话，先在ht[0]里进行查找，如果没找到，就会继续在ht[1]里进行查找。 另外，在渐进式rehash进行期间，新添加到字典里的键值对一律会保存到ht[1]里，而ht[0]不在进行任何添加操作，这一措施保证了ht[0]包含的键值对数量只减不增，并随着rehash操作的执行最终变成空表。 为了防止有的KV长时间不访问一直不迁移到ht[1]，会定时去rehash，每次迁移100个"},{"title":"分布式事务","path":"/2023/11/10/分布式事务/","content":"分布式事务： 在分布式系统中一次操作需要由多个服务协同完成，这种由不同的服务之间通过网络协同完成的事务 分布式事务分类 刚性事务 强一致性 XA模型 满足CAP理论的CP 柔性事务 保证最终一致性，事务结束后在可接收的时间范围内，可能出现短暂的不一致，最终会达成一致性 满足CAP理论的AP，满足BASE理论 刚性事务（XA） 柔性事务 业务改造 无 有 回滚 支持 实现补偿接口 一致性 强一致 最终一致 隔离性 原生支持 实现资源锁定接口 并发性能 严重衰退 略微衰退 适合场景 短事务并发低 长事务高并发 刚性事务 满足传统事务特性，ACID（原子性、一致性、隔离性、持久性） 满足XA模型 XA规范中定义了分布式事务处理模型，包含了3种核心角色 AP：Application Program，应用程序，通过TM定义事务边界（事务开始与结束），并且访问事务边界内的资源 RM：Resource Managers，资源管理器，提供数据资源的操作、管理接口，保证数据的一致性和完整性，主要包括数据库、MQ系统 、文件系统等。 TM：Transaction Managers，事务管理器，是一个协调者的角色，负责管理全局事务，分配全局事务ID，监测事务的执行速度，并负责事务的提交、回滚、失败恢复等。 2PC（两阶段提交）是XA规范标准实现 实现过程： AP发起一个全局事务，并且创建一个全局事务ID TM发起prepare投票，RM对投票进行表决 RM都同意后，TM发起commit提交。其中任何一个prepare时不同意，TM都会发起rollback。 在commit过程中，发生宕机等异常，在服务重启后根据XA recover再次进行补偿，保证最终commit操作成功。 缺点 同步阻塞模型 数据库资源锁定时间过程 全局锁（隔离级别串行化），并发能力低 不适合长事务 在一些异常情况下会有问题，在commit之后，回复给TM的ack丢失了，这时会导致TM不知道commit到底是否成功了，从而衍生出了三阶段提交来解决这个问题。 XA 两阶段提交协议设计上是要像本地事务一样实现事务的 ACID 四个特性： 原子性：在 prepare 和 commit 阶段保证事务是原子性的。 一致性：XA 协议实现的是强一致性。 隔离性：XA 事务在完成之前一直持有资源的锁，所以可以做到写隔离。 持久性：基于本地事务实现，所以这一点没有问题。 柔性事务满足CAP理论的AP，满足BASE理论，柔性事务是对XA协议的妥协，它通过降低强一致性，从而减少数据库资源的锁定时间，提升系统可用性。典型的架构实现： TCC模型 Saga模型 TCCTCC介绍TCC（Try Confirm Cancel）是应用层的两阶段提交，完全交由业务端实现，每个业务都需要实现Try、Confirm、Cancel接口，所以对代码的侵入性强。 TCC执行流程TCC的执行流程分为两个阶段： 第一阶段：Try，业务系统做监测并预留资源（加锁或锁住资源），比如常见的下单，在Try阶段，不是真正的减库存，而是把下单的库存给锁住。 第二阶段：根据第一阶段的结果决定是执行Confirm还是Cancel。Confirm：执行真正的业务（执行业务，释放锁）Cancel：是对Try阶段预留资源的释放（出问题，释放锁） TCC如何保证最终一致性 TCC事务以Try为中心的，Confirm确认操纵和Cancel取消操纵都是围绕try展开的。因此，Try阶段中的操纵，其保障性是最好的，即使失败仍然有Cancel取消操纵将其执行结果取消。 Try阶段执行成功并执行Confirm阶段时，默认Confirm阶段是不能出错的，也就是说，Try成功，Confirm一定成功（TCC设计之初的定义）。 Confirm和Cancel阶段如果执行失败，由TCC框架进行补偿。 存在极低概率在CC环节彻底失败，如果有，则需要定时任务或人工介入。 TCC的注意事项 允许空回滚空回滚出现的原因是Try超时或者丢包，导致TCC第二阶段触发Cancel操作，此时事务参与者未收到Try，但是却收到Cancel请求 所以，Cancel实现时允许空回滚，也就是Cancel执行时如果发现没有对应的事务xid或主键时，需要返回回滚成功，让事务管理器任务已经回滚。 防悬挂控制悬挂指的是二阶段的 Cancel 比 一阶段的Try 操作先执行，出现该问题的原因是 Try 由于网络拥堵而超时，导致事务管理器生成回滚，触发 Cancel 接口，但之后拥堵在网络的 Try 操作又被资源管理器收到了，但是 Cancel 比 Try 先到。但按照前面允许空回滚的逻辑，回滚会返回成功，事务管理器认为事务已回滚成功，所以此时应该拒绝执行空回滚之后到来的 Try 操作，否则会产生数据不一致。因此我们可以在 Cancel 空回滚返回成功之前，先记录该条事务 xid 或业务主键，标识这条记录已经回滚过，Try 接口执行前先检查这条事务xid或业务主键是否已经标记为回滚成功，如果是则不执行 Try 的业务操作。 幂等控制由于网络原因或者重试操作都有可能导致Try-Confirm-Cancel3个操作的重复执行，所以使用TCC时要注意这三个操纵的幂等控制，通常我们可以使用事务xid或业务主键来判重。 TCC方案的优缺点 相较于XA事务机制，有以下优点：性能提升： 具体业务来实现，控制资源锁的粒度变小，不会锁定整个资源。数据最终一致性： 基于Confirm和Cancel的幂等性，保证事务最终完成或取消，保证事务的一致性。可靠性： 解决了XA协议的协调者单点故障问题，由主业务方发起并控制整个活动，业务活动管理器也变成多点，引入集群。 缺点：TCC的Try、Confirm、Cancel要在具体的业务来实现，业务耦合度较高，提高了开发成本。 Saga什么是Saga事务aga事务核心思想是将长事务拆分为多个本地短事务并依次正常执行，如果所有短事务均执行成功，那么分布式事务提交；如果出现某个参与者执行本地事务失败，则由Saga事务协调器根据相反顺序调用补偿操作，回滚已提交的参与者，使分布式事务回到最初始的状态。Saga事务基本协议如下： 每个Saga事务由一系列幂等的有序子序列（sub-transaction）Ti组成 每个Ti都有对应的幂等补偿动作Ci，补偿动作用于撤销Ti造成的结果与TCC事务补偿机制相比，TCC有一个预留的Try动作，相当于先存一个草稿，然后才提交，Saga没有预留动作，直接提交。 Saga的恢复策略对于异常事务，Saga提供了两种恢复策略，分别如下： 向后恢复（backward recovery）当执行事务失败时，补偿所有已完成的事务，是“一退到底”的方式，这种做法的效果是撤销掉之前所有成功的子事务，使得整个 Saga 的执行结果撤销。如下图： 从上图可知事务执行到了支付事务T3，但是失败了，因此事务回滚需要从C3,C2,C1依次进行回滚补偿，对应的执行顺序为：T1,T2,T3,C3,C2,C1。 向前恢复(forward recovery)：对于执行不通过的事务，会尝试重试事务，这里有一个假设就是每个子事务最终都会成功，这种方式适用于必须要成功的场景，事务失败了重试，不需要补偿。流程如下图： Saga事务的实现方式 命令协调式（Order Orchestrator）中央协调器（Orchestrator，简称OSO）以命令&#x2F;回复的方式与每项服务进行通信，全权负责告诉每个参与者做什么以及什么时候该做什么。整体流程如下图： 事务发起方的主业务逻辑请求 OSO 服务开启订单事务 OSO 向库存服务请求扣减库存，库存服务回复处理结果。 OSO 向订单服务请求创建订单，订单服务回复创建结果。 OSO 向支付服务请求支付，支付服务回复处理结果。 主业务逻辑接收并处理 OSO 事务处理结果回复。中央协调器 OSO 必须事先知道执行整个事务所需的流程，如果有任何失败，它还负责通过向每个参与者发送命令来撤销之前的操作来协调分布式的回滚，基于中央协调器协调一切时，回滚要容易得多，因为协调器默认是执行正向流程，回滚时只要执行反向流程即可。 事件编排（Event Choreographyo）命令协调方式基于中央协调器实现，所以有单点风险，但是事件编排方式没有中央协调器。事件编排的实现方式中，每个服务产生自己的事件并监听其他服务的事件来决定是否应采取行动。 在事件编排方法中，第一个服务执行一个事务，然后发布一个事件，该事件被一个或多个服务进行监听，这些服务再执行本地事务并发布（或不发布）新的事件。当最后一个服务执行本地事务并且不发布任何事件时，意味着分布式事务结束，或者它发布的事件没有被任何 Saga 参与者听到都意味着事务结束。 事务发起方的主业务逻辑发布开始订单事件。 库存服务监听开始订单事件，扣减库存，并发布库存已扣减事件。 订单服务监听库存已扣减事件，创建订单，并发布订单已创建事件。 支付服务监听订单已创建事件，进行支付，并发布订单已支付事件。 主业务逻辑监听订单已支付事件并处理。 如果事务涉及 2 至 4 个步骤，则非常合适使用事件编排方式，它是实现 Saga 模式的自然方式，它很简单，容易理解，不需要太多的代码来构建。 Saga事务的优缺点 命令协调设计的优缺点： 优点：服务之间关系简单，避免服务间循环依赖，因为 Saga 协调器会调用 Saga 参与者，但参与者不会调用协调器。程序开发简单，只需要执行命令&#x2F;回复(其实回复消息也是一种事件消息)，降低参与者的复杂性。易维护扩展，在添加新步骤时，事务复杂性保持线性，回滚更容易管理，更容易实施和测试。 缺点：中央协调器处理逻辑容易变得庞大复杂，导致难以维护。存在协调器单点故障风险。 事件编排设计的优缺点： 优点：避免中央协调器单点故障风险。当涉及的步骤较少服务开发简单，容易实现。 缺点：服务之间存在循环依赖的风险。当涉及的步骤较多，服务间关系混乱，难以追踪调测。 由于 Saga 模型没有 Prepare 阶段，因此事务间不能保证隔离性。当多个 Saga 事务操作同一资源时，就会产生更新丢失、脏数据读取等问题，这时需要在业务层控制并发，例如：在应用层面加锁，或者应用层面预先冻结资源。 MQ事务消息MQ事务消息的执行流程基于MQ的分布式事务方案本质上是对本地消息表的封装，整体流程与本地消息表一致，唯一不同的就是将本地消息表存在了MQ内部，而不是业务数据库中，如下图： 由于将本地消息表存在了MQ内部，那么MQ内部的处理尤为重要，下面主要基于 RocketMQ4.3 之后的版本介绍 MQ 的分布式事务方案 RocketMQ事务消息在本地消息表方案中，保证事务主动方发写业务表数据和写消息表数据的一致性是基于数据库事务，而 RocketMQ 的事务消息相对于普通 MQ提供了 2PC 的提交接口，方案如下： 正常情况：在事务主动方服务正常，没有发生故障的情况下，发消息流程如下： 步骤①：发送方向 MQ Server(MQ服务方)发送 half 消息 步骤②：MQ Server 将消息持久化成功之后，向发送方 ack 确认消息已经发送成功 步骤③：发送方开始执行本地事务逻辑 步骤④：发送方根据本地事务执行结果向 MQ Server 提交二次确认（commit 或是 rollback）。 最终步骤：MQ Server 如果收到的是 commit 操作，则将半消息标记为可投递，MQ订阅方最终将收到该消息；若收到的是 rollback 操作则删除 half 半消息，订阅方将不会接受该消息 异常情况：在断网或者应用重启等异常情况下，图中的步骤④提交的二次确认超时未到达 MQ Server，此时的处理逻辑如下： 步骤⑤：MQ Server 对该消息发起消息回查 步骤⑥：发送方收到消息回查后，需要检查对应消息的本地事务执行的最终结果 步骤⑦：发送方根据检查得到的本地事务的最终状态再次提交二次确认。 最终步骤：MQ Server基于 commit&#x2F;rollback 对消息进行投递或者删除。 MQ事务消息的优缺点 优点：相比本地消息表方案，MQ 事务方案优点是： 消息数据独立存储 ，降低业务系统与消息系统之间的耦合 吞吐量大于使用本地消息表方案 缺点： 一次消息发送需要两次网络请求(half 消息 + commit&#x2F;rollback 消息) 。 业务处理服务需要实现消息状态回查接口。 最大努力通知最大努力通知也称为定期校对，是对MQ事务方案的进一步优化。它在事务主动方增加了消息校对的接口，如果事务被动方没有接收到主动方发送的消息，此时可以调用事务主动方提供的消息校对的接口主动获取 在可靠消息事务中，事务主动方需要将消息发送出去，并且让接收方成功接收消息，这种可靠性发送是由事务主动方保证的；但是最大努力通知，事务主动方仅仅是尽最大努力（重试，轮询….）将事务发送给事务接收方，所以存在事务被动方接收不到消息的情况，此时需要事务被动方主动调用事务主动方的消息校对接口查询业务消息并消费，这种通知的可靠性是由事务被动方保证的。 所以最大努力通知适用于业务通知类型，例如微信交易的结果，就是通过最大努力通知方式通知各个商户，既有回调通知，也有交易查询接口。 各方案常见使用场景总结 2PC&#x2F;3PC：依赖于数据库，能够很好的提供强一致性和强事务性，但延迟比较高，比较适合传统的单体应用，在同一个方法中存在跨库操作的情况，不适合高并发和高性能要求的场景。 TCC：适用于执行时间确定且较短，实时性要求高，对数据一致性要求高，比如互联网金融企业最核心的三个服务：交易、支付、账务。 本地消息表&#x2F;MQ 事务：适用于事务中参与方支持操作幂等，对一致性要求不高，业务上能容忍数据不一致到一个人工检查周期，事务涉及的参与方、参与环节较少，业务上有对账&#x2F;校验系统兜底。 Saga 事务：由于 Saga 事务不能保证隔离性，需要在业务层控制并发，适合于业务场景事务并发操作同一资源较少的情况。Saga 由于缺少预提交动作，导致补偿动作的实现比较麻烦，例如业务是发送短信，补偿动作则得再发送一次短信说明撤销，用户体验比较差。所以，Saga 事务较适用于补偿动作容易处理的场景"},{"title":"maven中deploy命令报401的原因及解决方案","path":"/2023/07/06/maven-deploy-401/","content":"idea使用过程中有时候会出现deploy时候报401错误，如下图 原因一、pom 文件李配置的私服仓库地址和settings.xml里配置的用户名和密码没有匹配上 pom.xml里的仓库配置12345678910111213&lt;!--项目分发信息，在执行mvn deploy后表示要发布的位置。有了这些信息就可以把网站部署到远程服务器或者把构件jar等部署到远程仓库 --&gt;&lt;distributionManagement&gt; &lt;repository&gt; &lt;id&gt;releases&lt;/id&gt;&lt;!-- 此处id和settings.xml的id保持一致 --&gt; &lt;name&gt;Release Deploy&lt;/name&gt; &lt;url&gt;http://10.20.105.11:8081/repository/maven-releases/&lt;/url&gt; &lt;/repository&gt; &lt;snapshotRepository&gt;&lt;!--构件的快照部署到哪里？如果没有配置该元素，默认部署到repository元素配置的仓库，参见distributionManagement/repository元素 --&gt; &lt;id&gt;snapshots&lt;/id&gt;&lt;!-- 此处id和settings.xml的id保持一致 --&gt; &lt;name&gt;Snapshot Deploy&lt;/name&gt; &lt;url&gt;http://10.20.105.11:8081/repository/maven-snapshots/&lt;/url&gt; &lt;/snapshotRepository&gt;&lt;/distributionManagement&gt; 此时对应的setting.xml里的配置信息为123456789101112&lt;servers&gt; &lt;server&gt; &lt;id&gt;snapshots&lt;/id&gt;&lt;!-- 此处id和上面pom.xml的id保持一致 --&gt; &lt;username&gt;zhangsan&lt;/username&gt; &lt;password&gt;123456&lt;/password&gt; &lt;/server&gt; &lt;server&gt; &lt;id&gt;releases&lt;/id&gt;&lt;!-- 此处id和上面pom.xml的id保持一致 --&gt; &lt;username&gt;zhangsan&lt;/username&gt; &lt;password&gt;123456&lt;/password&gt; &lt;/server&gt;&lt;/servers&gt; 检查两者信息是否一致，就可以解决问题。如果还是报401问题，则可能是下面的原因。 原因二、idea中自定义的settings.xml配置没有生效"},{"title":"BASE理论","path":"/2023/05/17/BASE/","content":"摘自JavaGuide BASE简介BASE理论起源于2008年，由eBay的架构师Dan Pritchett在ACM上发表。BASE是Basically Available（基本可用）、Soft-state（软状态）和Eventually Consistent（最终一致性）三个短语的缩写。BASE理论是对CAP中一致性C和可用性A权衡的结果，其来源于对大规模互联网系统分布式实践的总结，是基于CAP定理逐渐演化而来的，它大大降低了我们对系统的要求。 BASE理论的核心思想即使无法做到强一致性，但每个应用都可以根据自身的业务特点，采用适当的方式来达到最终一致性。 也就是牺牲数据的一致性来满足系统的高可用性，系统中一部分数据不可用或者不一致时，仍需要保持系统的整体可用。 BASE理论本质上是对CAP的延伸和补充，更具体的说，是对CAP中的AP方案的一个补充。 为什么这么说呢&#x2F; CAP理论这节我们也说过了 如果系统没有发生“分区”的话，节点间的网络连接通信正常的话，也就不存在 P 了。这个时候，我们就可以同时保证 C 和 A 了。因此，如果系统发生“分区”，我们要考虑选择 CP 还是 AP。如果系统没有发生“分区”的话，我们要思考如何保证 CA 。 因此，AP方案只是在系统发生分区的时候放弃一致性，而不是永远放弃一致性。在分区故障恢复后，系统应该达到最终一致性，这一点其实就是BASE理论延伸的地方。 BASE理论三要素 基本可用基本可用是指分布式系统在出现不可预知故障的时候，允许损失部分可用性。但是，这绝不等价于系统不可用。什么叫允许损失部分可用性呢？ 响应时间上的损失: 正常情况下，处理用户请求需要 0.5s 返回结果，但是由于系统出现故障，处理用户请求的时间变为 3 s。 系统功能上的损失：正常情况下，用户可以使用系统的全部功能，但是由于系统访问量突然剧增，系统的部分非核心功能无法使用。 软状态软状态指允许系统中的数据存在中间状态（CAP 理论中的数据不一致），并认为该中间状态的存在不会影响系统的整体可用性，即允许系统在不同节点的数据副本之间进行数据同步的过程存在延时。 最终一致性最终一致性强调的是系统中所有的数据副本，在经过一段时间的同步后，最终能够达到一个一致的状态。因此，最终一致性的本质是需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性。 分布式一致性的 3 种级别： 强一致性：系统写入了什么，读出来的就是什么。 弱一致性：不一定可以读取到最新写入的值，也不保证多少时间之后读取到的数据是最新的，只是会尽量保证某个时刻达到数据一致的状态。 最终一致性：弱一致性的升级版，系统会保证在一定时间内达到数据一致的状态。业界比较推崇是最终一致性级别，但是某些对数据一致要求十分严格的场景比如银行转账还是要保证强一致性。 那实现最终一致性的具体方式是什么呢? 《分布式协议与算法实战》 中是这样介绍： 读时修复 : 在读取数据时，检测数据的不一致，进行修复。比如 Cassandra 的 Read Repair 实现，具体来说，在向 Cassandra 系统查询数据的时候，如果检测到不同节点的副本数据不一致，系统就自动修复数据。 写时修复 : 在写入数据，检测数据的不一致时，进行修复。比如 Cassandra 的 Hinted Handoff 实现。具体来说，Cassandra 集群的节点之间远程写数据的时候，如果写失败 就将数据缓存下来，然后定时重传，修复数据的不一致性。 异步修复 : 这个是最常用的方式，通过定时对账检测副本数据的一致性，并修复。 比较推荐 写时修复，这种方式对性能消耗比较低。 总结ACID 是数据库事务完整性的理论，CAP 是分布式系统设计理论，BASE 是 CAP 理论中 AP 方案的延伸。"},{"title":"CAP理论","path":"/2023/05/15/CAP/","content":"CAP简介CAP（CAP theorem）定理又被称作布鲁尔定理（Brewer’s theorem），是加州大学伯克利分校的计算机科学家埃里克·布鲁尔（Eric Brewer）在2000年的ACM PODC上提出的一个猜想。2002年，麻省理工学院的赛斯·吉尔伯特（Seth Gilbert）和南希·林奇（Nancy Lynch）发表了布鲁尔猜想的证明，使之成为分布式计算领域公认的一个定理。 CAP是Consistency（一致性）、Availability（可用性）、Partition Tolerance（分区容错性） 这三个单词首字母组合。 在一个分布式系统（指互相连接并共享数据的节点的集合）中，当涉及到读写操作时，只能同时满足一下三点中的两个： 一致性：所有节点访问同一份最新的数据副本。 可用性：非故障的节点在合理的时间内返回合理的响应（不是错误或者超时的响应）。 分区容错性：分布式出现网络分区的时候，让能对外提供服务。 什么是网络分区？分布式系统中，多个节点之间的网络本来是连通的，但是因为某些故障（比如部分节点网络出现了问题）某些节点之间不连通了，整个网络就分成了几块区域，这就叫网络分区。 不是所谓的”3选2”大部分人解释这一定律时，常常简单的表述为”一致性、可用性、分区容错性，这三者只能同时达到两个，不可能同时达到”。实际上这是一个非常有误导性的说法，而且在CAP理论诞生12年之后，CAP之父也在2012年重写了之前的论文。 当发生网络分区的时候，如果我们要继续提供服务，那么强一致性合可用性只能2选1。也就是说当网络分区之后P是前提，决定了P之后才有C和A的选择。也就是说分区容错性（Partition Tolerance）我们是必须要实现的。简而言之就是：CAP理论中分区容错性P是一定要满足的，在此基础上，只能满足可用性A或者一致性C。 因此，分布式系统理论上不可能选择CA架构，只能选择CP或者AP架构。比如ZooKeeper、HBase就是CP架构，Cassandra、Eureka就是AP架构，Nacos不仅支持CP也支持AP。 为啥不能选择CA架构呢？举个例子：若系统出现分区，系统中某个节点在进行写操作。为了保证C，必须要禁止其他节点的读写操作，这就和A发生冲突了。如果为了保证A，其他节点的读写操作正常的话，那就和C发生冲突了。 选择CP和AP的关键在于当前的业务场景，没有定论，比如对于需要确保强一致性的场景如银行一般会选择CP。 另外需要补充说明一点的是，如果网络分区正常的话，也就是说不需要保证P的时候，C和A能够同时保证。 CAP实际应用案例以注册中心来探讨一下CAP的实际应用，下图是Dubbo的架构图，其中注册中心（Registry）负责服务地址的注册与查找，相当于目录服务，服务提供者和消费者只在启动时与注册中心交互，注册中心不转发请求，压力较小。 常见的可以作为注册中心的有：Zookeeper、Eureka、Nacos等。 Zookeeper保证的是CP。任何时刻对Zookeeper的读请求都能得到一致性的结果，但是，Zookeeper不保证每次请求的可用性，比如在Leader选举过程中或者半数以上的机器不可用的时候就是不可用的。 Eureka保证的则是AP。Eureka在设计的时候就是优先保证A（可用性）。在Eureka中不存在什么Leader节点，每个节点都是一样的、平等的。因此Eureka不会像Zookeeper那样出现选举过程中或者半数以上的机器不可用的时候服务就是不可用的情况，Eureka保证即使大部分节点挂掉也不会影响正常提供服务，只要有一个节点是可用的就行了，只不过这个节点上的数据可能并不是最新的。 Nacos不仅支持CP也支持AP。 总结在进行分布式系统设计和开发时，我们不应该仅仅局限在 CAP 问题上，还要关注系统的扩展性、可用性等等在系统发生“分区”的情况下，CAP 理论只能满足 CP 或者 AP。要注意的是，这里的前提是系统发生了“分区”如果系统没有发生“分区”的话，节点间的网络连接通信正常的话，也就不存在 P 了。这个时候，我们就可以同时保证 C 和 A 了。总结：如果系统发生“分区”，我们要考虑选择 CP 还是 AP。如果系统没有发生“分区”的话，我们要思考如何保证 CA 。"},{"title":"动态规划","path":"/2022/12/22/动态规划/","content":"不同路径 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061//一个机器人位于一个 m x n 网格的左上角 （起始点在下图中标记为 “Start” ）。//机器人每次只能向下或者向右移动一步。机器人试图达到网格的右下角（在下图中标记为 “Finish” ）。//问总共有多少条不同的路径？//输入：m = 3, n = 7//输出：28//示例 2：////输入：m = 2, n = 3//输出：3//解释： 从左上角开始，总共有 3 条路径可以到达右下角。////向右 -&gt; 向右 -&gt; 向下//向右 -&gt; 向下 -&gt; 向右//向下 -&gt; 向右 -&gt; 向右//示例 3：////输入：m = 7, n = 3//输出：28//示例 4：////输入：m = 3, n = 3//输出：6//提示：////1 &lt;= m, n &lt;= 100//题目数据保证答案小于等于 2 * 10^9public class UniquePaths &#123; /** * 1. 确定dp数组下标含义 dp[i][j] 到每一个坐标可能的路径种类 * 2. 递推公式 dp[i][j] = dp[i-1][j] dp[i][j-1] * 3. 初始化 dp[i][0]=1 dp[0][i]=1 初始化横竖就可 * 4. 遍历顺序 一行一行遍历 * 5. 推导结果 。。。。。。。。 * * @param m * @param n * @return */ public int uniquePaths(int m, int n) &#123; int[][] dp = new int[m][n]; //初始化 for (int i = 0; i &lt; m; i++) &#123; dp[i][0] = 1; &#125; for (int i = 0; i &lt; n; i++) &#123; dp[0][i] = 1; &#125; for (int i = 1; i &lt; m; i++) &#123; for (int j = 1; j &lt; n; j++) &#123; dp[i][j] = dp[i - 1][j] + dp[i][j - 1]; &#125; &#125; return dp[m - 1][n - 1]; &#125; public static void main(String[] args) &#123; UniquePaths uniquePaths = new UniquePaths(); System.out.println(uniquePaths.uniquePaths(3, 7)); &#125;&#125; 斐波那契数 123456789101112class Solution &#123; public int fib(int n) &#123; if (n &lt; 2) return n; int a = 0, b = 1, c = 0; for (int i = 1; i &lt; n; i++) &#123; c = a + b; a = b; b = c; &#125; return c; &#125;&#125;"},{"title":"排序","path":"/2022/12/22/排序/","content":"二分查找1234567891011121314151617181920212223242526272829303132public class BinarySearch &#123; //左闭右闭区间 public int binarySearch(int[] nums, int target) &#123; if (nums == null || nums.length == 0) &#123; return -1; &#125; int left = 0; int right = nums.length - 1; // 定义target在左闭右闭的区间里，[left, right] while (left &lt;= right) &#123; // 当left==right，区间[left, right]依然有效，所以用 &lt;= int mid = left + (right - left) / 2; // 防止溢出 等同于(left + right)/2 //int mid = left + ((right - left) &gt;&gt; 1); if (nums[mid] == target) &#123; return mid; &#125; else if (nums[mid] &lt; target) &#123; left = mid + 1; // target 在右区间，所以[middle + 1, right] &#125; else if (nums[mid] &gt; target) &#123; right = mid - 1; // target 在左区间，所以[left, middle - 1] &#125; &#125; return -1; &#125; public static void main(String[] args) &#123; int[] nums = &#123;1,2,3,4,5,6,7,8,9&#125;; BinarySearch binarySearch = new BinarySearch(); System.out.println(binarySearch.binarySearch(nums, 5)); &#125;&#125; 基础冒泡排序1234567891011121314151617181920212223242526272829public class BubbleSort &#123; /** * 优化版本一：添加提前结束标志 * 如果某一轮没有发生交换，说明已经有序，可以提前结束 */ public void bubbleSort(int[] arr) &#123; if (arr == null || arr.length &lt;= 1) &#123; return; &#125; int n = arr.length; boolean swap;// 标记是否发生交换 for (int i = 0; i &lt; n - 1; i++) &#123; swap = false; for (int j = 0; j &lt; n - 1 - i; j++) &#123; if (arr[j] &lt; arr[j + 1]) &#123; int temp = arr[j]; arr[j] = arr[j + 1]; arr[j + 1] = temp; swap = true; &#125; &#125; // 如果这一轮没有发生交换，说明已经有序，提前结束 if (!swap) &#123; break; &#125; &#125; &#125;&#125; 返回第k大的数12345678910111213141516171819202122//返回第k大的数，优先级队列，默认是小顶堆public class KthLargestElement1 &#123; public int findKthLargestElement(int[] nums, int k) &#123; PriorityQueue&lt;Integer&gt; queue = new PriorityQueue&lt;&gt;(); for (int num : nums) &#123; if (queue.size() &lt; k) &#123; queue.offer(num); &#125; else if (num &gt; queue.peek()) &#123; queue.poll(); queue.offer(num); &#125; &#125; return queue.peek(); &#125; public static void main(String[] args) &#123; int[] nums = &#123;3,6,8,4,1,2,9&#125;; int k = 2; KthLargestElement1 kthLargestElement1 = new KthLargestElement1(); System.out.println(&quot;第&quot; + k + &quot;大的数是: &quot; + kthLargestElement1.findKthLargestElement(nums, k)); &#125;&#125; 基础快速排序1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public class QuickSort &#123; public void quickSort(int[] arr) &#123; if (arr == null || arr.length &lt;= 1) &#123; return; &#125; quickSort(arr, 0, arr.length - 1); &#125; public void quickSort(int[] arr, int left, int right) &#123; if (left &lt;= right) &#123; return; &#125; // 获取分区点位置 int pivot = partition(arr, left, right); // 递归排序左右子数组 quickSort(arr, left, pivot - 1); quickSort(arr, pivot + 1, right); &#125; public int partition(int[] arr, int left, int right) &#123; // 选择最右元素作为基准 int pivot = arr[right]; // i 指向小于基准的区域的边界 int i = left - 1; for (int j = left; j &lt; right; j++) &#123; // 如果当前元素小于等于基准 if (arr[j] &lt; pivot) &#123; i++; swap(arr, i, j); &#125; &#125; // 将基准放到正确位置 swap(arr, i + 1, right); return i + 1; &#125; public void swap(int[] arr, int i, int j) &#123; int temp = arr[i]; arr[i] = arr[j]; arr[j] = temp; &#125;&#125;"},{"title":"线程协作","path":"/2022/11/05/线程协作/","content":"10个线程模拟赛马，所有马就绪后才能开跑，所有马到达终点后裁判宣布赛马成绩1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859public class HorseRace &#123; //记录赛马成绩 private static final List&lt;String&gt; result = Collections.synchronizedList(new ArrayList&lt;&gt;()); private static final CountDownLatch countDownLatch = new CountDownLatch(1); private static final CyclicBarrier barrier = new CyclicBarrier(10, new Runnable() &#123; @Override public void run() &#123; for (String res : result) &#123; System.out.println(res); &#125; &#125; &#125;); public static void main(String[] args) &#123; for (int i = 1; i &lt;= 10; i++) &#123; Thread thread = new Thread(new Horse(&quot;horse &quot; + i)); thread.start(); &#125; //所有马就绪后开始跑 countDownLatch.countDown(); &#125; public static class Horse implements Runnable &#123; private String name; public Horse(String name) &#123; this.name = name; &#125; @Override public void run() &#123; try &#123; //等待所有马准备好，等待开跑信号，count不为0就一直阻塞 countDownLatch.await(); &#125; catch (InterruptedException e) &#123; throw new RuntimeException(e); &#125; long time = (long)(Math.random() * 1000); try &#123; Thread.sleep(time); &#125; catch (InterruptedException e) &#123; throw new RuntimeException(e); &#125; result.add(name + &quot; 用时 &quot; + time); try &#123; //数量不到10个就阻塞 barrier.await(); &#125; catch (InterruptedException | BrokenBarrierException e) &#123; throw new RuntimeException(e); &#125; &#125; &#125;&#125; 两个线程，一个打印123，一个打印ABC，交替输出1A2B3C123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051public class NumberLetterPrinter &#123; private static final Object lock = new Object(); private static boolean printNumber = true; public static void main(String[] args) &#123; Thread t1 = new Thread(new NumberPrinter()); Thread t2 = new Thread(new LetterPrinter()); t1.start(); t2.start(); &#125; public static class NumberPrinter implements Runnable &#123; @Override public void run() &#123; synchronized (lock) &#123; for (int i = 1; i &lt;= 3; i++) &#123; while (!printNumber) &#123; try &#123; lock.wait(); &#125; catch (InterruptedException e) &#123; throw new RuntimeException(e); &#125; &#125; System.out.print(i); printNumber = false; lock.notify(); &#125; &#125; &#125; &#125; public static class LetterPrinter implements Runnable &#123; @Override public void run() &#123; synchronized (lock) &#123; for (char c = &#x27;A&#x27;; c &lt;= &#x27;C&#x27;; c++) &#123; while (printNumber) &#123; try &#123; lock.wait(); &#125; catch (InterruptedException e) &#123; throw new RuntimeException(e); &#125; &#125; System.out.print(c); printNumber = true; lock.notify(); &#125; &#125; &#125; &#125;&#125; 两个线程，一个打印奇数，一个打印偶数，然后顺序打印出1-10012345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455public class SequentialPrinter &#123; private static final Object lock = new Object(); private static int number = 1; private static final int max = 100; public static void main(String[] args) &#123; Thread t1 = new Thread(new OddPrinter()); Thread t2 = new Thread(new EvenPrinter()); t1.start(); t2.start(); &#125; public static class OddPrinter implements Runnable &#123; @Override public void run() &#123; while (number &lt;= max) &#123; synchronized (lock) &#123; if (number % 2 == 0) &#123; try &#123; lock.wait(); &#125; catch (InterruptedException e) &#123; throw new RuntimeException(e); &#125; &#125; else &#123; System.out.println(number); number++; lock.notify(); &#125; &#125; &#125; &#125; &#125; public static class EvenPrinter implements Runnable &#123; @Override public void run() &#123; while (number &lt;= max) &#123; synchronized (lock) &#123; if (number % 2 == 1) &#123; try &#123; lock.wait(); &#125; catch (InterruptedException e) &#123; throw new RuntimeException(e); &#125; &#125; else &#123; System.out.println(number); number++; lock.notify(); &#125; &#125; &#125; &#125; &#125;&#125; 五个线程abcde，想先执行a，再执行bcd，bcd执行完后执行e如何做12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758public class ThreadOrderControl &#123; public static void main(String[] args) &#123; CountDownLatch a = new CountDownLatch(1); CountDownLatch bcd = new CountDownLatch(3); Thread ta = new Thread(new Runnable() &#123; @Override public void run() &#123; System.out.println(&quot;a&quot;); a.countDown(); &#125; &#125;); Thread b = new Thread(new Task(&quot;b&quot;, a, bcd)); Thread c = new Thread(new Task(&quot;c&quot;, a, bcd)); Thread d = new Thread(new Task(&quot;d&quot;, a, bcd)); Thread e = new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; bcd.await(); &#125; catch (InterruptedException ex) &#123; throw new RuntimeException(ex); &#125; System.out.println(&quot;e&quot;); &#125; &#125;); ta.start(); b.start(); c.start(); d.start(); e.start(); &#125; public static class Task implements Runnable &#123; private String name; private CountDownLatch a; private CountDownLatch bcd; public Task(String name, CountDownLatch a, CountDownLatch bcd) &#123; this.name = name; this.a = a; this.bcd = bcd; &#125; @Override public void run() &#123; try &#123; a.await(); &#125; catch (InterruptedException e) &#123; throw new RuntimeException(e); &#125; System.out.println(name); bcd.countDown(); &#125; &#125;&#125;"},{"title":"字符串","path":"/2022/10/23/字符串/","content":"替换数字123456789101112131415161718192021222324252627282930313233343536373839404142434445//给定一个字符串 s，它包含小写字母和数字字符，请编写一个函数，将字符串中的字母字符保持不变，而将每个数字字符替换为number。//例如，对于输入字符串 &quot;a1b2c3&quot;，函数应该将其转换为 &quot;anumberbnumbercnumber&quot;。//对于输入字符串 &quot;a5b&quot;，函数应该将其转换为 &quot;anumberb&quot;//输入：一个字符串 s,s 仅包含小写字母和数字字符。//输出：打印一个新的字符串，其中每个数字字符都被替换为了number//样例输入：a1b2c3////样例输出：anumberbnumbercnumber//数据范围：1 &lt;= s.length &lt; 10000public class ReplaceNumber &#123; public String replaceNumber(String s) &#123; int count = 0; char[] oldChars = s.toCharArray(); for (char c : oldChars) &#123; if (Character.isDigit(c)) &#123; count++; &#125; &#125; char[] newChars = new char[oldChars.length + count * 5]; System.arraycopy(oldChars, 0, newChars, 0, oldChars.length); for (int i = newChars.length - 1, j = oldChars.length - 1; j &lt; i; i--, j--) &#123; if (!Character.isDigit(oldChars[j])) &#123; newChars[i] = oldChars[j]; &#125; else &#123; newChars[i] = &#x27;r&#x27;; newChars[i - 1] = &#x27;e&#x27;; newChars[i - 2] = &#x27;b&#x27;; newChars[i - 3] = &#x27;m&#x27;; newChars[i - 4] = &#x27;u&#x27;; newChars[i - 5] = &#x27;n&#x27;; i -= 5; &#125; &#125; return new String(newChars); &#125; public static void main(String[] args) &#123; ReplaceNumber replaceNumber = new ReplaceNumber(); System.out.println(replaceNumber.replaceNumber(&quot;a1b2c3&quot;)); &#125;&#125; 反转字符串1234567891011121314151617181920212223//编写一个函数，其作用是将输入的字符串反转过来。输入字符串以字符数组 char[] 的形式给出。//不要给另外的数组分配额外的空间，你必须原地修改输入数组、使用 O(1) 的额外空间解决这一问题。//你可以假设数组中的所有字符都是 ASCII 码表中的可打印字符。//示例 1：//输入：[&quot;h&quot;,&quot;e&quot;,&quot;l&quot;,&quot;l&quot;,&quot;o&quot;]//输出：[&quot;o&quot;,&quot;l&quot;,&quot;l&quot;,&quot;e&quot;,&quot;h&quot;]////示例 2：//输入：[&quot;H&quot;,&quot;a&quot;,&quot;n&quot;,&quot;n&quot;,&quot;a&quot;,&quot;h&quot;]//输出：[&quot;h&quot;,&quot;a&quot;,&quot;n&quot;,&quot;n&quot;,&quot;a&quot;,&quot;H&quot;]public void reverseString1(char[] chars) &#123; String str = &quot;ww&quot;; int i = 0; int j = chars.length - 1; while (i &lt; j) &#123; char temp = chars[i]; chars[i] = chars[j]; chars[j] = temp; i++; j--; &#125;&#125; 反转字符串II12345678910111213141516171819202122232425262728293031//给定一个字符串 s 和一个整数 k，从字符串开头算起, 每计数至 2k 个字符，就反转这 2k 个字符中的前 k 个字符。//如果剩余字符少于 k 个，则将剩余字符全部反转。//如果剩余字符小于 2k 但大于或等于 k 个，则反转前 k 个字符，其余字符保持原样。//示例://输入: s = &quot;abcdefg&quot;, k = 2//输出: &quot;bacdfeg&quot;public String reverseString2(String s, int k) &#123; char[] chars = s.toCharArray(); for (int i = 0; i &lt; chars.length; i += 2 * k) &#123; int start = i; // 判断尾数够不够k个来取决end指针的位置 int end = Math.min(chars.length - 1, start + k - 1); while (start &lt; end) &#123; char temp = chars[start]; chars[start] = chars[end]; chars[end] = temp; start++; end--; &#125; &#125; return new String(chars);&#125;public static void main(String[] args) &#123; ReverseString reverseString = new ReverseString(); System.out.println(reverseString.reverseString2(&quot;abcdefg&quot;, 2));&#125; 翻转字符串里的单词1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768public class ReverseWords &#123; /** * 不使用Java内置方法实现 * &lt;p&gt; * 1.去除首尾以及中间多余空格 * 2.反转整个字符串 * 3.反转各个单词 */ public String reverseWords(String s) &#123; // System.out.println(&quot;ReverseWords.reverseWords2() called with: s = [&quot; + s + &quot;]&quot;); // 1.去除首尾以及中间多余空格 StringBuilder stringBuilder = removeSpace(s); // 2.反转整个字符串 reverse(stringBuilder, 0, stringBuilder.length() - 1); // 3.反转各个单词 reverseEachWord(stringBuilder); return stringBuilder.toString(); &#125; public void reverseEachWord(StringBuilder stringBuilder) &#123; int start = 0, end = 1, n = stringBuilder.length(); while (start &lt; n) &#123; while (end &lt; n &amp;&amp; stringBuilder.charAt(end) != &#x27; &#x27;) &#123; end++; &#125; reverse(stringBuilder, start, end - 1); start = end + 1; end = start + 1; &#125; &#125; public StringBuilder removeSpace(String s) &#123; int start = 0; int end = s.length() - 1; while (s.charAt(start) == &#x27; &#x27;) &#123; start++; &#125; while (s.charAt(end) == &#x27; &#x27;) &#123; end--; &#125; StringBuilder stringBuilder = new StringBuilder(); while (start &lt;= end) &#123; if (s.charAt(start) != &#x27; &#x27; || stringBuilder.charAt(stringBuilder.length() - 1) != &#x27; &#x27;) &#123; stringBuilder.append(s.charAt(start)); start++; &#125; &#125; return stringBuilder; &#125; public void reverse(StringBuilder stringBuilder, int start, int end) &#123; while (start &lt; end) &#123; char temp = stringBuilder.charAt(start); stringBuilder.setCharAt(start, stringBuilder.charAt(end)); stringBuilder.setCharAt(end, temp); start++; end--; &#125; &#125; public static void main(String[] args) &#123; System.out.println(&quot; w3 23 &quot;.replaceFirst(&quot; &quot;, &quot;&quot;)); System.out.println(&quot; w3 23 &quot;.trim()); ReverseWords reverseWords = new ReverseWords(); System.out.println(reverseWords.reverseWords(&quot; hello world! &quot;)); &#125;&#125; 右旋字符串12345678910111213141516171819202122232425262728293031323334//字符串的右旋转操作是把字符串尾部的若干个字符转移到字符串的前面。给定一个字符串 s 和一个正整数 k，请编写一个函数，将字符串中的后面 k 个字符移到字符串的前面，实现字符串的右旋转操作。//例如，对于输入字符串 &quot;abcdefg&quot; 和整数 2，函数应该将其转换为 &quot;fgabcde&quot;。//输入：输入共包含两行，第一行为一个正整数 k，代表右旋转的位数。第二行为字符串 s，代表需要旋转的字符串。//输出：输出共一行，为进行了右旋转操作后的字符串。public class RightReverse &#123; public String rightReverse(String s, int k) &#123; char[] chars = s.toCharArray(); //整体翻转 reverseString(chars, 0, chars.length - 1); //第一段翻转 reverseString(chars, 0, k - 1); //第二段翻转 reverseString(chars, k, chars.length - 1); return new String(chars); &#125; public void reverseString(char[] chars, int start, int end) &#123; while (start &lt; end) &#123; char temp = chars[start]; chars[start] = chars[end]; chars[end] = temp; start++; end--; &#125; &#125; public static void main(String[] args) &#123; RightReverse rightReverse = new RightReverse(); String s = &quot;abcdefg&quot;; System.out.println(rightReverse.rightReverse(s, 2)); &#125;&#125;"},{"title":"算法-其他","path":"/2022/10/23/算法-其他/","content":"手写LRU缓存1234567891011121314151617181920212223242526272829303132333435public class LRUCache3&lt;K, V&gt; &#123; private int capacity; private Map&lt;K, V&gt; cache; private LinkedList&lt;K&gt; keyList; public LRUCache3(int capacity) &#123; this.capacity = capacity; cache = new HashMap&lt;&gt;(); keyList = new LinkedList&lt;&gt;(); &#125; public void put(K key, V value) &#123; if (cache.containsKey(key)) &#123; cache.remove(key); &#125; if (cache.size() &gt;= capacity) &#123; K k = keyList.removeFirst(); cache.remove(k); &#125; cache.put(key, value); keyList.addLast(key); &#125; public V get(K key) &#123; if (cache.containsKey(key)) &#123; keyList.removeFirst(); keyList.addLast(key); return cache.get(key); &#125; return null; &#125;&#125; 找出扑克牌中丢失的那张牌12345678910111213141516171819202122232425262728293031public class MissingCardFinder &#123; public int missingCardFinder(int[] cards) &#123; int result = 0; for (int card : cards) &#123; result ^= card; &#125; for (int i = 0; i &lt; 54; i++) &#123; result ^= i; &#125; return result; &#125; public static void main(String[] args) &#123; int[] cards = new int[53]; int missingCard = 35; int index = 0; for (int i = 0; i &lt; 54; i++) &#123; if (i != missingCard) &#123; cards[index] = i; index++; &#125; &#125; MissingCardFinder missingCardFinder = new MissingCardFinder(); System.out.println(missingCardFinder.missingCardFinder(cards)); &#125;&#125; 找出101-200有多少个质数，并输出所有质数，只能被1和它本身整除12345678910111213141516171819202122232425public class Prime &#123; public static void main(String[] args) &#123; int count = 0; for (int i = 101; i &lt;= 200; i++) &#123; if (isPrime(i)) &#123; count++; System.out.println(i); &#125; &#125; System.out.println(&quot;一共有&quot; + count + &quot;个质数&quot;); &#125; public static boolean isPrime(int num) &#123; if (num &lt;= 1) &#123; return false; &#125; for (int i = 2; i &lt; Math.sqrt(num); i++) &#123; if (num % i == 0) &#123; return false; &#125; &#125; return true; &#125;&#125;"},{"title":"栈","path":"/2022/10/23/栈/","content":"队列实现栈1234567891011121314151617181920212223242526272829303132333435363738394041424344public class QueueToStack&lt;E&gt; &#123; private Queue&lt;E&gt; queue; private Queue&lt;E&gt; tempQueue; public QueueToStack() &#123; queue = new LinkedList&lt;&gt;(); tempQueue = new LinkedList&lt;&gt;(); &#125; public void offer(E element)&#123; queue.offer(element); &#125; public E poll()&#123; if (queue.isEmpty()) &#123; throw new EmptyStackException(); &#125; while (queue.size() &gt; 1) &#123; tempQueue.offer(queue.poll()); &#125; E element = queue.poll(); Queue&lt;E&gt; temp = queue; queue = tempQueue; tempQueue = temp; return element; &#125; public E peek() &#123; if (queue.isEmpty()) &#123; throw new EmptyStackException(); &#125; while (queue.size() &gt; 1) &#123; tempQueue.offer(queue.poll()); &#125; E element = queue.poll(); tempQueue.offer(element); Queue&lt;E&gt; temp = queue; queue = tempQueue; tempQueue = temp; return element; &#125;&#125; 逆波兰表达式求值123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293//根据 逆波兰表示法，求表达式的值。////有效的运算符包括 + , - , * , / 。每个运算对象可以是整数，也可以是另一个逆波兰表达式。////说明：////整数除法只保留整数部分。 给定逆波兰表达式总是有效的。换句话说，表达式总会得出有效数值且不存在除数为 0 的情况。////示例 1：////输入: [&quot;2&quot;, &quot;1&quot;, &quot;+&quot;, &quot;3&quot;, &quot;*&quot;]//输出: 9//解释: 该算式转化为常见的中缀算术表达式为：((2 + 1) * 3) = 9//示例 2：////输入: [&quot;4&quot;, &quot;13&quot;, &quot;5&quot;, &quot;/&quot;, &quot;+&quot;]//输出: 6//解释: 该算式转化为常见的中缀算术表达式为：(4 + (13 / 5)) = 6//示例 3：////输入: [&quot;10&quot;, &quot;6&quot;, &quot;9&quot;, &quot;3&quot;, &quot;+&quot;, &quot;-11&quot;, &quot;*&quot;, &quot;/&quot;, &quot;*&quot;, &quot;17&quot;, &quot;+&quot;, &quot;5&quot;, &quot;+&quot;]////输出: 22////解释:该算式转化为常见的中缀算术表达式为：////((10 * (6 / ((9 + 3) * -11))) + 17) + 5//= ((10 * (6 / (12 * -11))) + 17) + 5//= ((10 * (6 / -132)) + 17) + 5//= ((10 * 0) + 17) + 5//= (0 + 17) + 5//= 17 + 5//= 22//逆波兰表达式：是一种后缀表达式，所谓后缀就是指运算符写在后面。////平常使用的算式则是一种中缀表达式，如 ( 1 + 2 ) * ( 3 + 4 ) 。////该算式的逆波兰表达式写法为 ( ( 1 2 + ) ( 3 4 + ) * ) 。////逆波兰表达式主要有以下两个优点：////去掉括号后表达式无歧义，上式即便写成 1 2 + 3 4 + * 也可以依据次序计算出正确结果。////适合用栈操作运算：遇到数字则入栈；遇到运算符则取出栈顶两个数字进行计算，并将结果压入栈中。import java.util.Deque;import java.util.LinkedList;//题外话//我们习惯看到的表达式都是中缀表达式，因为符合我们的习惯，但是中缀表达式对于计算机来说就不是很友好了。////例如：4 + 13 / 5，这就是中缀表达式，计算机从左到右去扫描的话，扫到13，还要判断13后面是什么运算符，还要比较一下优先级，然后13还和后面的5做运算，做完运算之后，还要向前回退到 4 的位置，继续做加法，你说麻不麻烦！////那么将中缀表达式，转化为后缀表达式之后：[&quot;4&quot;, &quot;13&quot;, &quot;5&quot;, &quot;/&quot;, &quot;+&quot;] ，就不一样了，计算机可以利用栈来顺序处理，不需要考虑优先级了。也不用回退了， 所以后缀表达式对计算机来说是非常友好的。////可以说本题不仅仅是一道好题，也展现出计算机的思考方式。////在1970年代和1980年代，惠普在其所有台式和手持式计算器中都使用了RPN（后缀表达式），直到2020年代仍在某些模型中使用了RPN。////参考维基百科如下：////During the 1970s and 1980s, Hewlett-Packard used RPN in all of their desktop and hand-held calculators, and continued to use it in some models into the 2020spublic class EvalRPN &#123; public int evalRPN(String[] array) &#123; Deque&lt;Integer&gt; deque = new LinkedList&lt;&gt;(); for (String s : array) &#123; if (&quot;+&quot;.equals(s)) &#123; deque.push(deque.pop() + deque.pop()); &#125; else if (&quot;-&quot;.equals(s)) &#123; deque.push(-deque.pop() + deque.pop()); &#125; else if (&quot;*&quot;.equals(s)) &#123; deque.push(deque.pop() * deque.pop()); &#125; else if (&quot;/&quot;.equals(s)) &#123; Integer i1 = deque.pop(); Integer i2 = deque.pop(); deque.push(i2 / i1); &#125; else &#123; deque.push(Integer.valueOf(s)); &#125; &#125; return deque.pop(); &#125; public static void main(String[] args) &#123; System.out.println(Integer.getInteger(&quot;2&quot;)); EvalRPN evalRPN = new EvalRPN(); System.out.println(evalRPN.evalRPN(new String[]&#123;&quot;2&quot;, &quot;1&quot;, &quot;+&quot;, &quot;3&quot;, &quot;*&quot;&#125;)); System.out.println(evalRPN.evalRPN(new String[]&#123;&quot;10&quot;, &quot;6&quot;, &quot;9&quot;, &quot;3&quot;, &quot;+&quot;, &quot;-11&quot;, &quot;*&quot;, &quot;/&quot;, &quot;*&quot;, &quot;17&quot;, &quot;+&quot;, &quot;5&quot;, &quot;+&quot;&#125;)); &#125;&#125; 删除字符串中的所有相邻重复项123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081//给出由小写字母组成的字符串 S，重复项删除操作会选择两个相邻且相同的字母，并删除它们。////在 S 上反复执行重复项删除操作，直到无法继续删除。////在完成所有重复项删除操作后返回最终的字符串。答案保证唯一。////示例：////输入：&quot;abbaca&quot;//输出：&quot;ca&quot;//解释：例如，在 &quot;abbaca&quot; 中，我们可以删除 &quot;bb&quot; 由于两字母相邻且相同，这是此时唯一可以执行删除操作的重复项。之后我们得到字符串 &quot;aaca&quot;，其中又只有 &quot;aa&quot; 可以执行重复项删除操作，所以最后的字符串为 &quot;ca&quot;。//提示：////1 &lt;= S.length &lt;= 20000//S 仅由小写英文字母组成import java.util.ArrayDeque;//题外话//这道题目就像是我们玩过的游戏对对碰，如果相同的元素挨在一起就要消除。////可能我们在玩游戏的时候感觉理所当然应该消除，但程序又怎么知道该如何消除呢，特别是消除之后又有新的元素可能挨在一起。////此时游戏的后端逻辑就可以用一个栈来实现（我没有实际考察对对碰或者爱消除游戏的代码实现，仅从原理上进行推断）。////游戏开发可能使用栈结构，编程语言的一些功能实现也会使用栈结构，实现函数递归调用就需要栈，但不是每种编程语言都支持递归，例如：public class RemoveDuplicates &#123; public String removeDuplicates1(String s) &#123; //ArrayDeque会比LinkedList在除了删除元素这一点外会快一点 //参考：https://stackoverflow.com/questions/6163166/why-is-arraydeque-better-than-linkedlist ArrayDeque&lt;Character&gt; deque = new ArrayDeque&lt;&gt;(); for (int i = 0; i &lt; s.length(); i++) &#123; char c = s.charAt(i); if (deque.isEmpty() || c != deque.peek()) &#123; deque.push(c); &#125; else &#123; deque.pop(); &#125; &#125; StringBuilder result = new StringBuilder(); //剩余的元素即为不重复的元素 while (!deque.isEmpty()) &#123; result.insert(0, deque.pop()); &#125; return result.toString(); &#125; public String removeDuplicates2(String s) &#123; // 将 res 当做栈 // 也可以用 StringBuffer 来修改字符串 // StringBuffer res = new StringBuffer(); StringBuilder res = new StringBuilder(); // top为 res 的长度 int top = -1; for (int i = 0; i &lt; s.length(); i++) &#123; char c = s.charAt(i); // 当 top &gt;= 0,即栈中有字符时，当前字符如果和栈中字符相等，弹出栈顶字符，同时 top-- if (top &gt;= 0 &amp;&amp; s.charAt(top) == c) &#123; res.deleteCharAt(top); top--; // 否则，将该字符 入栈，同时top++ &#125; else &#123; res.append(c); top++; &#125; &#125; return res.toString(); &#125; public static void main(String[] args) &#123; RemoveDuplicates removeDuplicates = new RemoveDuplicates(); System.out.println(removeDuplicates.removeDuplicates1(&quot;accabv&quot;)); System.out.println(removeDuplicates.removeDuplicates2(&quot;accabv&quot;)); &#125;&#125; 给定一个整数数组 temperatures ，表示每天的温度，返回一个数组 answer ，其中 answer[i] 是指对于第 i 天，下一个更高温度出现在几天后。如果气温在这之后都不会升高，请在该位置用 0 来代替。123456789101112131415161718192021public class Temperatures &#123; public int[] dailyTemperatures(int[] temperatures) &#123; Deque&lt;Integer&gt; deque = new LinkedList&lt;&gt;(); int[] answers = new int[temperatures.length]; for (int i = 0; i &lt; temperatures.length; i++) &#123; while (!deque.isEmpty() &amp;&amp; temperatures[i] &gt; temperatures[deque.peek()]) &#123; int index = deque.pop(); answers[index] = i - index; &#125; deque.push(i); &#125; return answers; &#125; public static void main(String[] args) &#123; int[] temperatures = &#123;73,74,75,71,69,72,76,73&#125;; Temperatures t = new Temperatures(); System.out.println(Arrays.toString(t.dailyTemperatures(temperatures))); &#125;&#125; 前 K 个高频元素1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950//给定一个非空的整数数组，返回其中出现频率前 k 高的元素。////示例 1:////输入: nums = [1,1,1,2,2,3], k = 2//输出: [1,2]//示例 2:////输入: nums = [1], k = 1//输出: [1]//提示：////你可以假设给定的 k 总是合理的，且 1 ≤ k ≤ 数组中不相同的元素的个数。//你的算法的时间复杂度必须优于 $O(n \\log n)$ , n 是数组的大小。//题目数据保证答案唯一，换句话说，数组中前 k 个高频元素的集合是唯一的。//你可以按任意顺序返回答案public class TopKFrequent &#123; public int[] topKFrequent(int[] nums, int k) &#123; Map&lt;Integer, Integer&gt; map = new HashMap&lt;&gt;(); for (int num : nums) &#123; map.put(num, map.getOrDefault(num, 0) + 1); &#125; PriorityQueue&lt;int[]&gt; queue = new PriorityQueue&lt;&gt;(Comparator.comparingInt(pair -&gt; pair[1])); for (Map.Entry&lt;Integer, Integer&gt; entry : map.entrySet()) &#123; if (queue.size() &lt; k) &#123; queue.add(new int[]&#123;entry.getKey(), entry.getValue()&#125;); &#125; else &#123; if (entry.getValue() &gt; queue.peek()[1]) &#123; queue.poll(); queue.add(new int[]&#123;entry.getKey(), entry.getValue()&#125;); &#125; &#125; &#125; int[] ans = new int[k]; for (int i = k - 1; i &gt;= 0; i--) &#123; ans[i] = queue.poll()[0]; &#125; return ans; &#125; public static void main(String[] args) &#123; TopKFrequent topKFrequent = new TopKFrequent(); System.out.println(Arrays.toString(topKFrequent.topKFrequent(new int[]&#123;1, 1, 1, 2, 2, 3&#125;, 2))); &#125;&#125; 有效的括号 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273//给定一个只包括 &#x27;(&#x27;，&#x27;)&#x27;，&#x27;&#123;&#x27;，&#x27;&#125;&#x27;，&#x27;[&#x27;，&#x27;]&#x27; 的字符串，判断字符串是否有效。////有效字符串需满足：////左括号必须用相同类型的右括号闭合。//左括号必须以正确的顺序闭合。//注意空字符串可被认为是有效字符串。//示例 1:////输入: &quot;()&quot;//输出: true//示例 2:////输入: &quot;()[]&#123;&#125;&quot;//输出: true//示例 3:////输入: &quot;(]&quot;//输出: false//示例 4:////输入: &quot;([)]&quot;//输出: false//示例 5:////输入: &quot;&#123;[]&#125;&quot;//输出: truepublic class Valid &#123; public boolean isValid1(String s) &#123; char[] chars = s.toCharArray(); Deque&lt;Character&gt; deque = new LinkedList&lt;&gt;(); for (char c : chars) &#123; if (c == &#x27;(&#x27;) &#123; deque.push(&#x27;)&#x27;); &#125; else if (c == &#x27;&#123;&#x27;) &#123; deque.push(&#x27;&#125;&#x27;); &#125; else if (c == &#x27;[&#x27;) &#123; deque.push(&#x27;]&#x27;); &#125; else if (deque.isEmpty() || c != deque.peek()) &#123; return false; &#125; else &#123; deque.pop(); &#125; &#125; return deque.isEmpty(); &#125; // 对应的另一半一定在栈顶 public boolean isValid2(String s) &#123; Stack&lt;Character&gt; stack = new Stack&lt;&gt;(); for (int i = 0; i &lt; s.length(); i++) &#123; // 有对应的另一半就直接消消乐 char c = s.charAt(i); if (c == &#x27;)&#x27; &amp;&amp; !stack.isEmpty() &amp;&amp; stack.peek() == &#x27;(&#x27;) &#123; stack.pop(); &#125; else if (c == &#x27;]&#x27; &amp;&amp; !stack.isEmpty() &amp;&amp; stack.peek() == &#x27;[&#x27;) &#123; stack.pop(); &#125; else if (c == &#x27;&#125;&#x27; &amp;&amp; !stack.isEmpty() &amp;&amp; stack.peek() == &#x27;&#123;&#x27;) &#123; stack.pop(); &#125; else &#123; stack.push(c); &#125; &#125; return stack.isEmpty(); &#125; public static void main(String[] args) &#123; Valid valid = new Valid(); System.out.println(valid.isValid1(&quot;(())&#123;&#123;[[]]&#125;&#125;&quot;)); System.out.println(valid.isValid2(&quot;(())&#123;&#123;[[]]&#125;&#125;&quot;)); &#125;&#125;"},{"title":"队列","path":"/2022/10/23/队列/","content":"栈实现队列123456789101112131415161718192021222324public class StackToQueue&lt;T&gt; &#123; private Stack&lt;T&gt; inStack; private Stack&lt;T&gt; outStack; public StackToQueue() &#123; inStack = new Stack&lt;&gt;(); outStack = new Stack&lt;&gt;(); &#125; public void inQueue(T t) &#123; inStack.push(t); &#125; public T outQueue() &#123; if (outStack.isEmpty()) &#123; while (!inStack.isEmpty()) &#123; outStack.push(inStack.pop()); &#125; &#125; return outStack.pop(); &#125;&#125;"},{"title":"二叉树","path":"/2022/08/30/二叉树/","content":"定义树 1234567891011public class TreeNode &#123; int val; TreeNode left; TreeNode right; public TreeNode (int val, TreeNode left, TreeNode right) &#123; this.val = val; this.left = left; this.right = right; &#125;&#125; 前序遍历-递归 1234567891011121314public List&lt;Integer&gt; preorder(TreeNode treeNode) &#123; List&lt;Integer&gt; result = new ArrayList&lt;&gt;(); preorderHelper(treeNode, result); return result;&#125;private void preorderHelper(TreeNode treeNode, List&lt;Integer&gt; result) &#123; if (treeNode == null) &#123; return; &#125; result.add(treeNode.val); preorderHelper(treeNode.left, result); preorderHelper(treeNode.right, result);&#125; 中序遍历-递归 1234567891011121314public List&lt;Integer&gt; inorder(TreeNode treeNode)&#123; List&lt;Integer&gt; result = new ArrayList&lt;&gt;(); inorderHelper(treeNode, result); return result;&#125;private void inorderHelper(TreeNode treeNode, List&lt;Integer&gt; result) &#123; if (treeNode == null) &#123; return; &#125; inorderHelper(treeNode.left, result); result.add(treeNode.val); inorderHelper(treeNode.right, result);&#125; 后序遍历-递归 123456789101112131415public List&lt;Integer&gt; postorder(TreeNode treeNode) &#123; List&lt;Integer&gt; result = new ArrayList&lt;&gt;(); postorderHelper(treeNode, result); return result;&#125;private void postorderHelper(TreeNode treeNode, List&lt;Integer&gt; result) &#123; if (treeNode == null) &#123; return; &#125; postorderHelper(treeNode.left, result); postorderHelper(treeNode.right, result); result.add(treeNode.val);&#125; 前序遍历-迭代,根左右，入栈顺序：根右左 1234567891011121314151617181920public List&lt;Integer&gt; preorderTraversal(TreeNode root) &#123; List&lt;Integer&gt; result = new ArrayList&lt;&gt;(); if (root == null) &#123; return result; &#125; Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;(); stack.push(root); while (!stack.isEmpty()) &#123; TreeNode node = stack.pop(); result.add(node.val); if (node.right != null) &#123; stack.push(node.right); &#125; if (node.left != null) &#123; stack.push(node.left); &#125; &#125; return result;&#125; 中序遍历-迭代 左根右 入栈顺序 左-右 1234567891011121314151617181920public List&lt;Integer&gt; inorderTraversal(TreeNode root) &#123; List&lt;Integer&gt; result = new ArrayList&lt;&gt;(); if (root == null) &#123; return result; &#125; Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;(); TreeNode curr = root; while (curr != null || !stack.isEmpty()) &#123; if (curr != null) &#123; stack.push(curr); curr = curr.left; &#125; else &#123; curr = stack.pop(); result.add(curr.val); curr = curr.right; &#125; &#125; return result;&#125; 后续遍历-迭代 左右根，入栈是根左右，出栈是根右左，再反转后变为左右根 12345678910111213141516171819202122public List&lt;Integer&gt; postorderTraversal(TreeNode root) &#123; List&lt;Integer&gt; result = new ArrayList&lt;&gt;(); if (root == null) &#123; return result; &#125; Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;(); stack.push(root); while (!stack.isEmpty()) &#123; TreeNode node = stack.pop(); result.add(node.val); if (node.left != null) &#123; stack.push(node.left); &#125; if (node.right != null) &#123; stack.push(node.right); &#125; &#125; Collections.reverse(result); return result;&#125; 第k个最小元素 123456789101112131415161718192021222324public class KthSmallest &#123; private int count; private Integer result = Integer.MIN_VALUE; public Integer kthSmallest(TreeNode node, int k) &#123; kthSmallestHelper(node, k); return result; &#125; public void kthSmallestHelper(TreeNode node, int k) &#123; if (node == null) &#123; return; &#125; kthSmallestHelper(node.left, k); count++; if (count == k) &#123; result = node.val; return; &#125; kthSmallestHelper(node.right, k); &#125;&#125; 二叉树的层序遍历 123456789101112131415161718192021222324252627282930313233343536//层序遍历一个二叉树。就是从左到右一层一层的去遍历二叉树。这种遍历的方式和我们之前讲过的都不太一样。//需要借用一个辅助数据结构即队列来实现，队列先进先出，符合一层一层遍历的逻辑，而用栈先进后出适合模拟深度优先遍历也就是递归的逻辑。//而这种层序遍历方式就是图论中的广度优先遍历，只不过我们应用在二叉树上。public class LevelOrder &#123; public List&lt;List&lt;Integer&gt;&gt; levelOrder(TreeNode root) &#123; List&lt;List&lt;Integer&gt;&gt; result = new ArrayList&lt;&gt;(); if (root == null) &#123; return result; &#125; Queue&lt;TreeNode&gt; queue = new LinkedList&lt;&gt;(); queue.offer(root); while (!queue.isEmpty()) &#123; List&lt;Integer&gt; item = new ArrayList&lt;&gt;(); int len = queue.size(); while (len &gt; 0) &#123; TreeNode node = queue.poll(); item.add(node.val); if (node.left != null) &#123; queue.offer(node.left); &#125; if (node.right != null) &#123; queue.offer(node.right); &#125; len--; &#125; result.add(item); &#125; return result; &#125;&#125; 最大深度 12345678910111213141516171819202122232425262728public class MaxDepth &#123; public int maxDepth(TreeNode root) &#123; if (root == null) &#123; return 0; &#125; Deque&lt;TreeNode&gt; deque = new LinkedList&lt;&gt;(); deque.offer(root); int depth = 0; while (!deque.isEmpty()) &#123; int size = deque.size(); depth++; for (int i = 0; i &lt; size; i++) &#123; TreeNode node = deque.pop(); if (node.left != null) &#123; deque.offer(node.left); &#125; if (node.right != null) &#123; deque.offer(node.right); &#125; &#125; &#125; return depth; &#125;&#125; 最小深度 12345678910111213141516171819202122232425262728293031public class MinDepth &#123; public int minDepth(TreeNode root) &#123; if (root == null) &#123; return 0; &#125; Deque&lt;TreeNode&gt; deque = new LinkedList&lt;&gt;(); deque.offer(root); int depth = 0; while (!deque.isEmpty()) &#123; int size = deque.size(); depth++; for (int i = 0; i &lt; size; i++) &#123; TreeNode node = deque.pop(); // 是叶子结点，直接返回depth，因为从上往下遍历，所以该值就是最小值 if (node.left == null &amp;&amp; node.right == null) &#123; return depth; &#125; if (node.left != null) &#123; deque.offer(node.left); &#125; if (node.right != null) &#123; deque.offer(node.right); &#125; &#125; &#125; return depth; &#125;&#125;"},{"title":"单例","path":"/2022/08/12/单例/","content":"单例的实现方式 饿汉，类初始化的时候就把单例对象创建出来了123456789public class Singleton &#123; private Singleton()&#123;&#125; private static final Singleton instance = new Singleton(); public static Singleton getInstance() &#123; return instance; &#125;&#125; 懒汉，需要的时候才去创建对象，好处是避免提前创建浪费资源，缺点也很明显，就是第一次创建的时候浪费时间123456789101112131415public class Singleton &#123; private Singleton() &#123; &#125; private static Singleton singleton; public synchronized static Singleton getInstance() &#123; if (singleton == null) &#123; singleton = new Singleton(); &#125; return singleton; &#125;&#125; 双重校验锁1234567891011121314151617181920public class Singleton &#123; private Singleton() &#123; &#125; private static volatile Singleton singleton; public static Singleton getInstance() &#123; if (singleton == null) &#123; synchronized (Singleton.class) &#123; if (singleton == null) &#123; singleton = new Singleton(); &#125; &#125; &#125; return singleton; &#125;&#125; 双重校验锁-防止反射创建实例1234567891011121314151617181920212223public class Singleton &#123; private Singleton() &#123; // 防止通过反射创建实例 if (instance != null) &#123; throw new RuntimeException(&quot;Use getInstance() method to get the single instance of this class.&quot;); &#125; &#125; private static volatile Singleton instance; public static Singleton getInstance() &#123; // 第一次检查，避免不必要的同步 if (instance == null) &#123; synchronized (Singleton.class) &#123; // 第二次检查，确保只有一个实例被创建 if (instance == null) &#123; instance = new Singleton(); &#125; &#125; &#125; return instance; &#125;&#125; 内部类1234567891011121314public class Singleton &#123; private Singleton() &#123; &#125; private static class SingletonHolder &#123; private static final Singleton instance = new Singleton(); &#125; public static Singleton getInstance() &#123; return SingletonHolder.instance; &#125;&#125; 枚举 123456789public enum Singleton4 &#123; INSTANCE; private void methodA() &#123; &#125;&#125; 为什么说枚举是实现单例最好的方式《Effective Java》一书中，明确表达过一种观点： 使用枚举实现单例的方法虽然还没有广泛采用，但是单元素的枚举类型已经成为实现Singleton的最佳方法。 究其原因，主要有以下三个好处： 枚举实现的单例写法简单简单对比下“双重校验锁”方式和枚举的方式，就会发现，枚举实现单例的方式精简很多。上面的双重锁校验的代码之所以很臃肿，是因为大部分代码都是在保证线程安全。为了在保证线程安全和锁粒度之间做权衡，代码难免会写的复杂些。但是，这段代码还是有问题的，因为他无法解决反序列化会破坏单例的问题。 枚举可解决线程安全问题上面的双重锁校验的代码之所以很臃肿，是因为大部分代码都是在保证线程安全。为了在保证线程安全和锁粒度之间做权衡，代码难免会写的复杂些。但是，这段代码还是有问题的，因为他无法解决反序列化会破坏单例的问题。其实，并不是使用枚举就不需要保证线程安全，只不过线程安全的保证不需要我们关心而已。也就是说，其实在“底层”还是做了线程安全方面的保证的。那么，“底层”到底指的是什么？这就要说到关于枚举的实现了：定义枚举时使用enum和class一样，是Java中的一个关键字。就像class对应用一个Class类一样，enum也对应有一个Enum类。通过将定义好的枚举反编译，我们就能发现，其实枚举在经过javac的编译之后，会被转换成形如public final class T extends Enum的定义。而且，枚举中的各个枚举项同时通过static来定义的。如： 123public enum T &#123; SPRING,SUMMER,AUTUMN,WINTER;&#125; 反编译后代码为： 12345678910111213141516171819public final class T extends Enum&#123; //省略部分内容 public static final T SPRING; public static final T SUMMER; public static final T AUTUMN; public static final T WINTER; private static final T ENUM$VALUES[]; static &#123; SPRING = new T(&quot;SPRING&quot;, 0); SUMMER = new T(&quot;SUMMER&quot;, 1); AUTUMN = new T(&quot;AUTUMN&quot;, 2); WINTER = new T(&quot;WINTER&quot;, 3); ENUM$VALUES = (new T[] &#123; SPRING, SUMMER, AUTUMN, WINTER &#125;); &#125;&#125; 了解JVM的类加载机制的朋友应该对这部分比较清楚。static类型的属性会在类被加载过程中被初始化，当一个Java类第一次被真正使用到的时候静态资源被初始化、Java类的加载和初始化过程都是线程安全的（因为虚拟机在加载枚举的类的时候，会使用ClassLoader的loadClass方法，而这个方法使用同步代码块保证了线程安全）。所以，创建一个enum类型是线程安全的。也就是说，我们定义的一个枚举，在第一次被真正用到的时候，会被虚拟机加载并初始化，而这个初始化过程是线程安全的。而我们知道，解决单例的并发问题，主要解决的就是初始化过程中的线程安全问题。所以，由于枚举的以上特性，枚举实现的单例是天生线程安全的。 枚举实现的单例可避免被反序列化破坏所以，由于枚举的以上特性，枚举实现的单例是天生线程安全的。这意味着即使类的构造函数是私有的，反序列化仍然可以创建该类的实例，因为它不依赖于常规的构造过程。但是，枚举的反序列化并不是通过Unsafe，也不是通过反射实现的。所以，也就不会发生由于反序列化导致的单例破坏问题。 如何破坏单例单例模式主要是通过把一个类的构造方法私有化，来避免重复创建多个对象的。那么，想要破坏单例，只要想办法能够执行到这个私有的构造方法就行了。一般来说做法有使用反射及使用反序列化都可以破坏单例。 反射破坏单例以上面的双重校验锁代码为例，我们尝试通过反射技术，来破坏单例： 12345678910Singleton singleton1 = Singleton.getSingleton();//通过反射获取到构造函数 Constructor&lt;Singleton&gt; constructor = Singleton.class.getDeclaredConstructor();//将构造函数设置为可访问类型constructor.setAccessible(true);//调用构造函数的newInstance创建一个对象Singleton singleton2 = constructor.newInstance();//判断反射创建的对象和之前的对象是不是同一个对象System.out.println(s1 == s2); 以上代码，输出结果为false，也就是说通过反射技术，我们给单例对象创建出来了一个”兄弟”。 setAccessible(true)，使得反射对象在使用时应该取消 Java 语言访问检查，使得私有的构造函数能够被访问。 反序列化坏单例我们尝试通过序列化+反序列化来破坏一下单例： 123456789101112131415161718package com.hollis;import java.io.*;public class SerializableDemo1 &#123; //为了便于理解，忽略关闭流操作及删除文件操作。真正编码时千万不要忘记 //Exception直接抛出 public static void main(String[] args) throws IOException, ClassNotFoundException &#123; //Write Obj to file ObjectOutputStream oos = new ObjectOutputStream(new FileOutputStream(&quot;tempFile&quot;)); oos.writeObject(Singleton.getSingleton()); //Read Obj from file File file = new File(&quot;tempFile&quot;); ObjectInputStream ois = new ObjectInputStream(new FileInputStream(file)); Singleton newInstance = (Singleton) ois.readObject(); //判断是否是同一个对象 System.out.println(newInstance == Singleton.getSingleton()); &#125;&#125;//false 输出结构为false，说明： 通过对Singleton的序列化与反序列化得到的对象是一个新的对象，这就破坏了Singleton的单例性。 这里，在介绍如何解决这个问题之前，我们先来深入分析一下，为什么会这样？在反序列化的过程中到底发生了什么。 ObjectInputStream对象的序列化过程通过ObjectOutputStream和ObjectInputStream来实现的，那么带着刚刚的问题，分析一下ObjectInputStream的readObject方法执行情况到底是怎样的。为了节省篇幅，这里给出ObjectInputStream的readObject的调用栈： 这里看一下重点代码，readOrdinaryObject方法的代码片段： 12345678910111213141516171819202122232425private Object readOrdinaryObject(boolean unshared) throws IOException&#123; //此处省略部分代码 Object obj; try &#123; obj = desc.isInstantiable() ? desc.newInstance() : null; &#125; catch (Exception ex)&#123; throw (IOException) new InvalidClassException( desc.forClass().getName(), &quot;unable to create instance&quot;).initCause(ex); &#125; //此处省略部分代码 if (obj != null &amp;&amp; handles.lookupException(passHandle) == null &amp;&amp; desc.hasReadResolveMethod()) &#123; Object rep = desc.invokeReadResolve(obj); if (unshared &amp;&amp; rep.getClass().isArray())&#123; rep =cloneArray(rep); &#125; if (rep != obj)&#123; handles.setObject(passHandle, obj =rep); &#125; &#125; return obj;&#125; 以上中主要贴出两部分代码。先分析第一部分： 123456Object obj; try &#123; obj = desc.isInstantiable() ? desc.newInstance() : null; &#125; catch (Exception ex)&#123; throw (IOException) new InvalidClassException(desc.forClass().getName(),&quot;unable to create instance&quot;).initCause(ex); &#125; 这里创建的这个obj对象，就是本方法要返回的对象，也可以暂时理解为是ObjectInputStream的readObject返回的对象。 isInstantiable：如果一个serializable&#x2F;externalizable的类可以在运行时被实例化，那么该方法就返回true。针对serializable和externalizable我会在其他文章中介绍。desc.newInstance：该方法通过反射的方式新建一个对象。 然后看一下newInstance的源码： 1234567891011121314151617181920public T newInstance(Object ... initargs) throws InstantiationException, IllegalAccessException, IllegalArgumentException, InvocationTargetException&#123; if (!override) &#123; if (!Reflection.quickCheckMemberAccess(clazz, modifiers)) &#123; Class&lt;?&gt; caller = Reflection.getCallerClass(); checkAccess(caller, clazz, null, modifiers); &#125; &#125; if ((clazz.getModifiers() &amp; Modifier.ENUM) != 0) throw new IllegalArgumentException(&quot;Cannot reflectively create enum objects&quot;); ConstructorAccessor ca = constructorAccessor; // read volatile if (ca == null) &#123; ca = acquireConstructorAccessor(); &#125; @SuppressWarnings(&quot;unchecked&quot;) T inst = (T) ca.newInstance(initargs); return inst;&#125; 其中关键的就是T inst = (T) ca.newInstance(initargs);这一步，这里实现的话在BootstrapConstructorAccessorImpl中，实现如下： 12345678910public Object newInstance(Object[] args) throws IllegalArgumentException, InvocationTargetException&#123; try &#123; return UnsafeFieldAccessorImpl.unsafe. allocateInstance(constructor.getDeclaringClass()); &#125; catch (InstantiationException e) &#123; throw new InvocationTargetException(e); &#125;&#125; 可以看到，这里通过Java 的 Unsafe 机制来创建对象的，而不是通过调用构造函数。这意味着即使类的构造函数是私有的，反序列化仍然可以创建该类的实例，因为它不依赖于常规的构造过程。所以。到目前为止，也就可以解释，为什么序列化可以破坏单例了？ 答：序列化会通过Unsafe直接分配内存的方式来创建一个新的对象。 如何避免单例被破坏避免反射破坏单例反射是调用默认的构造函数创建出来的，只需要我们改造下构造函数，使其在反射调用的时候识别出来对象是不是被创建过就行了 12345private Singleton() &#123; if (singleton != null)&#123; throw new RuntimeException(&quot;单例对象只能创建一次... &quot;); &#125; &#125; 避免反序列化破坏单例解决反序列化的破坏单例，只需要我们自定义反序列化的策略就行了，就是说我们不要让他走默认逻辑一直调用到Unsafe创建对象，而是我们干预他的这个过程，干预方式就是在Singleton类中定义readResolve，这样就可以解决该问题： 1234567891011121314151617181920212223public class Singleton implements Serializable&#123; private Singleton() &#123; &#125; private static volatile Singleton singleton; public static Singleton getInstance() &#123; if (singleton == null) &#123; synchronized (Singleton.class) &#123; if (singleton == null) &#123; singleton = new Singleton(); &#125; &#125; &#125; return singleton; &#125; private Object readResolve() &#123; return singleton; &#125;&#125; 还是运行以下测试类： 123456789101112131415161718package com.hollis;import java.io.*;public class SerializableDemo1 &#123; //为了便于理解，忽略关闭流操作及删除文件操作。真正编码时千万不要忘记 //Exception直接抛出 public static void main(String[] args) throws IOException, ClassNotFoundException &#123; //Write Obj to file ObjectOutputStream oos = new ObjectOutputStream(new FileOutputStream(&quot;tempFile&quot;)); oos.writeObject(Singleton.getSingleton()); //Read Obj from file File file = new File(&quot;tempFile&quot;); ObjectInputStream ois = new ObjectInputStream(new FileInputStream(file)); Singleton newInstance = (Singleton) ois.readObject(); //判断是否是同一个对象 System.out.println(newInstance == Singleton.getSingleton()); &#125;&#125;//true 本次输出结果为true。具体原理，我们回过头继续分析code 3中的第二段代码: 123456789101112if (obj != null &amp;&amp; handles.lookupException(passHandle) == null &amp;&amp; desc.hasReadResolveMethod()) &#123; Object rep = desc.invokeReadResolve(obj); if (unshared &amp;&amp; rep.getClass().isArray())&#123; rep =cloneArray(rep); &#125; if (rep != obj)&#123; handles.setObject(passHandle, obj =rep); &#125; &#125; hasReadResolveMethod:如果实现了serializable 或者 externalizable接口的类中包含readResolve则返回trueinvokeReadResolve:通过反射的方式调用要被反序列化的类的readResolve方法。所以，原理也就清楚了，只要在Singleton中定义readResolve方法，并在该方法中指定要返回的对象的生成策略，就可以防止单例被破坏。"},{"title":"数组","path":"/2022/08/12/数组/","content":"有效的字母异位词123456789101112131415161718192021222324252627282930313233343536373839//给定两个字符串 s 和 t ，编写一个函数来判断 t 是否是 s 的字母异位词。//示例 1: 输入: s = &quot;anagram&quot;, t = &quot;nagaram&quot; 输出: true//示例 2: 输入: s = &quot;rat&quot;, t = &quot;car&quot; 输出: false//说明: 你可以假设字符串只包含小写字母。//定义一个数组叫做record用来上记录字符串s里字符出现的次数。//需要把字符映射到数组也就是哈希表的索引下标上，因为字符a到字符z的ASCII是26个连续的数值，所以字符a映射为下标0，相应的字符z映射为下标25。//再遍历 字符串s的时候，只需要将 s[i] - ‘a’ 所在的元素做+1 操作即可，并不需要记住字符a的ASCII，只要求出一个相对数值就可以了。 这样就将字符串s中字符出现的次数，统计出来了。//那看一下如何检查字符串t中是否出现了这些字符，同样在遍历字符串t的时候，对t中出现的字符映射哈希表索引上的数值再做-1的操作。//那么最后检查一下，record数组如果有的元素不为零0，说明字符串s和t一定是谁多了字符或者谁少了字符，return false。//最后如果record数组所有元素都为零0，说明字符串s和t是字母异位词，return true。//时间复杂度为O(n)，空间上因为定义是的一个常量大小的辅助数组，所以空间复杂度为O(1)。public class Anagram &#123; public boolean isAnagram(String s, String t) &#123; int[] record = new int[26]; for (int i = 0; i &lt; s.length(); i++) &#123; record[s.charAt(i) - &#x27;a&#x27;]++; // 并不需要记住字符a的ASCII，只要求出一个相对数值就可以了 &#125; for (int i = 0; i &lt; t.length(); i++) &#123; record[t.charAt(i) - &#x27;a&#x27;]--; &#125; for (int item : record) &#123; if (item &gt; 0) &#123; // record数组如果有的元素不为零0，说明字符串s和t 一定是谁多了字符或者谁少了字符。 return false; &#125; &#125; return true;// record数组所有元素都为零0，说明字符串s和t是字母异位词 &#125; public static void main(String[] args) &#123; String s = &quot;void&quot;; String t = &quot;idov&quot;; Anagram anagram = new Anagram(); System.out.println(anagram.isAnagram(s, t)); &#125;&#125; 赎金信12345678910111213141516171819202122232425262728293031//给定一个赎金信 (ransom) 字符串和一个杂志(magazine)字符串，判断第一个字符串 ransom 能不能由第二个字符串 magazines 里面的字符构成。如果可以构成，返回 true ；否则返回 false。//(题目说明：为了不暴露赎金信字迹，要从杂志上搜索各个需要的字母，组成单词来表达意思。杂志字符串中的每个字符只能在赎金信字符串中使用一次。)//注意：//你可以假设两个字符串均只含有小写字母。//canConstruct(&quot;a&quot;, &quot;b&quot;) -&gt; false//canConstruct(&quot;aa&quot;, &quot;ab&quot;) -&gt; false//canConstruct(&quot;aa&quot;, &quot;aab&quot;) -&gt; truepublic class Construct &#123; public boolean canConstruct(String ransom, String magazine) &#123; if (ransom.length() &gt; magazine.length()) &#123; return false; &#125; int[] res = new int[26]; for (int i = 0; i &lt; magazine.length(); i++) &#123; res[magazine.charAt(i) - &#x27;a&#x27;]++; &#125; for (int i = 0; i &lt; ransom.length(); i++) &#123; res[ransom.charAt(i) - &#x27;a&#x27;]--; &#125; for (int i : res) &#123; if (res[i] &lt; 0) &#123; return false; &#125; &#125; return true; &#125;&#125; 快乐数1234567891011121314151617181920212223242526272829303132//编写一个算法来判断一个数 n 是不是快乐数。//「快乐数」定义为：对于一个正整数，每一次将该数替换为它每个位置上的数字的平方和，然后重复这个过程直到这个数变为 1，也可能是 无限循环 但始终变不到 1。如果 可以变为 1，那么这个数就是快乐数。//如果 n 是快乐数就返回 True ；不是，则返回 False 。//示例：//输入：19//输出：true//解释：//1^2 + 9^2 = 82//8^2 + 2^2 = 68//6^2 + 8^2 = 100//1^2 + 0^2 + 0^2 = 1public class HappyNum &#123; public boolean isHappy(int num) &#123; HashSet&lt;Integer&gt; set = new HashSet&lt;&gt;(); while (num != 1 &amp;&amp; !set.contains(num)) &#123; set.add(num); num = getNextNumber(num); &#125; return num == 1; &#125; private int getNextNumber(int n) &#123; int res = 0; while (n &gt; 0) &#123; int temp = n % 10; res += temp * temp; n = n / 10; &#125; return res; &#125;&#125; 两个数组的交集123456789101112131415161718192021public class Intersection &#123; public int[] intersection(int[] num1, int[] num2) &#123; if (num1 == null || num1.length == 0 || num2 == null || num2.length == 0) &#123; return new int[0]; &#125; HashSet&lt;Integer&gt; set1 = new HashSet&lt;&gt;(); HashSet&lt;Integer&gt; resSet = new HashSet&lt;&gt;(); for(int i : num1) &#123; set1.add(i); &#125; for (int i : num2) &#123; if (set1.contains(i)) &#123; resSet.add(i); &#125; &#125; return resSet.stream().mapToInt(Integer::intValue).toArray(); &#125;&#125; 移除元素1234567891011121314151617181920212223242526//给你一个数组 nums 和一个值 val，你需要 原地 移除所有数值等于 val 的元素，并返回移除后数组的新长度。//不要使用额外的数组空间，你必须仅使用 O(1) 额外空间并原地修改输入数组。//元素的顺序可以改变。你不需要考虑数组中超出新长度后面的元素。//示例 1: 给定 nums = [3,2,2,3], val = 3, 函数应该返回新的长度 2, 并且 nums 中的前两个元素均为 2。 你不需要考虑数组中超出新长度后面的元素。//示例 2: 给定 nums = [0,1,2,2,3,0,4,2], val = 2, 函数应该返回新的长度 5, 并且 nums 中的前五个元素为 0, 1, 3, 0, 4。public class RemoveElement &#123; public static void main(String[] args) &#123; int[] nums = &#123;0,1,2,2,3,0,4,2&#125;; System.out.println(removeElement(nums, 2)); int[] nums1 = &#123;3,2,2,3&#125;; System.out.println(removeElement(nums1, 3)); &#125; public static int removeElement(int[] nums, int val) &#123; //快慢指针 int slowIndex = 0; for (int fastIndex = 0; fastIndex &lt; nums.length; fastIndex++) &#123; if (nums[fastIndex] != val) &#123; nums[slowIndex] = nums[fastIndex]; slowIndex++; &#125; &#125; return slowIndex; &#125;&#125; 两数之和 1234567891011121314151617181920212223242526//给定一个整数数组 nums 和一个目标值 target，请你在该数组中找出和为目标值的那 两个 整数，并返回他们的数组下标。//你可以假设每种输入只会对应一个答案。但是，数组中同一个元素不能使用两遍。//示例://给定 nums = [2, 7, 11, 15], target = 9//因为 nums[0] + nums[1] = 2 + 7 = 9//所以返回 [0, 1]public class TwoSum &#123; public int[] twoSum(int[] nums, int target) &#123; int[] res = new int[2]; if (nums == null || nums.length == 0) &#123; return new int[0]; &#125; Map&lt;Integer, Integer&gt; tempMap = new HashMap&lt;&gt;(); for (int i = 0; i &lt; nums.length; i++) &#123; int temp = target - nums[i]; if (tempMap.containsKey(temp)) &#123; res[0] = i; res[1] = tempMap.get(temp); break; &#125; tempMap.put(nums[i], i); &#125; return res; &#125;&#125; 四数相加II-中等1234567891011121314151617181920212223242526272829303132333435//给定四个包含整数的数组列表 A , B , C , D ,计算有多少个元组 (i, j, k, l) ，使得 A[i] + B[j] + C[k] + D[l] = 0。//为了使问题简单化，所有的 A, B, C, D 具有相同的长度 N，且 0 ≤ N ≤ 500 。所有整数的范围在 -2^28 到 2^28 - 1 之间，最终结果不会超过 2^31 - 1 。//例如://输入://A = [ 1, 2]//B = [-2,-1]//C = [-1, 2]//D = [ 0, 2]//输出://2////解释://两个元组如下://(0, 0, 0, 1) -&gt; A[0] + B[0] + C[0] + D[1] = 1 + (-2) + (-1) + 2 = 0//(1, 1, 0, 0) -&gt; A[1] + B[1] + C[0] + D[0] = 2 + (-1) + (-1) + 0 = 0public class FourSumCount &#123; public int fourSumCount(int[] numA, int[] numB, int[] numC, int[] numD) &#123; int res = 0; Map&lt;Integer, Integer&gt; map = new HashMap&lt;&gt;(); for (int i : numA) &#123; for (int j : numB) &#123; int num = i + j; map.put(num, map.getOrDefault(num, 0) + 1); &#125; &#125; for (int i : numC) &#123; for (int j : numD) &#123; res += map.getOrDefault(0 -i - j, 0); &#125; &#125; return res; &#125;&#125;"},{"title":"链表","path":"/2022/08/12/链表/","content":"定义链表 12345678910111213public class ListNode &#123; int val; ListNode next; ListNode(int val)&#123; this.val = val; &#125; ListNode()&#123; &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940public class MyLinkedList &#123; int size; //虚拟头节点 ListNode head; public MyLinkedList(int size)&#123; this.size = size; head = new ListNode(0); &#125; public int get(int index) &#123; if (index &lt; 0 || index &gt; size) &#123; return -1; &#125; ListNode cur = head; //第0个节点是虚拟头节点，所以查找第 index+1 个节点 for (int i = 0; i &lt;= index; i++) &#123; cur = cur.next; &#125; return cur.val; &#125; public void addAtHead(int val) &#123; ListNode newNode = new ListNode(val); newNode.next = head.next; head.next = newNode; size++; &#125; public void addAtTail(int val) &#123; ListNode newNode = new ListNode(val); ListNode curr = head; while (curr.next != null) &#123; curr = curr.next; &#125; curr.next = newNode; size++; &#125;&#125; 移除链表元素1234567891011121314151617public class RemoveElement &#123; public ListNode removeElement(ListNode head, int target) &#123; ListNode dummy = new ListNode(); dummy = head; ListNode curr = dummy; while (curr.next != null) &#123; if (curr.next.val == target) &#123; curr.next = curr.next.next; &#125; else &#123; curr = curr.next; &#125; &#125; return dummy.next; &#125;&#125; 删除链表的倒数第N个节点给你一个链表，删除链表的倒数第 n 个结点，并且返回链表的头结点。1234567891011121314151617181920212223public class removeNthFromEnd &#123; public ListNode removeNthFromEnd(ListNode head, int n) &#123; ListNode dummy = new ListNode(0); dummy.next = head; ListNode fast = dummy; ListNode slow = dummy; for (int i = 0; i &lt;= n; i++) &#123; fast = fast.next; &#125; while (fast != null) &#123; fast = fast.next; slow = slow.next; &#125; if (slow.next != null) &#123; slow.next = slow.next.next; &#125; return dummy.next; &#125;&#125; 链表翻转12345678910111213141516public class ReverseList &#123; public ListNode reverseList(ListNode head) &#123; ListNode pre = null; ListNode cur = head; ListNode temp; while (cur != null) &#123; temp = cur.next; cur.next = pre; pre = cur; cur = temp; &#125; return pre; &#125;&#125; 两两交换链表中的节点给定一个链表，两两交换其中相邻的节点，并返回交换后的链表。你不能只是单纯的改变节点内部的值，而是需要实际的进行节点交换。12345678910111213141516171819202122232425public class SwapPairs &#123; public ListNode swapPairs(ListNode head) &#123; ListNode dummy = new ListNode(-1); dummy = head; ListNode cur = dummy; ListNode temp;// 临时节点，保存两个节点后面的节点 ListNode first;// 临时节点，保存两个节点之中的第一个节点 ListNode second;// 临时节点，保存两个节点之中的第二个节点 while (cur.next != null &amp;&amp; cur.next.next != null) &#123; temp = cur.next.next.next; first = cur.next; second = cur.next.next; cur.next = second; second.next = first; first.next = temp; cur = first; &#125; return dummy.next; &#125;&#125; 环形链表II题意： 给定一个链表，返回链表开始入环的第一个节点。 如果链表无环，则返回 null。主要考察两知识点：判断链表是否环如果有环，如何找到这个环的入口 判断链表是否有环可以使用快慢指针法，分别定义 fast 和 slow 指针，从头结点出发，fast指针每次移动两个节点，slow指针每次移动一个节点，如果 fast 和 slow指针在途中相遇 ，说明这个链表有环。为什么fast 走两个节点，slow走一个节点，有环的话，一定会在环内相遇呢，而不是永远的错开呢首先第一点：fast指针一定先进入环中，如果fast指针和slow指针相遇的话，一定是在环中相遇，这是毋庸置疑的。那么来看一下，为什么fast指针和slow指针一定会相遇呢？可以画一个环，然后让 fast指针在任意一个节点开始追赶slow指针。 从头结点出发一个指针，从相遇节点 也出发一个指针，这两个指针每次只走一个节点， 那么当这两个指针相遇的时候就是 环形入口的节点。 123456789101112131415161718192021222324public class DetectCycle &#123; public ListNode detectCycle(ListNode head) &#123; ListNode fast = head; ListNode slow = head; while (fast != null &amp;&amp; fast.next != null) &#123; slow = slow.next; fast = fast.next.next; if (fast == slow) &#123;// 有环 ListNode index1 = fast; ListNode index2 = head; // 两个指针，从头结点和相遇结点，各走一步，直到相遇，相遇点即为环入口 while (index1 != index2) &#123; index1 = index1.next; index2 = index2.next; &#125; return index1; &#125; &#125; return null; &#125;&#125; 给你两个单链表的头节点 headA 和 headB ，请你找出并返回两个单链表相交的起始节点。如果两个链表没有交点，返回 null 。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546//(版本一)先行移动长链表实现同步移动//我们求出两个链表的长度，并求出两个链表长度的差值，然后让curA移动到，和curB 末尾对齐的位置//此时我们就可以比较curA和curB是否相同，如果不相同，同时向后移动curA和curB，如果遇到curA == curB，则找到交点。//否则循环退出返回空指针。public ListNode getIntersectionNode(ListNode headA, ListNode headB) &#123; ListNode curA = headA, curB = headB; int lenA = 0, lenB = 0; while (curA != null) &#123; lenA++; curA = curA.next; &#125; while (curB != null) &#123; lenB++; curB = curB.next; &#125; curA = headA; curB = headB; if (lenB &gt; lenA) &#123; int lenTemp = lenA; lenA = lenB; lenB = lenTemp; ListNode tempNode = curA; curA = curB; curB = tempNode; &#125; int gap = lenA - lenB; while (gap-- &gt; 0) &#123; curA = curA.next; &#125; while (curA != null) &#123; if (curA == curB) &#123; return curA; &#125; curA = curA.next; curB = curB.next; &#125; return null;&#125; 1234567891011121314151617181920212223//(版本二) 合并链表实现同步移动//创建两个指针p1和p2，分别指向两个A、B链表的头结点，p1和p2指针同时移动。当p1和p2走到链表末尾时，分别指向另外一个链表的头结点，即：p1.next=headB,p2.next=headA。如此操作可实现将A、B两个链表进行合并和同时移动。最后当p1和p2相等时，即可证明两链表相交，返回p1结点。public ListNode getIntersectionNode1(ListNode headA, ListNode headB) &#123; // p1 指向 A 链表头结点，p2 指向 B 链表头结点 ListNode p1 = headA; ListNode p2 = headB; while (p1 != p2) &#123; // p1 走一步，如果走到 A 链表末尾，转到 B 链表 if (p1 == null) &#123; p1 = headB; &#125; else &#123; p1 = p1.next; &#125; // p2 走一步，如果走到 B 链表末尾，转到 A 链表 if (p2 == null) &#123; p2 = headA; &#125; else &#123; p2 = p2.next; &#125; &#125; return p1;&#125; 第k个最小元素 123456789101112131415161718192021222324public class KthSmallest &#123; private int count; private Integer result = Integer.MIN_VALUE; public Integer kthSmallest(TreeNode node, int k) &#123; kthSmallestHelper(node, k); return result; &#125; public void kthSmallestHelper(TreeNode node, int k) &#123; if (node == null) &#123; return; &#125; kthSmallestHelper(node.left, k); count++; if (count == k) &#123; result = node.val; return; &#125; kthSmallestHelper(node.right, k); &#125;&#125; 二叉树的层序遍历 123456789101112131415161718192021222324252627282930313233343536//层序遍历一个二叉树。就是从左到右一层一层的去遍历二叉树。这种遍历的方式和我们之前讲过的都不太一样。//需要借用一个辅助数据结构即队列来实现，队列先进先出，符合一层一层遍历的逻辑，而用栈先进后出适合模拟深度优先遍历也就是递归的逻辑。//而这种层序遍历方式就是图论中的广度优先遍历，只不过我们应用在二叉树上。public class LevelOrder &#123; public List&lt;List&lt;Integer&gt;&gt; levelOrder(TreeNode root) &#123; List&lt;List&lt;Integer&gt;&gt; result = new ArrayList&lt;&gt;(); if (root == null) &#123; return result; &#125; Queue&lt;TreeNode&gt; queue = new LinkedList&lt;&gt;(); queue.offer(root); while (!queue.isEmpty()) &#123; List&lt;Integer&gt; item = new ArrayList&lt;&gt;(); int len = queue.size(); while (len &gt; 0) &#123; TreeNode node = queue.poll(); item.add(node.val); if (node.left != null) &#123; queue.offer(node.left); &#125; if (node.right != null) &#123; queue.offer(node.right); &#125; len--; &#125; result.add(item); &#125; return result; &#125;&#125; 最大深度 12345678910111213141516171819202122232425262728public class MaxDepth &#123; public int maxDepth(TreeNode root) &#123; if (root == null) &#123; return 0; &#125; Deque&lt;TreeNode&gt; deque = new LinkedList&lt;&gt;(); deque.offer(root); int depth = 0; while (!deque.isEmpty()) &#123; int size = deque.size(); depth++; for (int i = 0; i &lt; size; i++) &#123; TreeNode node = deque.pop(); if (node.left != null) &#123; deque.offer(node.left); &#125; if (node.right != null) &#123; deque.offer(node.right); &#125; &#125; &#125; return depth; &#125;&#125; 最小深度 12345678910111213141516171819202122232425262728293031public class MinDepth &#123; public int minDepth(TreeNode root) &#123; if (root == null) &#123; return 0; &#125; Deque&lt;TreeNode&gt; deque = new LinkedList&lt;&gt;(); deque.offer(root); int depth = 0; while (!deque.isEmpty()) &#123; int size = deque.size(); depth++; for (int i = 0; i &lt; size; i++) &#123; TreeNode node = deque.pop(); // 是叶子结点，直接返回depth，因为从上往下遍历，所以该值就是最小值 if (node.left == null &amp;&amp; node.right == null) &#123; return depth; &#125; if (node.left != null) &#123; deque.offer(node.left); &#125; if (node.right != null) &#123; deque.offer(node.right); &#125; &#125; &#125; return depth; &#125;&#125;"},{"title":"Java锁机制","path":"/2022/07/26/Java锁机制/","content":"1.什么是锁在并发情况下，多个线程会对同一个资源进行争抢，那么可能会导致数据不一致的问题，为了解决这个问题很多编程语言都引入了锁机制，通过一种抽象的锁来对资源进行锁定 2. 锁机制是怎么设计的在谈锁之前先了解一些Java虚拟机内存结构的知识，JVM运行时内存结构主要包含了程序计数器、JVM栈、Native栈、堆、方法区，对于程序计数器、JVM栈、Native栈是线程私有的，对于这个区域的数据，不会出现线程竞争的问题，而堆、方法区中的数据被所有线程共享，其中Java堆中存放的是所有对象，方法区中存放着类信息、常量、静态变量等数据。所以当多个线程在竞争其中一些数据时，有可能会发生难以预料的异常情况，因此需要锁机制进行限制， 锁是一种抽象的概念，那么它在代码层面是究竟如何实现的呢？简单来说，在Java中，每个Object，也就是每个对象都拥有一把锁，这把锁存放在对象头中，锁中记录了当前对象被哪个线程所占用。 刚才提到了锁是存放在对象头中的，那么对象、对象头的结构分别是什么呢？先来看下对象的结构，Java对象包含了三个部分：对象头、实例数据、对齐填充字节，其中对齐填充字节是为了满足Java对象的大小必须是8比特的倍数这一条件设置的，对齐填充字节正如它的名字一样，是为了帮只对象来对齐而填充的一些无用字节，大可不必理会，实例数据就是你在初始化对象时，设定的属性和状态的内容，对象头则是要说的重点之一，它存放了一些对象本身的运行时信息，对象头包含了两部分，Mark Word和Class Point，相较于实例数据，对象头属于一些额外的存储开销，所以它被设计的极小来提高效率"},{"title":"Hibernate Validator 参数校验","path":"/2022/05/04/Hibernate-Validator/","content":"背景在任何时候，当你要处理一个应用程序的业务逻辑，数据校验是你必须要考虑和面对的事情。应用程序必须通过某种手段来确保输入进来的数据从语义上来讲是正确的。在通常的情况下，应用程序是分层的，不同的层由不同的开发人员来完成。很多时候同样的数据验证逻辑会出现在不同的层，这样就会导致代码冗余和一些管理的问题，比如说语义的一致性等。为了避免这样的情况发生，最好是将验证逻辑与相应的域模型进行绑定。 以上摘自https://www.ibm.com/developerworks/cn/java/j-lo-jsr303/index.html，还有一些更具体的介绍 Hibernate Validator简介Hibernate Validator实现了JSR-303规范,Hibernate Validator是Bean Validation的参考实现，Hibernate Validator提供了JSR-303规范所有内置的约束，并额外附加了一些约束。 新版本会不断有新的附加约束，不限于以下，以最新的版本为主 Bean Validation 中的 constraintHibernate Validator 附加的 constraint Constraint 详细信息 @CreditCardNumber 被注释的元素必须是合法的信用卡号码 @Email 被注释的元素必须是电子邮箱地址 @Length 被注释的字符串的大小必须在指定的范围内 @NotBlank 被注释的字符串的必须非空 @NotEmpty 被注释的字符串的必须非空 @Range 被注释的元素必须在合适的范围内 @SafeHtml 被注释的元素可能有不安全的HTML内容 @ScriptAssert 被注释的元素执行脚本表达式”{script}”没有返回期望结果 @URL 被注释的元素必须是一个合法的URL Hibernate Validator使用以Spring MVC为例 maven引入jar包，按照自己的实际需要可以引入其他版本12345&lt;dependency&gt; &lt;groupId&gt;org.hibernate&lt;/groupId&gt; &lt;artifactId&gt;hibernate-validator&lt;/artifactId&gt; &lt;version&gt;5.2.4.Final&lt;/version&gt;&lt;/dependency&gt; spring-mvc.xml主要的配置123456789101112131415&lt;mvc:annotation-driven validator=&quot;validator&quot;/&gt;&lt;bean id=&quot;validator&quot; class=&quot;org.springframework.validation.beanvalidation.LocalValidatorFactoryBean&quot;&gt; &lt;property name=&quot;providerClass&quot; value=&quot;org.hibernate.validator.HibernateValidator&quot;/&gt; &lt;property name=&quot;validationMessageSource&quot; ref=&quot;validatorMessageSource&quot;/&gt;&lt;/bean&gt;&lt;bean id=&quot;validatorMessageSource&quot; class=&quot;org.springframework.context.support.ResourceBundleMessageSource&quot;&gt; &lt;property name=&quot;basenames&quot;&gt; &lt;list&gt; &lt;value&gt;org.hibernate.validator.ValidationMessages&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;property name=&quot;defaultEncoding&quot; value=&quot;UTF-8&quot;/&gt;&lt;/bean&gt; 请求参数@RequestParam校验在@RequestParam前加相应的约束就可以，比如@NotBlank、@Length，message可以设置提示语 1234567@RequestMapping(value = &quot;/sendBusinessJmq&quot;, method = &#123;RequestMethod.GET, RequestMethod.POST&#125;, produces = &quot;application/json;charset=utf-8&quot;)@ResponseBodypublic Object sendBusinessJmq(@NotBlank(message = &quot;单号不能为空&quot;) @RequestParam(&quot;settlementNo&quot;) final String settlementNo, @NotBlank(message = &quot;状态不能为空&quot;) @Length(max = 2, message = &quot;长度不能超过2&quot;) @RequestParam(&quot;status&quot;) final int status) &#123; //TODO return &quot;success&quot;;&#125; 请求参数@RequestBody校验在@RequestBody加上@Validated就可以对后边的model参数进行校验了，model会对具体的字段做校验，下面会写 网上说需要在Controller上加@Validated，自己试过不加也可以 12345@PostMapping(&quot;/systemConfig/add&quot;)public Object settleObjectTypeAdd(HttpServletRequest request, @RequestBody @Validated SystemConfigVo vo) &#123; //TODO return &quot;success&quot;;&#125; model校验在每个字段上加相应的约束就可以了 注意：Long类型上要加@NotNull，不要加@NotBlank，如果碰到下面这个异常就是这个问题：javax.validation.UnexpectedTypeException: HV000030: No validator could be found for constraint ‘org.hibernate.validator.constraints.NotBlank’ validating type ‘java.lang.Long’. Check configuration for ‘id’ 12345678910111213141516@Datapublic class SystemConfigVo &#123; @NotNull(message = &quot;id不能为null&quot;) private Long id; @NotBlank @Length(min = 1, max = 20) private String key; @NotBlank @Length(min = 1, max = 50, message = &quot;长度需要在1-50之间&quot;) private String desc;&#125; 对象嵌套校验有的时候，入参的对象可能包含List，或者对象包含其他model，同时也要对嵌套的对象进行校验Controller代码： 12345@PostMapping(&quot;/settlementObject/operation:add&quot;)public Object operationAdd(HttpServletRequest request, @RequestBody @Validated SettlementObjectListVo listVo) &#123; //TODO return &quot;success&quot;;&#125; SettlementObjectListVo代码：嵌套的对象上要加@Valid，就可以对SettlementObjectVo的字段进行校验了 12345678@Datapublic class SettlementObjectListVo &#123; private String regionId; @Valid private List&lt;SettlementObjectVo&gt; settlementObjectList;&#125; SettlementObjectVo代码： 123456789101112131415161718192021222324@Datapublic class SettlementObjectVo implements Serializable &#123; private static final long serialVersionUID = 6063724154195494666L; @Email @Length(max = 50) private String email; @Length(min = 1, max = 20) private String businessErp; @NotNull @Range(min = 1, max = 5) private Byte settlementCycle; @NotBlank @Length(min = 1, max = 40) private String bankCompanyName; @NotBlank @Length(min = 1, max = 30) @Pattern(regexp = &quot;[0-9]*$&quot;, message = &quot;必须是数字&quot;) private String bankCardNo;&#125; 分组校验有这样一种场景，新增系统配置信息的时候，不需要验证id（因为系统生成）；修改的时候需要验证id，这时候就用到validator的分组验证功能定义分组： 12345public interface Add &#123;&#125;public interface Modify &#123;&#125; model代码：增加groups属性，id在修改的时候才去校验，key是增加和修改的时候都去校验，desc增加的时候校验不能为空，修改的时候校验长度 12345678910111213141516@Datapublic class SystemConfigVo &#123; @NotNull(groups = &#123;Modify.class&#125;) private Long id; @NotBlank(groups = &#123;Modify.class, Add.class&#125;) @Length(max = 20, groups = &#123;Modify.class, Add.class&#125;) private String key; @NotBlank(groups = &#123;Add.class&#125;) @Length(max = 50, groups = &#123;Modify.class, Add.class&#125;) private String desc;&#125; Controller代码：@Validated增加Add.class去校验SystemConfigVo的时候只校验设置Add.class分组的字段，比如上面的key和desc 12345@PostMapping(&quot;/systemConfig/add&quot;)public Object costTypeAdd(HttpServletRequest request, @RequestBody @Validated(&#123;Add.class&#125;) SystemConfigVo vo) &#123; //TODO return &quot;success&quot;;&#125; 请求参数List类型校验适合入参有List集合的参数，需要定义一个ValidList，里面的list上要加@Valid；Controller方法入参类型由原来的List改为ValidListValidList代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128public class ValidList&lt;E&gt; implements List&lt;E&gt; &#123; @Valid private List&lt;E&gt; list = new LinkedList&lt;&gt;(); public List&lt;E&gt; getList() &#123; return list; &#125; public void setList(List&lt;E&gt; list) &#123; this.list = list; &#125; @Override public int size() &#123; return list.size(); &#125; @Override public boolean isEmpty() &#123; return list.isEmpty(); &#125; @Override public boolean contains(Object o) &#123; return list.contains(o); &#125; @Override public Iterator&lt;E&gt; iterator() &#123; return list.iterator(); &#125; @Override public Object[] toArray() &#123; return list.toArray(); &#125; @Override public &lt;T&gt; T[] toArray(T[] a) &#123; return list.toArray(a); &#125; @Override public boolean add(E e) &#123; return list.add(e); &#125; @Override public boolean remove(Object o) &#123; return list.remove(o); &#125; @Override public boolean containsAll(Collection&lt;?&gt; c) &#123; return list.containsAll(c); &#125; @Override public boolean addAll(Collection&lt;? extends E&gt; c) &#123; return list.addAll(c); &#125; @Override public boolean addAll(int index, Collection&lt;? extends E&gt; c) &#123; return list.addAll(index, c); &#125; @Override public boolean removeAll(Collection&lt;?&gt; c) &#123; return list.removeAll(c); &#125; @Override public boolean retainAll(Collection&lt;?&gt; c) &#123; return list.retainAll(c); &#125; @Override public void clear() &#123; list.clear(); &#125; @Override public E get(int index) &#123; return list.get(index); &#125; @Override public E set(int index, E element) &#123; return list.set(index, element); &#125; @Override public void add(int index, E element) &#123; list.add(index, element); &#125; @Override public E remove(int index) &#123; return list.remove(index); &#125; @Override public int indexOf(Object o) &#123; return list.indexOf(o); &#125; @Override public int lastIndexOf(Object o) &#123; return list.lastIndexOf(o); &#125; @Override public ListIterator&lt;E&gt; listIterator() &#123; return list.listIterator(); &#125; @Override public ListIterator&lt;E&gt; listIterator(int index) &#123; return list.listIterator(index); &#125; @Override public List&lt;E&gt; subList(int fromIndex, int toIndex) &#123; return list.subList(fromIndex, toIndex); &#125;&#125; Controller代码： 12345@PostMapping(&quot;/systemConfig/add&quot;)public Object costTypeAdd(HttpServletRequest request, @RequestBody @ValidList&lt;SystemConfigVo&gt; listVo) &#123; //TODO return &quot;success&quot;;&#125; 自定义校验自带的约束可能满足不了自己的业务校验规则，可以自定义一个，比如定义一个输入的值是否在指定范围的注解定义注解： 12345678910111213@Documented@Retention(RetentionPolicy.RUNTIME)@Target(&#123;ElementType.FIELD, ElementType.PARAMETER&#125;)@Constraint(validatedBy = FlagValidatorClass.class)public @interface FlagValidator &#123; String[] value() default &#123;&#125;; String message() default &quot;flag is not found&quot;; Class&lt;?&gt;[] groups() default &#123;&#125;; Class&lt;? extends Payload&gt;[] payload() default &#123;&#125;;&#125; 定义一个校验器，具体校验规则： 123456789101112131415161718192021public class FlagValidatorClass implements ConstraintValidator&lt;FlagValidator, Integer&gt; &#123; private String[] values; @Override public void initialize(FlagValidator flagValidator) &#123; this.values = flagValidator.value(); &#125; @Override public boolean isValid(Integer value, ConstraintValidatorContext constraintValidatorContext) &#123; if (value == null) &#123; return true; &#125; for (String v : values) &#123; if (v.equals(String.valueOf(value))) &#123; return true; &#125; &#125; return false; &#125;&#125; 字段上设置刚自定义的校验注解，如果输入的不在这5个其中的一个会提示状态有误 12@FlagValidator(value = &#123;&quot;-1&quot;, &quot;4&quot;, &quot;5&quot;, &quot;7&quot;, &quot;12&quot;&#125;, message = &quot;状态有误&quot;)private Integer status; 校验错误提示信息上面看到的约束都没配置提示信息，没配置用的是Hibernate Validator自带的配置，默认是英文提示，如下： 12@NotNull(groups = &#123;Modify.class&#125;)private Long id; 默认的配置在包里，如下： 也可以配置自己想要的提示语，增加message属性 12@NotNull(groups = &#123;Modify.class&#125;, message = &quot;id不能为null&quot;)private Long id; 提示语也可以统一到配置文件message.properties中，如下： 12@NotNull(groups = &#123;Modify.class&#125;, message = &quot;&#123;id.NotNull.message&#125;&quot;)private Long id; message.properties如下： 12345id.NotNull.message = id不能为nullkey.NotBlank.message = 编码不能为空key.Length.message = 编码长度不能超过&#123;max&#125;desc.NotBlank.message = 名称不能为空desc.Length.message = 名称长度不能超过&#123;max&#125; 异常统一处理在全局异常拦截中添加验证异常的处理： 1234567891011121314@ExceptionHandler(MethodArgumentNotValidException.class)@ResponseBodypublic OpenApiResponse methodArgumentNotValid(HttpServletRequest request, MethodArgumentNotValidException e) &#123; logger.error(&quot;enter MethodArgumentNotValidException&quot;); BindingResult bindingResult = e.getBindingResult(); Map&lt;String, String&gt; messageMap = new HashMap&lt;&gt;(); for (FieldError fieldError : bindingResult.getFieldErrors()) &#123; messageMap.put(String.valueOf(fieldError.getRejectedValue()), fieldError.getDefaultMessage()); &#125; String message = JSON.toJSONString(messageMap); logger.error(message); return response(request, message); &#125; 返回的提示语类似这样： 提示消息国际化国际化的配置在上面spring-mvc.xml已经配置过了 12345678&lt;bean id=&quot;validatorMessageSource&quot; class=&quot;org.springframework.context.support.ResourceBundleMessageSource&quot;&gt; &lt;property name=&quot;basenames&quot;&gt; &lt;list&gt; &lt;value&gt;org.hibernate.validator.ValidationMessages&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;property name=&quot;defaultEncoding&quot; value=&quot;UTF-8&quot;/&gt;&lt;/bean&gt; org.hibernate.validator.ValidationMessages我配置的是Hibernate Validator自带的国际化配置文件，配置后默认就是中文啦； 也可以自定义配置文件，在工程的resource下定义i18n&#x2F;message_zh_CN.propertiesorg.hibernate.validator.ValidationMessages改为i18n.message即可 解释 名词 解释 JCP Java Community Process，Java社区 JSR Java Specification Requests，Java规范提案。是指向JCP提出新增一个标准化技术规范的正式请求，认识人都可以提交JSR，以向Java平台增添新的API和服务 JSR-303 JAVA EE 6 中的一项子规范，叫做Bean Validation"},{"title":"Kafka","path":"/2022/03/02/Kafka/","content":"第1章Kafka概述1.1 定义Kafka是一个分布式的基于发布/订阅模式的消息队列（Message Queue），主要应用于大数据实时处理领域。 1.2 消息队列1.2.1 传统消息队列的应用场景 使用消息队列的好处 解藕，允许你独立的扩展或修改两边的处理过程，只要确保他们遵守同样的接口约束。 可恢复性，系统的一部分组件失效时，不会影响到整个系统。消息队列降低了进程间的耦合度，所以即使一个处理消息的进程挂掉，加入队列中的消息仍然可以在系统恢复后被处理。 缓冲，有助于控制和优化数据流经过系统的速度，解决生产消息和消费消息的处理速度不一致的情况。 灵活性 &amp; 峰值处理能力，在访问量剧增的情况下，应用仍然需要继续发挥作用，但是这样的突发流量并不常见。如果为以能处理这类峰值访问为标准来投入资源随时待命无疑是巨大的浪费。使用消息队列能够使关键组件顶住突发的访问压力，而不会因为突发的超负荷的请求而完全崩溃。异步通信，很多时候，用户不想也不需要立即处理消息。消息队列提供了异步处理机制，允许用户把一个消息放入队列，但并不立即处理它。想向队列中放入多少消息就放多少，然后在需要的时候再去处理它们。 1.2.1 消息队列的两种模式 点对点模式（一对一，消费者主动拉取数据，消息收到后消息清除） 消息生产者生产消息发送到Queue 中，然后消息消费者从Queue 中取出并且消费消息。消息被消费以后，queue 中不再有存储，所以消息消费者不可能消费到已经被消费的消息。Queue 支持存在多个消费者， 但是对一个消息而言， 只会有一个消费者可以消费。 发布&#x2F;订阅模式（一对多，消费者消费数据之后消息不会清楚） 消息生产者（发布）将消息发布到 topic 中，同时有多个消息消费者（订阅）消费该消息。和点对点方式不同，发布到 topic 的消息会被所有订阅者消费。 1.3 Kafka基础架构 Producer ：消息生产者，就是向 kafka broker 发消息的客户端； Consumer ：消息消费者，向 kafka broker 取消息的客户端； Consumer Group （CG）：消费者组，由多个 consumer 组成。消费者组内每个消费者负责消费不同分区的数据，一个分区只能由一个组内消费者消费；消费者组之间互不影响。所有的消费者都属于某个消费者组，即消费者组是逻辑上的一个订阅者。 Broker ：一台 kafka 服务器就是一个 broker。一个集群由多个 broker 组成。一个 broker可以容纳多个topic。 Topic ：可以理解为一个队列，生产者和消费者面向的都是一个topic； Partition：为了实现扩展性，一个非常大的 topic 可以分布到多个broker（即服务器）上，一个topic可以分为多个partition，每个 partition 是一个有序的队列； Replica：副本，为保证集群中的某个节点发生故障时，该节点上的 partition 数据不丢失，且kafka 仍然能够继续工作，kafka 提供了副本机制，一个 topic 的每个分区都有若干个副本，一个 leader 和若干个 follower。 leader：每个分区多个副本的“主”，生产者发送数据的对象，以及消费者消费数据的对象都是leader。 follower：每个分区多个副本中的“从”，实时从 leader 中同步数据，保持和 leader 数据的同步。leader 发生故障时，某个 follower 会成为新的 leader。 第2章Kafka快速入门第3章Kafka架构深入3.1 Kafka工作流程及文件存储机制 Kafka 中消息是以 topic 进行分类的，生产者生产消息，消费者消费消息，都是面向 topic的。 topic 是逻辑上的概念，而 partition 是物理上的概念，每个 partition 对应于一个 log 文件，该 log 文件中存储的就是 producer 生产的数据。Producer 生产的数据会被不断追加到该log 文件末端，且每条数据都有自己的 offset。消费者组中的每个消费者，都会实时记录自己消费到了哪个offset，以便出错恢复时，从上次的位置继续消费。 由于生产者生产的消息会不断追加到 log 文件末尾，为防止 log 文件过大导致数据定位效率低下，Kafka 采取了分片和索引机制，将每个 partition 分为多个 segment。每个 segment 对应两个文件——“.index”文件和“.log”文件。这些文件位于一个文件夹下，该文件夹的命名规则为：topic 名称+分区序号。例如，first 这个 topic 有三个分区，则其对应的文件夹为 first- 0,first-1,first-2。 12345600000000000000000000.index00000000000000000000.log00000000000000170410.index00000000000000170410.log00000000000000239430.index00000000000000239430.log index 和 log 文件以当前 segment 的第一条消息的 offset 命名。下图为 index 文件和 log文件的结构示意图。 “.index”文件存储大量的索引信息，“.log”文件存储大量的数据，索引文件中的元 数据指向对应数据文件中message 的物理偏移地址。 3.2 Kafka生产者3.2.1 分区策略 分区的原因 方便在集群中扩展，每个 Partition 可以通过调整以适应它所在的机器，而一个 topic又可以有多个 Partition 组成，因此整个集群就可以适应任意大小的数据了； 可以提高并发，因为可以以Partition 为单位读写了。 分区的原则 我们需要将 producer 发送的数据封装成一个 ProducerRecord 对象。 指明 partition 的情况下，直接将指明的值直接作为 partiton 值； 没有指明 partition 值但有 key 的情况下，将 key 的 hash 值与 topic 的 partition数进行取余得到 partition 值； 既没有 partition 值又没有 key 值的情况下，第一次调用时随机生成一个整数（ 后面每次调用在这个整数上自增），将这个值与 topic 可用的 partition 总数取余得到partition值，也就是常说的 round-robin 算法。 3.2.2 数据可靠性保证为保证 producer 发送的数据，能可靠的发送到指定的 topic，topic 的每个 partition 收到producer 发送的数据后，都需要向 producer 发送 ack（acknowledgement 确认收到），如果producer 收到 ack，就会进行下一轮的发送，否则重新发送数据。 副本数据同步策略 方案 优点 缺点 半数以上完成同步，就发送 ack 延迟低 选举新的 leader 时，容忍 n 台节点的故障，需要 2n+1 个副 本 全部完成同步，才发送 ack 选举新的 leader 时，容忍 n 台节点的故障，需要 n+1 个副 本 延迟高 Kafka 选择了第二种方案，原因如下： 同样为了容忍 n 台节点的故障，第一种方案需要 2n+1 个副本，而第二种方案只需要 n+1个副本，而Kafka 的每个分区都有大量的数据，第一种方案会造成大量数据的冗余。 虽然第二种方案的网络延迟会比较高，但网络延迟对 Kafka 的影响较小。 ISR 采用第二种方案之后，设想以下情景：leader 收到数据，所有 follower 都开始同步数据， 但有一个 follower，因为某种故障，迟迟不能与 leader 进行同步，那 leader 就要一直等下去， 直到它完成同步，才能发送 ack。这个问题怎么解决呢？ Leader 维护了一个动态的 in-sync replica set (ISR)，意为和 leader 保持同步的 follower 集合。当 ISR 中的 follower 完成数据的同步之后，leader 就会给 follower 发送 ack。如果 follower长 时 间 未 向 leader 同 步 数 据 ， 则 该 follower 将 被 踢 出 ISR ， 该 时 间 阈 值 由replica.lag.time.max.ms 参数设定。Leader 发生故障之后，就会从 ISR 中选举新的 leader。 ack应答机制 对于某些不太重要的数据，对数据的可靠性要求不是很高，能够容忍数据的少量丢失， 所以没必要等ISR 中的 follower 全部接收成功。 所以 Kafka 为用户提供了三种可靠性级别，用户根据对可靠性和延迟的要求进行权衡， 选择以下的配置。 acks 参数配置： acks: 0：producer 不等待 broker 的 ack，这一操作提供了一个最低的延迟，broker 一接收到还没有写入磁盘就已经返回，当 broker 故障时有可能丢失数据； 1：producer 等待broker 的 ack，partition 的 leader 落盘成功后返回ack，如果在 follower同步成功之前leader 故障，那么将会丢失数据； -1（all）：producer 等待 broker 的 ack，partition 的 leader 和 follower 全部落盘成功后才返回 ack。但是如果在 follower 同步完成后，broker 发送 ack 之前，leader 发生故障，那么会造成数据重复。 故障处理节 LEO：指的是每个副本最大的 offset； HW：指的是消费者能见到的最大的 offset，ISR 队列中最小的 LEO。 （1） follower 故障 follower 发生故障后会被临时踢出 ISR，待该 follower 恢复后，follower 会读取本地磁盘记录的上次的HW，并将 log 文件高于 HW 的部分截取掉，从 HW 开始向 leader 进行同步。等该 follower 的 LEO 大于等于该 Partition 的 HW，即 follower 追上 leader 之后，就可以重新加入 ISR 了。 （2） leader 故障 leader 发生故障之后，会从 ISR 中选出一个新的 leader，之后，为保证多个副本之间的数据一致性，其余的 follower 会先将各自的 log 文件高于 HW 的部分截掉，然后从新的 leader同步数据。 注意：这只能保证副本之间的数据一致性，并不能保证数据不丢失或者不重复。 3.2.3 Exactly Once 语义​ 将服务器的 ACK 级别设置为-1，可以保证 Producer 到 Server 之间不会丢失数据，即 At Least Once 语义。相对的，将服务器 ACK 级别设置为 0，可以保证生产者每条消息只会被发送一次，即At Most Once 语义。 ​ At Least Once 可以保证数据不丢失，但是不能保证数据不重复；相对的，At Least Once 可以保证数据不重复，但是不能保证数据不丢失。但是，对于一些非常重要的信息，比如说交易数据，下游数据消费者要求数据既不重复也不丢失，即 Exactly Once 语义。在 0.11 版本以前的 Kafka，对此是无能为力的，只能保证数据不丢失，再在下游消费者对数据做全局去重。对于多个下游应用的情况，每个都需要单独做全局去重，这就对性能造成了很大影响。 ​ 0.11 版本的 Kafka，引入了一项重大特性：幂等性。所谓的幂等性就是指Producer 不论向 Server 发送多少次重复数据，Server 端都只会持久化一条。幂等性结合 At Least Once 语义，就构成了Kafka 的Exactly Once 语义。即： ​ At Least Once + 幂等性 = Exactly Once ​ 要启用幂等性，只需要将 Producer 的参数中 enable.idompotence 设置为 true 即可。Kafka 的幂等性实现其实就是将原来下游需要做的去重放在了数据上游。开启幂等性的 Producer 在初始化的时候会被分配一个 PID，发往同一 Partition 的消息会附带 Sequence Number。而Broker 端会对&lt;PID, Partition, SeqNumber&gt;做缓存，当具有相同主键的消息提交时，Broker 只会持久化一条。 ​ 但是PID 重启就会变化，同时不同的 Partition 也具有不同主键，所以幂等性无法保证跨分区跨会话的Exactly Once。"},{"title":"TiDB-SQL的一生","path":"/2022/02/17/TiDB-SQL的一生/","content":"SQL的一生 从客户端的Socket读取一条SQL 获取一个token 从PD获取TSO（事务的时间戳） 使用Parser将SQL parse为AST 将AST compile为执行计划 Logical Optimizer Physical Optimizer Execute Plan 根据对应的执行计划，最底层的Executor会根据这条SQL处理的Key范围构建出多个要下发到TiKV的请求，并通过distsql的API将这些请求分发到TiKV TiKV对结果做一些处理（包括filter,limit等）后，将中间结果集反馈给TiDB TiDB进行一些表关联、聚合运算最终返回给Client TiKV收到请求后，会将请求分为两类： storage read pool执行引擎：负责主键和唯一索引点查 coprocessor执行引擎：其余的请求 如何定位slow queryslow query产生的原因 按组件划分 TiDB parse慢 complie慢，生成物理执行计划 get token慢，分配线程慢 执行计划不正确 PD 获取tso慢，每个SQL要获取时间戳的 TiKV 需要扫描大量的key，耗时久 coprocessor cpu打满，资源等待 读热点 slow query获取渠道 集群监控上的metrics信息 slow-query log，对标MySQL格式，支持市场上的MySQL慢查询分析工具 TiDB的慢查询SQL内存表 TiKV节点日志slow-query反查 TiDB - Parse Metrics 位置：TiDB-&gt;Executor-&gt;Parse Duration parse慢可能原因：TiDB节点CPU压力大 图例： TiDB - Compile Metrics 位置：TiDB-&gt;Executor-&gt;Compile Duration parse慢可能原因： TiDB节点CPU压力大 in子查询结果集多，跟参数tidb_opt_insuqquery_unfold有关(2.1) 这个参数控制in子查询执行计划的处理，设置为1的情况，会默认把in的子查询结果集当做一个常量返回给上一层做where条件的过滤，compile是要生成物理执行计划的，在执行计划时in的查询已经执行了，只在2.1有开关，默认关闭，3.0后没有这个参数 图例： TiDB - Get Token Duration Metrics 位置：TiDB-&gt;Server-&gt;Get Token Duration parse慢可能原因：token个数不足，需要调整token-limit 说明连接过多，token默认上限是1000，超过1000后，客户端或者前台需要等待，如果这个等待比较长，并且连接数也超过上限了，就可以适当的调整这个参数，或者加TiDB节点 图例： PD - tso 位置：PD-&gt;Grpc-&gt;99% completed_cmd_duration_seconds-&gt;txn 可能慢的原因： PD Leader切换 PD Leader节点异常，包括cpu、磁盘等。 图例： TIKV - Grpc Duration Metrics 位置：TiKV-&gt;Grpc-&gt;Coprocessor 可能慢的原因： 需要大量扫描的key，某个SQL扫了大量的key Coprocessor CPU打满，造成资源等待 图例： TIKV - Coprocessor Cpu 位置：TiKV-&gt;Thead Cpu-&gt;Coprocessor Cpu 可能慢的原因： 大量扫描的key，将cpu资源占满 读热点，造成cpu资源等待 图例： 慢查询排查技能 - slow query log#Time：2019-04-25-15:19:33.26029 +0800 –记录了sql执行完成的时间，不是开始的时间 #Txn_start_ts：407942403346923524 –事务开始的时间戳 #User：&#114;&#111;&#x6f;&#116;&#x40;&#x31;&#50;&#55;&#x2e;&#x30;&#46;&#x30;&#46;&#49; #Conn_ID：1 #Query_time：2.632671582 –sql整体执行时间 #Process_time：0.079 Wait_time：0.009 Backoff_time：0.1 Request_count：8 Total_keys：20008 Process_keys：20000 –TiKV相关时间 #DB：test #Index_ids：[1] –TiDB，生成计划用到的索引 #Is_internal：false #Digest：edb16a8f28d9c48790925fd1c868fdae3feb49bc58481dda7df228625a5ba6e1 #Stats：t_wide:407941920305971202,t_slim:pseudo –TiDB #Cop_proc_avg：0.009875 Cop_proc_p90：0.018 Cop_proc_max：0.018 Cop_proc_addr：127.0.0.1:22160 –详细Cop信息 #Cop_wait_avg：0.001125 Cop_wait_p90：0.002 Cop_wait_max：0.002 Cop_wait_addr：127.0.0.1:24160 #Mem_max：195349 –TiDB 内存使用大小 select count(1) from t_slim,t_wide where t.clim.c0&gt;t.wide.c0 and t.slim.c1&gt;t.wide.c1 and t_wide.c0 &gt; 5000; Red color is related with TiDB Blue color is related with TiKV&#x2F;Coprocessor 慢查询排查技能 - 内存表INFOMATION_SCHEMA: select * from slow_query order by query_time desc, total_keys&#x2F;process_keys\\G; slow_query的字段值跟slow query log是对应的 slow log相关参数 slow-threshold 参数含义：输出慢sql的耗时阈值，单位ms，静态参数，需要重启TiDB SERVER生效 建议值：在OLTP系统，建议设置50ms左右，OLTP系统SQL一版有以下特点：SQL短小、单次运行时间短、执行次数多；OLAP系统可以适当放大点。 query-log-max-len 参数含义：日志记录的SQL长度，超过这个长度会截断输出，单位为字符，静态参数，需要重启TiDB SERVER生效 建议值：4096 tidb_slow_query_file 参数含义：日志记录的SQL长度，超过这个长度会截断输出，单位为字符，静态参数，需要重启TiDB SERVER生效 建议值：慢查询日志的文件名，默认值为tidb_slow.log，可以存储300M内容，动态参数，可以通过会话变量生效 建议值：tidb_slow.log 300M以后会重新生成一个文件，所以如果要定位某一段时间的slow log，需要设置该环境变量，TiDB通过session变量tidb_slow_query_file控制查询 INFOMATION_SCHEMA.SLOW_QUERY时要读取和解析的文件，可通过修改session变量的值来查询其他慢查询日志文件的内容 用SQL从多个维度查询slow log表查询INFOMATION_SCHEMA.SLOW_QUERY TiKV日志反查slow log在某个SQL执行导致的TiKV cpu异常高的场合下，使用TiKV可以快速的定位问题SQL TiKV日志中记录了slow-query的几个关键信息： ipv4：发出请求的TiDB地址，通过该地址可以确定SQL在哪个TiDB节点执行 start_ts：事务的start_ts，通过start_ts反查slow log，快速定位问题SQL，不太适合特别频繁的查询场景，start_ts很多，很难查 table_id：查询的那张表，可以通过information_schema.tables的tidb_table_id反查表名 使用方法总结 通过监控的相关metrics可以获取到TiDB集群的整体运行情况，比如某些TiKV节点CPU高不高等，对于定位慢查询来说可以有一个初步的认知，用metrics来判断哪些时间段存在问题，可能存在哪些问题 slow query log及slow log内存表，可以从整体上观察SQL运行情况，比如SQL执行花了多长时间，执行多少次，哪些SQL最耗时，哪些SQL占用资源最多等等 但是slow log内存表还存在一定的局限性，因为每个TiDB有自己的slow log且slow log file到达300M后还会切换，所以使用起来还是有一定的局限性 TiKV日志能够快速定位问题SQL，对于执行一次或几次的的大SQL来说，非常容易定位问题，但是如果单次执行快但是执行频率高的SQL来说，查看TiKV日志还是相对比较麻烦 综上所述，结合的使用三个工具能比较快速定位问题SQL AP场景加速相关并发参数扫描key数量比较多 tidb_distsql_scan_concurrency 这个变量用来设置scan操作的并发度，对于AP类应用，最大建议值不要超过所有TiKV节点的CPU核数 tidb_index_serial_scan_concurrency 这个变量用来设置顺序scan操作的并发度 tidb_index_lookup_scan_concurrency 这个变量用来设置index lookup操作的并发度 tidb_index_lookup_join_concurrency 这个变量用来设置index lookup join算法的并发度 tidb_hash_join_concurrency 这个变量用来设置hash join算法的并发度 SQL优化案例TiDB SQL优化需要注意的点 where col1条件or col2条件，在TiDB中无法使用多个索引即MySQL的index merge功能，会导致全表扫描，可以转换为from xxx where col1 union from xxx where col2，这样就可以同时使用col1索引和col2索引。注意：需要注意本身结果集是否有重复数据，若有重复数据union和or无法等价转换 index join的被驱动表，不能有函数处理的过滤条件，类似where year(xxx)&#x3D;’2020’，尽量不要在过滤条件增加对列的函数处理，（现在还不支持函数索引，4.0后会支持） 其余与通用数据库SQL一样，需要注意尽量减少not in，not exits使用，尽量用表关联 亮点：TiDB支持null及like ‘abc%’使用索引，!&#x3D;xxx在TiDB也能转换为索引范围扫描，可以转换为[-inf,xxx),(xxx,+inf]，在某些特定场景比较有意义"},{"title":"ThreadPoolExecutor的拒绝策略CallerRunsPolicy的一个潜在的大坑","path":"/2020/12/03/MySQL JDBC的queryTimeout的一个坑/","content":"https://blog.csdn.net/xieyuooo/article/details/39898449?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.control&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.control"},{"title":"TiDB SQL监控及典型的优化案例","path":"/2020/08/31/TiDB-SQL监控及典型的优化案例/","content":"TiDB读请求的执行流程SQL的一生 从客户端的Socket读取一条SQL 获取一个token 从PD获取TSO（事务的时间戳） 使用Parser将SQL parse为AST 将AST compile为执行计划 Logical Optimizer Physical Optimizer Execute Plan 根据对应的执行计划，最底层的Executor会根据这条SQL处理的Key范围构建出多个要下发到TiKV的请求，并通过distsql的API将这些请求分发到TiKV TiKV对结果做一些处理（包括filter,limit等）后，将中间结果集反馈给TiDB TiDB进行一些表关联、聚合运算最终返回给Client TiKV收到请求后，会将请求分为两类： storage read pool执行引擎：负责主键和唯一索引点查 coprocessor执行引擎：其余的请求 如何定位slow queryslow query产生的原因 按组件划分 TiDB parse慢 complie慢，生成物理执行计划 get token慢，分配线程慢 执行计划不正确 PD 获取tso慢，每个SQL要获取时间戳的 TiKV 需要扫描大量的key，耗时久 coprocessor cpu打满，资源等待 读热点 slow query获取渠道 集群监控上的metrics信息 slow-query log，对标MySQL格式，支持市场上的MySQL慢查询分析工具 TiDB的慢查询SQL内存表 TiKV节点日志slow-query反查 TiDB - Parse Metrics 位置：TiDB-&gt;Executor-&gt;Parse Duration parse慢可能原因：TiDB节点CPU压力大 图例： TiDB - Compile Metrics 位置：TiDB-&gt;Executor-&gt;Compile Duration parse慢可能原因： TiDB节点CPU压力大 in子查询结果集多，跟参数tidb_opt_insuqquery_unfold有关(2.1) 这个参数控制in子查询执行计划的处理，设置为1的情况，会默认把in的子查询结果集当做一个常量返回给上一层做where条件的过滤，compile是要生成物理执行计划的，在执行计划时in的查询已经执行了，只在2.1有开关，默认关闭，3.0后没有这个参数 图例： TiDB - Get Token Duration Metrics 位置：TiDB-&gt;Server-&gt;Get Token Duration parse慢可能原因：token个数不足，需要调整token-limit 说明连接过多，token默认上限是1000，超过1000后，客户端或者前台需要等待，如果这个等待比较长，并且连接数也超过上限了，就可以适当的调整这个参数，或者加TiDB节点 图例： PD - tso 位置：PD-&gt;Grpc-&gt;99% completed_cmd_duration_seconds-&gt;txn 可能慢的原因： PD Leader切换 PD Leader节点异常，包括cpu、磁盘等。 图例： TIKV - Grpc Duration Metrics 位置：TiKV-&gt;Grpc-&gt;Coprocessor 可能慢的原因： 需要大量扫描的key，某个SQL扫了大量的key Coprocessor CPU打满，造成资源等待 图例： TIKV - Coprocessor Cpu 位置：TiKV-&gt;Thead Cpu-&gt;Coprocessor Cpu 可能慢的原因： 大量扫描的key，将cpu资源占满 读热点，造成cpu资源等待 图例： 慢查询排查技能 - slow query log#Time：2019-04-25-15:19:33.26029 +0800 –记录了sql执行完成的时间，不是开始的时间 #Txn_start_ts：407942403346923524 –事务开始的时间戳 #User：&#114;&#111;&#x6f;&#x74;&#64;&#x31;&#50;&#x37;&#46;&#x30;&#46;&#x30;&#46;&#x31; #Conn_ID：1 #Query_time：2.632671582 –sql整体执行时间 #Process_time：0.079 Wait_time：0.009 Backoff_time：0.1 Request_count：8 Total_keys：20008 Process_keys：20000 –TiKV相关时间 #DB：test #Index_ids：[1] –TiDB，生成计划用到的索引 #Is_internal：false #Digest：edb16a8f28d9c48790925fd1c868fdae3feb49bc58481dda7df228625a5ba6e1 #Stats：t_wide:407941920305971202,t_slim:pseudo –TiDB #Cop_proc_avg：0.009875 Cop_proc_p90：0.018 Cop_proc_max：0.018 Cop_proc_addr：127.0.0.1:22160 –详细Cop信息 #Cop_wait_avg：0.001125 Cop_wait_p90：0.002 Cop_wait_max：0.002 Cop_wait_addr：127.0.0.1:24160 #Mem_max：195349 –TiDB 内存使用大小 select count(1) from t_slim,t_wide where t.clim.c0&gt;t.wide.c0 and t.slim.c1&gt;t.wide.c1 and t_wide.c0 &gt; 5000; Red color is related with TiDB Blue color is related with TiKV&#x2F;Coprocessor 慢查询排查技能 - 内存表INFOMATION_SCHEMA: select * from slow_query order by query_time desc, total_keys&#x2F;process_keys\\G; slow_query的字段值跟slow query log是对应的 slow log相关参数 slow-threshold 参数含义：输出慢sql的耗时阈值，单位ms，静态参数，需要重启TiDB SERVER生效 建议值：在OLTP系统，建议设置50ms左右，OLTP系统SQL一版有以下特点：SQL短小、单次运行时间短、执行次数多；OLAP系统可以适当放大点。 query-log-max-len 参数含义：日志记录的SQL长度，超过这个长度会截断输出，单位为字符，静态参数，需要重启TiDB SERVER生效 建议值：4096 tidb_slow_query_file 参数含义：日志记录的SQL长度，超过这个长度会截断输出，单位为字符，静态参数，需要重启TiDB SERVER生效 建议值：慢查询日志的文件名，默认值为tidb_slow.log，可以存储300M内容，动态参数，可以通过会话变量生效 建议值：tidb_slow.log 300M以后会重新生成一个文件，所以如果要定位某一段时间的slow log，需要设置该环境变量，TiDB通过session变量tidb_slow_query_file控制查询 INFOMATION_SCHEMA.SLOW_QUERY时要读取和解析的文件，可通过修改session变量的值来查询其他慢查询日志文件的内容 用SQL从多个维度查询slow log表查询INFOMATION_SCHEMA.SLOW_QUERY TiKV日志反查slow log在某个SQL执行导致的TiKV cpu异常高的场合下，使用TiKV可以快速的定位问题SQL TiKV日志中记录了slow-query的几个关键信息： ipv4：发出请求的TiDB地址，通过该地址可以确定SQL在哪个TiDB节点执行 start_ts：事务的start_ts，通过start_ts反查slow log，快速定位问题SQL，不太适合特别频繁的查询场景，start_ts很多，很难查 table_id：查询的那张表，可以通过information_schema.tables的tidb_table_id反查表名 使用方法总结 通过监控的相关metrics可以获取到TiDB集群的整体运行情况，比如某些TiKV节点CPU高不高等，对于定位慢查询来说可以有一个初步的认知，用metrics来判断哪些时间段存在问题，可能存在哪些问题 slow query log及slow log内存表，可以从整体上观察SQL运行情况，比如SQL执行花了多长时间，执行多少次，哪些SQL最耗时，哪些SQL占用资源最多等等 但是slow log内存表还存在一定的局限性，因为每个TiDB有自己的slow log且slow log file到达300M后还会切换，所以使用起来还是有一定的局限性 TiKV日志能够快速定位问题SQL，对于执行一次或几次的的大SQL来说，非常容易定位问题，但是如果单次执行快但是执行频率高的SQL来说，查看TiKV日志还是相对比较麻烦 综上所述，结合的使用三个工具能比较快速定位问题SQL AP场景加速相关并发参数扫描key数量比较多 tidb_distsql_scan_concurrency 这个变量用来设置scan操作的并发度，对于AP类应用，最大建议值不要超过所有TiKV节点的CPU核数 tidb_index_serial_scan_concurrency 这个变量用来设置顺序scan操作的并发度 tidb_index_lookup_scan_concurrency 这个变量用来设置index lookup操作的并发度 tidb_index_lookup_join_concurrency 这个变量用来设置index lookup join算法的并发度 tidb_hash_join_concurrency 这个变量用来设置hash join算法的并发度 SQL优化案例TiDB SQL优化需要注意的点 where col1条件or col2条件，在TiDB中无法使用多个索引即MySQL的index merge功能，会导致全表扫描，可以转换为from xxx where col1 union from xxx where col2，这样就可以同时使用col1索引和col2索引。注意：需要注意本身结果集是否有重复数据，若有重复数据union和or无法等价转换 index join的被驱动表，不能有函数处理的过滤条件，类似where year(xxx)&#x3D;’2020’，尽量不要在过滤条件增加对列的函数处理，（现在还不支持函数索引，4.0后会支持） 其余与通用数据库SQL一样，需要注意尽量减少not in，not exits使用，尽量用表关联 亮点：TiDB支持null及like ‘abc%’使用索引，!&#x3D;xxx在TiDB也能转换为索引范围扫描，可以转换为[-inf,xxx),(xxx,+inf]，在某些特定场景比较有意义"},{"title":"SQL诊断优化-持续更新","path":"/2020/07/20/SQL优化/","content":"Explain诊断Explain各参数的含义如下： 列名 说明 id 执行编号，标识select所属的行，如果语句中没有子查询或者关联查询，只有唯一的select，执行编号显示1，否则，内层的select语句一般会顺序编号，对应于其在原始语句的位置 select_key 显示本行是简单或者复杂select，如果查询有任何复杂的子查询，则最外层标记为PRIMARY（DERIVED、UNION、UNION RESULT） table 访问引用哪个表 type 数据访问&#x2F;读取操作类型（All、index、range、ref、eq_ref、const&#x2F;system、NULL） possible_keys 揭示哪一些索引可能有利于高效的查找 key 显示mysql实际决定采用哪个索引来优化查询 key_len 显示mysql在索引里使用的字节数 ref 显示了之前的表在key列记录的索引中查找值所用的列或常量 rows 为了找到所需要的行而需要读取的行数、估算值 Extra 额外信息，如using index、filesort等 select_type常见类型及其含义 SIMPLE：不包含子查询或者UNION操作的查询 PRIMARY：查询中如果包含任何子查询，那么最外层的查询被标记为PRIMARY SUBQUERY：子查询中第一个SELECT DEPENDENT SUBQUERY：子查询中的第一个SELECT，取决于外部查询 UNION：UNION操作的第二个或者之后的查询 DEPENDENT UNION：UNION操作的第二个或者之后的查询，取决于外部查询 UNION RESULT：UNION产生的结果集 DERIVED：出现在FROM子句中子查询 type常见类型及其含义 system：这是const类型的一个特例，只会出现在待查询的表只有一行数据的情况下 consts：常出现在主键或唯一索引与常量值进行比较的场景下，此时查询性能最优 eq_ref：当连接使用的是完整的索引并且是PRIMARY KEY或UNIQUE NOT NULL INDEX时使用它 ref：当连接使用的是前缀索引或连接条件不是PRIMARY KEY或UNIQUE INDEX时则使用它 ref_or_null：类似于ref的查询，但是附加了对NULL值列的查询 index_merge：该连接类型表示使用了索引进行合并优化 rang：使用索引进行范围扫描，常见于between、&gt;、&lt;这样的查询条件 index：索引连接类型与ALL相同，只是扫描的是索引树，通常出现在索引是该查询的覆盖索引的情况 ALL：全表扫描，效率最差的查找方式 阿里编码规范要求：至少要达到range级别，要求是ref级别，如果可以是consts最好 key列实际在查询中是否使用到索引标志字段 Extra列Extea列主要用于显示额外信息，常见信息及含义如下： Using where：MySQL服务器会在存储引擎检索行后再进行过滤 Using filesort：通常出现在GROUP BY或GROUP BY语句中，且排序或分组没有基于索引，此时需要使用文件在内存中排序，因为使用索引排序的性能好于使用文件排序，所以出现这种情况就可以考虑通过添加索引进行优化 Using index：使用了覆盖索引进行查询，此时不需要访问表，从索引中就可以获取到所需要的全部数据 Using index condition：查找使用了索引，但需要回表查询数据 Using temporary：标识需要使用临时表来处理查询，常出现GROUP BY或者GROUP BY语句中 SQL优化深度分页12-- 反例select * from table where 条件 limit 1000000, 10 MySQL并不是跳过offset行，而是取offset+N行，然后返回放弃前offset行，返回N行，那当offset特别大的时候，效率就特别低下 12-- 正例select a.* from table a, (select id from table where 条件 limit 1000000, 10) b where a.id = b.id 该方案的核心逻辑即聚簇索引，再不通过回表的情况下，快速拿到指定偏移量数据主键的id，然后利用聚簇索引进行回表查询，此时总量仅为10条，效率很高 1234567891011-- 其他思路-- id是连续的SELECT * from table WHERE id between 1000000 and 1000010;-- id不连续select * from table where id &gt;= (select id from table WHERE 条件 limit 1000000,1) limit 10-- 滚动查询，属性lastBatchMaxId存放了本次查询结果集中的最大id，开始于0，同时它也是下一批查询的起始id，直到返回空列表才退出死循环，适合大量数据导出等操作，select * from table where id &gt; #&#123;lastBatchMaxId&#125; order by id asc limit 10-- order by id desc怎么滚动查询？？？-- 第一次id起始为0，后面属性lastBatchMaxId存放了本次查询结果集中的最小id，是下一批查询的起始id，直到返回空列表才退出死循环select * from table where id &gt; 0 order by id desc limit 10select * from table where id &lt; #&#123;lastBatchMaxId&#125; order by id desc limit 10"},{"title":"String.valueOf慎用","path":"/2020/07/10/String.valueOf慎用/","content":"Java API中String.valueOf(Object obj)方法，当传入的参数为一个引用，并且引用为null时，方法会返回字符串&quot;null&quot;，这样就会引发一些你意向不到的”血案” jdk1.8.0_181中String.valueOf(Object obj)源码： 123456789101112/** * Returns the string representation of the &#123;@code Object&#125; argument. * * @param obj an &#123;@code Object&#125;. * @return if the argument is &#123;@code null&#125;, then a string equal to * &#123;@code &quot;null&quot;&#125;; otherwise, the value of * &#123;@code obj.toString()&#125; is returned. * @see java.lang.Object#toString() */public static String valueOf(Object obj) &#123; return (obj == null) ? &quot;null&quot; : obj.toString();&#125; 但当你用main方法验证时，又出现了NPE，What？ 123456789public static void main(String[] args) &#123; System.out.println(String.valueOf(null));&#125;Caused by: java.lang.NullPointerException\tat java.lang.String.&lt;init&gt;(String.java:166)\tat java.lang.String.valueOf(String.java:3008)\tat com.jcloud.billing.operation.Test.main(Test.java:21)\t... 5 more 点进方法后，发现调的是这个： 12345678910111213/** * Returns the string representation of the &#123;@code char&#125; array * argument. The contents of the character array are copied; subsequent * modification of the character array does not affect the returned * string. * * @param data the character array. * @return a &#123;@code String&#125; that contains the characters of the * character array. */public static String valueOf(char data[]) &#123; return new String(data);&#125; 这是两个问题： 1、String.valueOf(Object obj)，如果obj为null，则返回”null”这没什么好说的，因为源码就是这样写的，当你使用这个方法时，只需要特别注意一下返回值的判断，不是if(str == null)，而是if(str.equals(&quot;null&quot;)) 另外一点也体现出看源码的重要性，所以以后再调用任何API时都养成看源码的好习惯 2、System.out.println(String.valueOf(null))为什么会走到String valueOf(char data[])，而不是String.valueOf(Object obj)？看下String类库，有如下几种valueOf方法： 其中红色框中都是基本类型，null不是基本类型，能接受String.valueOf(null)的只有蓝色框中的方法，因为char[]比Object更精确，所以选择了String.valueOf(char[]) 何谓精确：这两个都能接受null的参数，这种情况下，java的重载会选取其中更精确的一个，所谓精确，比如有重载方法A和B，如果方法A入参是方法B入参的子集，则A比B更精确，换句话说就是char[]是Object的子集，毕竟Object是老大，当直接传null时会选择String.valueOf(char[])"},{"title":"Spring事务传播机制实战","path":"/2020/05/11/Spring事务传播机制实战/","content":"背景前段时间项目中做了一次大升级，其中包括数据库分库，由于分了库，原来的单库事务就变成了跨库事务，所以用到了分布式事务，引入了阿里开源的seata；由于业务的特殊性，是属于定点执行任务，在某一时间集中处理业务，所以在压力测试时，seata-server端由于事务并发量大导致seata-server的表有死锁问题，一直重试。显然不可接受，所以决定暂时舍弃seata，还是采用Spring的事务来实现跨库事务。虽然也达到了目的，但如果跨太多个库，还是不建议用Spring事务来解决，可能会造成数据不一致的情况。 Spring事务失效的原因(事务生效的条件)经常犯的错误： 方法不是public的，@Transactional作用于public方法上才会生效，如果方法不是public，能编译过能正常运行，但事务不生效，经常发生 类自身调用，同一个类的方法A调用方法B，方法B上有事务注解，事务不会生效，因为自身调用没用经过Spring代理类，默认只有在外部调用时才会生效，经常发生 异常被吃了，或者异常类型错误，Spring事务默认回滚的是RuntimeException，如果想触发其他异常回滚需要在注解上配置一下 1@Transactional(rollbackFor = Exception.class) 这个配置仅限于 Throwable 异常类及其子类 其他错误： 没有被Spring管理，类没有加注解@Service或者其他 数据源没有配置数据管理器 数据库引擎不支持事务，比如MyISAM 为什么会有传播机制Spring对事务的控制，是使用aop切面实现的，我们不用关心事务的开始、提交、回滚，只需要在方法上加上注解@Transactional即可，那这时候就有问题了： 场景1：serviceA方法调用serviceB方法，但两个方法都有事务，这个时候如果serviceB方法异常，是让serviceB方法提交，还是两个一起回滚； 场景2：serviceA方法调用serviceB方法，只有serviceA方法加了事务，serviceB方法是否也要加入serviceA方法的事务，如果serviceB方法异常，是否回滚serviceA； 场景3：serviceA方法调用serviceB方法，两者都有事务，serviceB正常执行完，但serviceA异常，是否需要回滚serviceB； 传播机制类型PROPAGATION_REQUIRED(默认) 支持当前事务，如果当前没有事务，则新建事务 如果当前存在事务，则加入当前事务，合并为一个事务 REQUIRES_NEW 新建事务，如果当前存在事务，则把当前事务挂起 这个类型，是独立提交事务，不受调用者的事务影响，父级异常，也不影响提交 NESTED 如果当前存在事务，它将成为父级事务的一个子事务，方法结束后并没有提交，只有等父级事务结束才提交 如果当前没有事务，则新建事务 如果它异常，父级可以捕获它的异常而不进行回滚，正常提交 但如果父级异常，它必回滚，这就是和REQUIRES_NEW的区别 SUPPORTS 如果当前存在事务，则加入事务 如果当前不存在事务，则以非事务方式运行，这个和没写没有区别 NOT_SUPPORTED 以非事务方式运行 如果当前存在事务，则把当前事务挂起 MANDATORY 如果当前存在事务，则运行在当前事务中 如果当前不存在事务，则抛出异常，也即父级方法必须有事务 NEVER 以非事务方式运行，如果当前存在事务，则抛出异常，即父级方法必须无事务 实现跨库事务前提：事务都是跟着数据库连接走的，一个@Transactional只能控制一个数据库的事务，所以一个事务方法内操作多个库的话，只有一个库事务启作用 有两个注意点： 一个事务里涉及到多个库，一定要使用嵌套方法调用，每个方法都加@Transactional，每个方法操作一个库，比如A调B，B调C，C调D，这样每个方法（事务）都会等下一个方法（事务）处理完才能提交，如果有一个方法（事务）异常，则全部回滚 不能A调完B再调C，这样B方法执行完，如果没有异常直接提交了，那A方法后边的逻辑异常了，B是不会回滚的，所以第二点需要注意的是，嵌套调用时，调用方法后不能有任何逻辑，否则，主方法异常后，子方法不能回滚 场景1： 123456@Transactional(&quot;库A&quot;)public void serviceA()&#123; //库A-表1 //库A-表2 //库B-表1&#125; 如果跨两个库，可以不用嵌套事务，一个事务就可以，因为库A的事务等执行完整个方法才提交，所以库B成功，则库A也提交，库B失败，则库A也回滚，但库B后边不能有任何逻辑，参考第二注意点 场景2： 12345678910111213141516@Transactional(&quot;库A&quot;)public void serviceA()&#123; //库A-表1 //库A-表2 serviceB();&#125;@Transactional(&quot;库B&quot;)public void serviceB()&#123; //库B-表1 serviceC();&#125;@Transactional(&quot;库C&quot;)public void serviceC()&#123; //库C-表1&#125; 涉及3个以上的库，嵌套调用，如果serviceB异常，serviceA还没提交，还在等serviceB执行，捕捉到serviceB异常后，回滚，同理serviceC异常，serviceA等serviceB，serviceB等serviceC，捕获到异常后都回滚。 注意：抛RuntimeException，或者注解中指定异常类型"},{"title":"一次系统启动失败经历","path":"/2020/05/09/一次系统启动失败经历/","content":"最近遇到一个很头疼的问题，在一次合代码后，系统启不来了，确切的说，本地可以起来，生成容器镜像后启不来，报了一大堆创建Spring bean异常，本地能启动，镜像启不来，这是不是很怀疑人生，通过各种排除法手段合并代码、删除代码，但就是不行，第二天又重新倒腾了一下（merge），好了，也不知道为啥就好了。 接着又有新需求了，从master上拉新的分支，吭哧吭哧干完了，编译、打包、生成镜像、部署、启动，完，又出现那个熟悉的异常了，我始终都相信代码不会说谎，绝不会是偶然，所以我决定好好找找原因。 123Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name &#x27;meteringLastBillingRecordMapper&#x27; defined in URL [jar:file:/export/Packages/jcloud-newbilling/feature-settlement-info-add-fee-20200423-c4b21536-0423144327/webapps/ROOT/WEB-INF/lib/billing-dao-1.0-SNAPSHOT.jar!/com/jcloud/billing/mapper_metering/MeteringLastBillingRecordMapper.class]: Cannot resolve reference to bean &#x27;meteringSqlSessionFactory&#x27; while setting bean property &#x27;sqlSessionFactory&#x27;; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name &#x27;meteringSqlSessionFactory&#x27;: FactoryBean threw exception on object creation; nested exception is java.lang.IllegalArgumentException: Property &#x27;dataSource&#x27; is requiredRelated cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name &#x27;meteringOverViewMapper&#x27; defined in URL [jar:file:/export/Packages/jcloud-newbilling/feature-settlement-info-add-fee-20200423-c4b21536-0423144327/webapps/ROOT/WEB-INF/lib/billing-dao-1.0-SNAPSHOT.jar!/com/jcloud/billing/mapper_metering/MeteringOverViewMapper.class]: Cannot resolve reference to bean &#x27;meteringSqlSessionFactory&#x27; while setting bean property &#x27;sqlSessionFactory&#x27;; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name &#x27;meteringSqlSessionFactory&#x27;: FactoryBean threw exception on object creation; nested exception is java.lang.IllegalArgumentException: Property &#x27;dataSource&#x27; is required 观察异常信息后，都是创建bean的错误，前提是所有的bean都被扫描到了，有一个现象是本地虽然能起来，但日志里也有WARN信息，信息也是关于创建bean的，决定升级下Spring试试，从4.3.2-&gt;5.2.5，完事后镜像启动，还是报错； 再观察异常信息后，发现最后都是以sharding-JDBC相关的持久层mapper结束的，所以从sharding-JDBC数据源的xml开始，把原来的配置文件备份下，重新搞一个新的，并且跟官方的shardingsphere-example对比，写完后，再镜像启动，还是报错； 再看shardingsphere-example后，持久层是用@Mapper注解，项目工程是用@Repository注解，难道是这个问题吗？到了这个份上，可能你觉得什么都有可能是错的，那就试一下吧，持久层注解改成@Mapper，咦？没有这个注解，项目工程的mybatis包版本是3.2.5，版本低，还没有这个注解，那就升下级，升级到3.4.2，顺便把mybatis-spring也升下级，1.2.1-&gt;1.3.0，完事后镜像启动，好了，很鸡冻。 此时由恍惚变得清醒了点，其实跟注解是@Mapper没啥关系，主要还是很mybatis、mybatis-spring版本低有关，重新把@Mapper换成@Repository后，镜像启动，也没毛病。 所以最终还是mybatis、mybatis-spring的版本和spring的版本不匹配导致，至于在本地启动可以，在镜像启动不行，这个问题还是无解，可能是跟环境有关"},{"title":"Sharding JDBC生产问题","path":"/2020/04/10/Sharding-JDBC/","content":"最近项目中在使用数据库中间件，采用ShardingSphere中的Sharding-JDBC，总结下最近出现的问题 ShardingSphere官网 分布式主键重复问题问题描述告警双写失败，库中新增metering和last_billing_record违反唯一主键约束 问题分析因为涉及到分库分表，数据是分散存放的，为了避免分表中的主键不重复，所以使用分布式主键算法生成主键，目前使用的是Sharding内置的SnowFlake(Twitter开源的)算法SnowFlake算法生成ID的结构如下图： 其中工作机器id(workerId)是用来区分不同的机器而设计，上线时，未设置这个参数，默认为0，也就是线上8台服务的workerId都为0，这样就会出现某几台服务器在同一时间(毫秒)写同一张表，按照原理图可以得出时间戳一样，工作机器id一样，序列号一样，所以就出现了多台服务器同一时间用同一个主键id写表，报了违反唯一主键约束的错误 基于计费的业务特殊性，0-半点之间会跑分布式任务，所以很容易造成多台服务器同一时间写同一张表的情况 解决方案为每台服务器设置单独的workerId，workerId取值范围0-1023 Spring Sharding配置文件是支持注入worker.id值的，但只能在配置文件配置，不能动态生成，最终会部署到8台服务器上，读的还是一个值，咨询过Sharding创始人，也没有扩展接口可以重写workerId的值 起初想过通过云翼的外挂配置文件important.properties设置workerId值，Spring配置文件再读取important.properties设置的值，不行，因为云翼上是按照az外挂的，每个az有两台服务器，那这两台服务器读取的还是一个值，还是会出现问题 最后是通过实现Spring IOC容器给我们提供的一个扩展接口BeanPostProcessor实现的，BeanPostProcessor提供了一个postProcessBeforeInitialization方法，也就是在bean初始化前可以修改worker.id的值，这样就可以为所欲为了，最终是用redis为每个服务器ip生成自增的序列值，存到redis里，每次启动时根据键ip读取redis的值，以后扩容机器接着申请即可 时钟回拨问题问题描述邮件告警双写失败，Cause: java.lang.IllegalStateException: Clock is moving backwards, last time is %d milliseconds, current time is %d milliseconds [1584000400088, 1584000399959] 问题分析错误信息中Clock is moving backwards，是由于时钟回退导致，当前生成id的时间早于最后一次生成id的时间 SnowFlake生成规则依赖时间戳，这样由于时间校准，以及其他因素，可能导致服务器时间回退（时间向前快进不会有问题），如果恰巧回退前生成过一些id，而时间回退后，生成的id就有可能重复。官方对于此并没有给出解决方案，而是简单的抛错处理，这样会造成在时间被追回之前的这段时间服务不可用 恰好10.160.10.50容器的时间比其他容器的时间快1-2s，每隔一段时间发生一次时间校准，在时间校准前后都发生了insert操作，导致了当前生成id的时间早于最后一次生成id的时间，框架报错，insert失败 解决方案 代码方面 Sharding提供了一个扩展接口ShardingKeyGenerator，可以自己实现一套分布式id生成算法，这里只是在内置的SnowFlake基础上改进了时钟回拨导致服务不可用的问题 SnowFlake算法给workerId预留了10位，即workerId的取值范围为[0, 1023]，事实上实际生产环境不大可能需要部署1024个服务 所以采用备用workerId的方式，举例：将workerId取值范围缩小为[0, 511]，[512, 1023]这个范围的workerId当做备用workerId。workerId为0的备用workerId是512，workerId为1的备用workerId是513，以此类推…… 备用workerId数量越多, 可靠性越高, 但是可部署的服务就越少 目前计费是设置了7个备份，最多能部署128个节点，足够用了 这样改后，虽然解决了不可用问题，但频繁的时钟回退导致主键不具备递增特性，根据MySql聚集索引的特性，如果主不是递增，会导致索引数据结构(B+Tree)频繁的进行分裂和平衡，随着数据量越大，性能损耗越严重 硬件方面 10.160.10.50容器迁移到了时钟正常的物理机上 多线程并发时路由不准确问题问题描述告警双写失败，更新metering或者last_billing_record条数为0，未更新成功 问题分析通过查日志发现，同一台服务器上的两个线程操作同一张表时，路由会混乱，a线程会使用b线程的路由分片值 a线程做数据迁移，迁移的时间2019-03-14 - 2019-03-31 b线程处理Kafka消息，生成用量数据，时间是2020-03-17 两个线程并发时，a线程会使用b线程的路由分片值，这样就导致a线程先用路由分片值2019-03-17插入表metering_1903成功，再用路由值2020-03-17更新表metering_2003的状态，两个表不一致，导致更新不到数据 Spring注入的路由实现类是单例的，为了方便取值，路由实现类将路由分片值定义成了实例变量，如果单例中使用实例变量，多线程不安全 解决方案删除实例变量，路由值通过方法参数传递，或者Spring注入实例设置为多例"},{"title":"Slf4j MDC 实现日志追踪","path":"/2020/04/09/log-mdc/","content":"背景随着项目的越来越大，应用处理用户请求越来越多的时候，查日志是个头疼的事，因为你要去海量日志里追踪某个用户的相关日志记录时，非常麻烦；刚开始用线程名称找，线程池的线程很快就被复用，还是很难区分，MDC就可以解决这个问题。 什么是MDCMDC（Mapped Diagnostic Context，映射调试上下文） 概括一下： MDC是一个与当前线程绑定的哈希表，可以往其中添加键值对 MDC中包含的内容可以被同一线程中执行的代码所访问 当前线程的子线程会继承其父线程中的MDC的内容，当需要记录日志时，只需要从MDC中获取所需的信息即可 MDC的内容则由程序在适当的时候保存进去。对于一个Web应用来说，通常是在请求被处理的最开始保存这些数据 web项目用法对于一个正在迭代的工程来说，肯定对代码的侵入越小越好，结合代码来看看怎么用 定义一个键值对，比如key是traceId，值是上游请求带过来的，如果没带过来，则新生成一个UUID代替，当然要根据你的实际情况定义有实际意义业务上的字段 在拦截web请求的preHandle回调方法里保存这个键值对 123456789@Overridepublic boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) &#123; String traceId = request.getHeader(&quot;traceId&quot;); if (StringUtils.isBlank(traceId)) &#123; traceId = &quot;s-&quot; + UUID.randomUUID().toString().replace(&quot;-&quot;, &quot;&quot;).toLowerCase(); &#125; MDC.put(&quot;traceId&quot;, traceId); return true; &#125; 在整个请求处理完毕afterCompletion回调方法里清除键值对，为什么要清除，因为线程池的线程是可以复用的，如果不清除，下次复用处理其他请求时，还会打印相同的traceId，就会产生混淆 1234@Override public void afterCompletion(HttpServletRequest arg0, HttpServletResponse arg1, Object arg2, Exception arg3) &#123; MDC.clear(); &#125; 最后一步，在log4j的xml文件里把%X&#123;traceId&#125;加到你想要打印的位置 1&lt;PatternLayout charset=&quot;UTF-8&quot; pattern=&quot;%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; [%t] %X&#123;traceId&#125; %-5level %C.%M %L - %m%n&quot;/&gt; 如果新启了子线程，在子线程中MDC.get(“traceId”)就可以取到父线程设置的值 定时任务用法如果你有定时任务在跑，而且还有很多，同样，加上MDC，只需要通过Spring AOP定义一个拦截器，然后在Spring配置文件中定义切点和切面即可 123456789public class TaskLog4jAdvice implements MethodInterceptor &#123; @Override public Object invoke(MethodInvocation invocation) throws Throwable &#123; MDC.put(&quot;traceId&quot;, &quot;s-&quot; + UUID.randomUUID().toString().replace(&quot;-&quot;, &quot;&quot;).toLowerCase()); Object object = invocation.proceed(); MDC.clear(); return object; &#125;&#125; 12345678&lt;aop:aspectj-autoproxy /&gt; &lt;bean id=&quot;taskLog4jAdvice&quot; class=&quot;com.wurimo.common.advice.TaskLog4jAdvice&quot;/&gt; &lt;aop:config&gt; &lt;aop:advisor id=&quot;taskMethodLog&quot; advice-ref=&quot;taskLog4jAdvice&quot; pointcut=&quot; (execution(* com.wurimo.*.task.*.*(..))) &quot;/&gt; &lt;/aop:config&gt; 多线程场景用法当创建子线程的时候，我们期望子线程一样可以使用MDC的信息来实现日志追踪，先通过MDC.getCopyOfContextMap()方法将MDC内存获取出来，并在子线程的执行最开始调用MDC.setContextMap(context)方法将父线程的MDC内容传给子线程，执行结束后调用clear清空。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100package com.surimo.common.log4j;import org.slf4j.MDC;import org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor;import java.util.Map;import java.util.concurrent.Callable;import java.util.concurrent.Future;public class MdcThreadPoolTaskExecutor extends ThreadPoolTaskExecutor &#123; private static final long serialVersionUID = 1L; private boolean useFixedContext = false; private Map&lt;String, String&gt; fixedContext; public MdcThreadPoolTaskExecutor() &#123; super(); &#125; public MdcThreadPoolTaskExecutor(Map&lt;String, String&gt; fixedContext) &#123; super(); this.fixedContext = fixedContext; useFixedContext = (fixedContext != null); &#125; private Map&lt;String, String&gt; getContextForTask() &#123; return useFixedContext ? fixedContext : MDC.getCopyOfContextMap(); &#125; @Override public void execute(Runnable task, long startTimeout) &#123; this.execute(task); &#125; @Override public Future&lt;?&gt; submit(Runnable task) &#123; return super.submit(wrapSubmit(task, getContextForTask())); &#125; @Override public void execute(Runnable task) &#123; super.execute(wrapExecute(task, getContextForTask())); &#125; @Override public &lt;T&gt; Future&lt;T&gt; submit(Callable&lt;T&gt; task) &#123; return super.submit(wrapSubmit(task, getContextForTask())); &#125; private &lt;T&gt; Callable&lt;T&gt; wrapSubmit(final Callable&lt;T&gt; task, final Map&lt;String, String&gt; context) &#123; return new Callable&lt;T&gt;() &#123; @Override public T call() throws Exception &#123; Map&lt;String, String&gt; previous = MDC.getCopyOfContextMap(); if (context == null) &#123; MDC.clear(); &#125; else &#123; MDC.setContextMap(context); &#125; try &#123; return task.call(); &#125; finally &#123; if (previous == null) &#123; MDC.clear(); &#125; else &#123; MDC.setContextMap(previous); &#125; &#125; &#125; &#125;; &#125; private Runnable wrapSubmit(final Runnable runnable, final Map&lt;String, String&gt; context) &#123; return this.wrapExecute(runnable, context); &#125; private Runnable wrapExecute(final Runnable runnable, final Map&lt;String, String&gt; context) &#123; return new Runnable() &#123; @Override public void run() &#123; Map&lt;String, String&gt; previous = MDC.getCopyOfContextMap(); if (context == null) &#123; MDC.clear(); &#125; else &#123; MDC.setContextMap(context); &#125; try &#123; runnable.run(); &#125; finally &#123; if (previous == null) &#123; MDC.clear(); &#125; else &#123; MDC.setContextMap(previous); &#125; &#125; &#125; &#125;; &#125;&#125; 新开线程用法当我们随时都想开启一个新线程处理异步流程时，当然也少不了日志追踪，举个栗子，发通知邮件，定一个抽象类实现Runnable： 1234567891011121314151617181920212223242526272829package com.wurimo.common.mdc;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.slf4j.MDC;import java.util.Map;public abstract class MdcRunnable implements Runnable &#123; private final static Logger logger = LoggerFactory.getLogger(MdcRunnable.class); private final Map&lt;String, String&gt; mdcContext = MDC.getCopyOfContextMap(); @Override public void run() &#123; try &#123; if (mdcContext != null) &#123; MDC.setContextMap(mdcContext); &#125; process(); &#125; catch (Exception e) &#123; logger.error(e.getMessage(), e); &#125; finally &#123; MDC.clear(); &#125; &#125; public abstract void process();&#125; 继承后处理自己的业务逻辑： 1234567class EmailProcess extends MdcRunnable &#123; @Override public void process() &#123; //TODO &#125; &#125; MDC原理MDC实现多线程下区分不同信息就是通过ThreadLocal，我们放进去的键值对最终定义在这里：private final ThreadLocal&lt;Map&lt;String, String&gt;&gt; localMap;MDC.put时： 12345678910@Overridepublic void put(final String key, final String value) &#123; if (!useMap) &#123; return; &#125; Map&lt;String, String&gt; map = localMap.get(); map = map == null ? new HashMap&lt;String, String&gt;() : new HashMap&lt;String, String&gt;(map); map.put(key, value); localMap.set(Collections.unmodifiableMap(map));&#125; MDC.get时： 12345@Overridepublic String get(final String key) &#123; final Map&lt;String, String&gt; map = localMap.get(); return map == null ? null : map.get(key);&#125; MDC.clear时： 1234567891011121314@Overridepublic void clear() &#123; localMap.remove();&#125;@Overridepublic void remove(final String key) &#123; final Map&lt;String, String&gt; map = localMap.get(); if (map != null) &#123; final Map&lt;String, String&gt; copy = new HashMap&lt;String, String&gt;(map); copy.remove(key); localMap.set(Collections.unmodifiableMap(copy)); &#125;&#125; 有兴趣可以自行看下源码https://hexo.io/docs/one-command-deployment.html)"},{"title":"Windows剪贴板增强软件-ClipX","path":"/2020/04/09/Windows剪贴板增强软件-ClipX/","content":"效率神器-只限windows官网下载：http://bluemars.org/clipx/ 能干啥ClipX可以保存最近多次复制的内容，重复的内容不用来回复制了，支持文本、图片，而且复制的内容可以保存再本地，电脑重启后还能用 使用，图标右键 扩充剪贴板数量最多支持保存1024条记录，正常人够用了 随时预览剪贴板内容 可以编辑剪贴板内容 数据本地存储还得手动，自动存储就好了 快捷键设置 优点 免费 内存占用低 简单易用 侵入性小，不影响系统自带剪贴板 支持插件扩展"},{"title":"APM-Pinpoint","path":"/2020/04/09/APM-Pinpoint/","content":"架构图 单机模式安装准备安装包： Hbase hbase-2.1.5-bin.tar.gz hbase-create.hbase下载地址：http://apache.fayea.com/hbase/wget http://apache.fayea.com/hbase/2.1.5/hbase-2.1.5-bin.tar.gzhttps://github.com/naver/pinpoint/tree/master/hbase/scripts/hbase-create.hbase pinpoint pinpoint-collector-1.8.4.war pinpoint-web-1.8.4.war pinpoint-agent-1.8.4.tar.gz下载地址：https://github.com/naver/pinpoint/releases 安装Hbase1、解压hbase-2.1.5-bin.tar.gz 1tar xf hbase-2.1.5-bin.tar.gz 2、启动 12cd hbase-2.1.5/bin./start-hbase.sh 需要jdk8版本，手动指定jdk配置：cd hbase-2.1.5&#x2F;confvim hbase-env.sh增加export JAVA_HOME&#x3D;&#x2F;export&#x2F;software&#x2F;jdk1.8.0_20&#x2F;保存后启动 3、初始化脚本 1./hbase shell /home/ssh/pinpoint/hbase-create.hbase 4、web访问http://127.0.0.1:16010/ 安装pinpoint-collector解压war包到tomcat下 123unzip pinpoint-collector-1.8.4.war -d /home/ssh/pinpoint/tomcat-collector/webapps/ROOTcd /home/ssh/pinpoint/tomcat-collector/bin./startup.sh 安装pinpoint-web解压war包到tomcat下（端口号28080） 123unzip pinpoint-web-1.8.4.war -d /home/ssh/pinpoint/tomcat-web/webapps/ROOTcd /home/ssh/pinpoint/tomcat-web/bin./startup.sh 注意修改tomcat端口号 web访问：http://127.0.0.1:28080/ 安装pinpoint-agent tomcat启动的应用在tomcat bin下的catalina.sh增加配置 123CATALINA_OPTS=&quot;$CATALINA_OPTS -javaagent:/export/software/pinpoint/agent/pinpoint-bootstrap-1.8.4.jar&quot;CATALINA_OPTS=&quot;$CATALINA_OPTS -Dpinpoint.agentId=settlment1&quot;CATALINA_OPTS=&quot;$CATALINA_OPTS -Dpinpoint.applicationName=settlement&quot; springboot启动的应用jar启动方式为： 1nohup java -javaagent:/home/ssh/software/pinpoint/agent/pinpoint-bootstrap-1.8.4.jar -Dpinpoint.applicationName=settlement -Dpinpoint.agentId=settlement1 -jar settlement-latest.jar &gt;/dev/null 2&gt;&amp;1 &amp; 使用手册https://blog.csdn.net/xvshu/article/details/79866237 参考https://www.jianshu.com/p/a803571cd570"},{"title":"redis分布式锁","path":"/2020/04/09/redis分布式锁/","content":"加锁伪代码 123456789101112private static final String LOCK_SUCCESS = &quot;OK&quot;;public boolean acquireLock(String lockKey, String value) &#123; logger.info(&quot;acquireLock lockKey:&#123;&#125;, value:&#123;&#125;&quot;, lockKey, value); //NX是不存在时才set， XX是存在时才set， EX是秒，PX是毫秒 String result = jedis.set(key, value, &quot;NX&quot;, &quot;PX&quot;, LOCK_TIMEOUT); boolean flag = false; if(LOCK_SUCCESS.equals(result))&#123; flag = true; &#125; logger.info(&quot;acquireLock result:&quot; + flag); return flag; &#125; 第一个为key，我们使用key来当锁名第二个为value，我们传的是uid，唯一随机数，也可以使用本机mac地址 + uuid第三个为NX，意思是SET IF NOT EXIST，即当key不存在时，我们进行set操作；若key已经存在，则不做任何操作第四个为PX，意思是我们要给这个key加一个过期的设置，具体时间由第五个参数决定第五个为time，代表key的过期时间，对应第四个参数 PX毫秒，EX秒 释放锁伪代码 123456789101112private static final Long RELEASE_LOCK_SUCCESS = 1L;public boolean releaseLock(String lockKey, String value) &#123; logger.info(&quot;releaseLock lockKey:&#123;&#125;, value:&#123;&#125;&quot;, lockKey, value); String luaScript = &quot;if redis.call(&#x27;get&#x27;, KEYS[1]) == ARGV[1] then return redis.call(&#x27;del&#x27;, KEYS[1]) else return 0 end&quot;; Object result = jedis.eval(script, Collections.singletonList(lockKey), Collections.singletonList(value)); boolean flag = false; if(RELEASE_LOCK_SUCCESS.equals(result))&#123; flag = true; &#125; logger.info(&quot;releaseLock result:&quot; + flag); return flag; &#125; redis可以保证lua中的键的原子操作上边lua脚本的意思：if redis.call(&quot;get&quot;,&quot;“ + lock + “&quot;) &#x2F;&#x2F; redisGET命令 &#x3D;&#x3D; &quot;“ +uid + &#x2F;&#x2F; 判断是否是当前线程 “&quot;then return redis.call(&quot;del&quot;,&quot;“ + lock + “&quot;) &#x2F;&#x2F; 如果是，执行redis DEL操作，删除锁 else return 0 end 关于luaLua 是一种轻量小巧的脚本语言，用标准C语言编写并以源代码形式开放， 其设计目的是为了嵌入应用程序中，从而为应用程序提供灵活的扩展和定制功能。Lua 提供了交互式编程模式。我们可以在命令行中输入程序并立即查看效果。 lua脚本优点 减少网络开销：本来多次网络请求的操作，可以用一个请求完成，原先多次请求的逻辑放在redis服务器上完成。使用脚本，减少了网络往返时延 原子操作：Redis会将整个脚本作为一个整体执行，中间不会被其他命令插入 复用：客户端发送的脚本会永久存储在Redis中，意味着其他客户端可以复用这一脚本而不需要使用代码完成同样的逻辑"},{"title":"APM-Skywalking","path":"/2020/04/09/APM-Skywalking/","content":"简介官网：http://skywalking.apache.org/zh/ 安装：https://github.com/apache/skywalking/blob/5.x/docs/README_ZH.md mysql版本安装：https://blog.csdn.net/sinat_30126855/article/details/100895591 查看效果：http://blog.wurimo.com:8070/ 对比：https://www.jianshu.com/p/0fbbf99a236e 安装es：https://www.jianshu.com/p/c5347f502205https://www.jianshu.com/p/e3d7c50651f6https://www.jianshu.com/p/a803571cd570)"}]